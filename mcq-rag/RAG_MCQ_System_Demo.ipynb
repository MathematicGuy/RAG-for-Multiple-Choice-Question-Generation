{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e18e2357",
   "metadata": {},
   "source": [
    "# RAG System for Multiple Choice Question (MCQ) Generation\n",
    "\n",
    "## Comprehensive Demonstration and Implementation Guide\n",
    "\n",
    "This notebook demonstrates a complete implementation of a Retrieval-Augmented Generation (RAG) system specifically designed for generating high-quality Multiple Choice Questions from educational documents.\n",
    "\n",
    "### System Overview\n",
    "- **Document Processing**: PDF text extraction and semantic chunking\n",
    "- **Vector Database**: FAISS with Vietnamese language embeddings\n",
    "- **Question Generation**: LLM-powered MCQ creation with structured output\n",
    "- **Quality Assurance**: Automatic validation and difficulty assessment\n",
    "- **Batch Processing**: Scalable question generation capabilities\n",
    "\n",
    "### Key Features\n",
    "- üåê Vietnamese language support\n",
    "- üìö Multi-document processing\n",
    "- üéØ Multiple question types (definition, application, analysis)\n",
    "- üìä Quality scoring and validation\n",
    "- ‚ö° Optimized performance with quantized models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bac29a6",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies\n",
    "\n",
    "First, let's install and import all required libraries for our RAG-MCQ system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6583f9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce GTX 1650\n",
      "CUDA memory: 4.3 GB\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install langchain langchain-community langchain-experimental langchain-huggingface\n",
    "# !pip install transformers torch accelerate bitsandbytes\n",
    "# !pip install faiss-cpu sentence-transformers\n",
    "# !pip install pypdf unstructured\n",
    "# !pip install numpy pandas matplotlib seaborn\n",
    "# !pip install nltk rouge-score\n",
    "\n",
    "# Check if CUDA is available\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a5710fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "üì¶ LangChain version: 0.3.26\n",
      "ü§ó Transformers version: 4.53.2\n",
      "üî• PyTorch version: 2.7.1+cu118\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from transformers.utils.quantization_config import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "# Transformers imports\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from transformers.pipelines import pipeline\n",
    "from transformers.utils.quantization_config import BitNetQuantConfig\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üì¶ LangChain version: {getattr(__import__('langchain'), '__version__', 'Unknown')}\")\n",
    "print(f\"ü§ó Transformers version: {getattr(__import__('transformers'), '__version__', 'Unknown')}\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adf39df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration and data classes defined!\n",
      "üìã Using embedding model: bkai-foundation-models/vietnamese-bi-encoder\n",
      "ü§ñ Using LLM model: unsloth/Qwen2.5-7B\n"
     ]
    }
   ],
   "source": [
    "# Configuration and Data Classes\n",
    "class QuestionType(Enum):\n",
    "    \"\"\"Enumeration of different question types\"\"\"\n",
    "    DEFINITION = \"definition\"\n",
    "    COMPARISON = \"comparison\"\n",
    "    APPLICATION = \"application\"\n",
    "    ANALYSIS = \"analysis\"\n",
    "    EVALUATION = \"evaluation\"\n",
    "\n",
    "class DifficultyLevel(Enum):\n",
    "    \"\"\"Enumeration of difficulty levels\"\"\"\n",
    "    EASY = \"easy\"\n",
    "    MEDIUM = \"medium\"\n",
    "    HARD = \"hard\"\n",
    "    EXPERT = \"expert\"\n",
    "\n",
    "@dataclass\n",
    "class MCQOption:\n",
    "    \"\"\"Data class for MCQ options\"\"\"\n",
    "    label: str\n",
    "    text: str\n",
    "    is_correct: bool\n",
    "\n",
    "@dataclass\n",
    "class MCQQuestion:\n",
    "    \"\"\"Data class for Multiple Choice Question\"\"\"\n",
    "    question: str\n",
    "    context: str\n",
    "    options: List[MCQOption]\n",
    "    explanation: str\n",
    "    difficulty: str\n",
    "    topic: str\n",
    "    question_type: str\n",
    "    source: str\n",
    "    confidence_score: float = 0.0\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert to dictionary format\"\"\"\n",
    "        return {\n",
    "            \"question\": self.question,\n",
    "            \"context\": self.context,\n",
    "            \"options\": {opt.label: opt.text for opt in self.options},\n",
    "            \"correct_answer\": next(opt.label for opt in self.options if opt.is_correct),\n",
    "            \"explanation\": self.explanation,\n",
    "            \"difficulty\": self.difficulty,\n",
    "            \"topic\": self.topic,\n",
    "            \"question_type\": self.question_type,\n",
    "            \"source\": self.source,\n",
    "            \"confidence_score\": self.confidence_score\n",
    "        }\n",
    "\n",
    "# System Configuration\n",
    "CONFIG = {\n",
    "    \"embedding_model\": \"bkai-foundation-models/vietnamese-bi-encoder\",\n",
    "    \"llm_model\": \"unsloth/Qwen2.5-7B\",\n",
    "    \"chunk_size\": 500,\n",
    "    \"chunk_overlap\": 50,\n",
    "    \"retrieval_k\": 5,\n",
    "    \"generation_temperature\": 0.7,\n",
    "    \"max_tokens\": 512,\n",
    "    \"diversity_threshold\": 0.7\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuration and data classes defined!\")\n",
    "print(f\"üìã Using embedding model: {CONFIG['embedding_model']}\")\n",
    "print(f\"ü§ñ Using LLM model: {CONFIG['llm_model']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c5d0ba",
   "metadata": {},
   "source": [
    "## 2. Document Processing Pipeline\n",
    "\n",
    "Let's implement the document loading and text extraction pipeline for PDF documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66df219f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Document processor initialized!\n"
     ]
    }
   ],
   "source": [
    "class DocumentProcessor:\n",
    "    \"\"\"Handles document loading and preprocessing\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.supported_formats = ['.pdf', '.txt']\n",
    "\n",
    "    def load_documents(self, folder_path: str) -> Tuple[List[Document], List[str]]:\n",
    "        \"\"\"Load and process documents from folder\"\"\"\n",
    "        folder = Path(folder_path)\n",
    "        if not folder.exists():\n",
    "            raise FileNotFoundError(f\"Folder not found: {folder}\")\n",
    "\n",
    "        pdf_files = list(folder.glob(\"*.pdf\"))\n",
    "        if not pdf_files:\n",
    "            print(f\"‚ö†Ô∏è  No PDF files found in: {folder}\")\n",
    "            return [], []\n",
    "\n",
    "        all_docs, filenames = [], []\n",
    "        total_pages = 0\n",
    "\n",
    "        print(f\"üìÅ Processing {len(pdf_files)} PDF files...\")\n",
    "\n",
    "        for pdf_file in pdf_files:\n",
    "            try:\n",
    "                print(f\"üìÑ Loading: {pdf_file.name}\")\n",
    "                loader = PyPDFLoader(str(pdf_file))\n",
    "                docs = loader.load()\n",
    "\n",
    "                # Add metadata\n",
    "                for doc in docs:\n",
    "                    doc.metadata['source_file'] = pdf_file.name\n",
    "                    doc.metadata['file_path'] = str(pdf_file)\n",
    "\n",
    "                all_docs.extend(docs)\n",
    "                filenames.append(pdf_file.name)\n",
    "                total_pages += len(docs)\n",
    "\n",
    "                print(f\"  ‚úÖ Loaded {len(docs)} pages\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Failed loading {pdf_file.name}: {e}\")\n",
    "\n",
    "        print(f\"\\nüìä Summary:\")\n",
    "        print(f\"  üìö Files loaded: {len(filenames)}\")\n",
    "        print(f\"  üìÑ Total pages: {total_pages}\")\n",
    "        print(f\"  üìù Average pages per file: {total_pages/len(filenames):.1f}\")\n",
    "\n",
    "        return all_docs, filenames\n",
    "\n",
    "    def analyze_document_stats(self, docs: List[Document]) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze document statistics\"\"\"\n",
    "        if not docs:\n",
    "            return {}\n",
    "\n",
    "        # Calculate statistics\n",
    "        total_chars = sum(len(doc.page_content) for doc in docs)\n",
    "        total_words = sum(len(doc.page_content.split()) for doc in docs)\n",
    "\n",
    "        char_lengths = [len(doc.page_content) for doc in docs]\n",
    "        word_lengths = [len(doc.page_content.split()) for doc in docs]\n",
    "\n",
    "        stats = {\n",
    "            \"total_documents\": len(docs),\n",
    "            \"total_characters\": total_chars,\n",
    "            \"total_words\": total_words,\n",
    "            \"avg_chars_per_doc\": np.mean(char_lengths),\n",
    "            \"avg_words_per_doc\": np.mean(word_lengths),\n",
    "            \"min_chars\": np.min(char_lengths),\n",
    "            \"max_chars\": np.max(char_lengths),\n",
    "            \"min_words\": np.min(word_lengths),\n",
    "            \"max_words\": np.max(word_lengths)\n",
    "        }\n",
    "\n",
    "        return stats\n",
    "\n",
    "# Test the document processor\n",
    "doc_processor = DocumentProcessor()\n",
    "print(\"‚úÖ Document processor initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00d4e56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Using sample documents for demonstration\n",
      "üìÑ Total documents: 3\n",
      "\n",
      "üìä Document Statistics:\n",
      "  total_documents: 3\n",
      "  total_characters: 1435\n",
      "  total_words: 243\n",
      "  avg_chars_per_doc: 478.3\n",
      "  avg_words_per_doc: 81.0\n",
      "  min_chars: 458\n",
      "  max_chars: 511\n",
      "  min_words: 78\n",
      "  max_words: 83\n"
     ]
    }
   ],
   "source": [
    "# Load sample documents (update path as needed)\n",
    "# Uncomment and modify the path to your PDF folder\n",
    "\n",
    "# folder_path = \"../pdf_folder\"  # Update this path\n",
    "# docs, filenames = doc_processor.load_documents(folder_path)\n",
    "\n",
    "# For demonstration, let's create some sample documents\n",
    "sample_docs = [\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Object-Oriented Programming (OOP) l√† m·ªôt m√¥ h√¨nh l·∫≠p tr√¨nh ƒë∆∞·ª£c x√¢y d·ª±ng d·ª±a tr√™n kh√°i ni·ªám ƒë·ªëi t∆∞·ª£ng.\n",
    "        OOP t·ªï ch·ª©c m√£ ngu·ªìn xung quanh c√°c ƒë·ªëi t∆∞·ª£ng thay v√¨ c√°c h√†m v√† logic.\n",
    "\n",
    "        C√°c nguy√™n l√Ω c∆° b·∫£n c·ªßa OOP bao g·ªìm:\n",
    "        1. Encapsulation (ƒê√≥ng g√≥i): ·∫®n gi·∫•u chi ti·∫øt tri·ªÉn khai\n",
    "        2. Inheritance (K·∫ø th·ª´a): T√°i s·ª≠ d·ª•ng code t·ª´ class cha\n",
    "        3. Polymorphism (ƒêa h√¨nh): C√πng m·ªôt interface, nhi·ªÅu implementation\n",
    "        4. Abstraction (Tr·ª´u t∆∞·ª£ng): ƒê∆°n gi·∫£n h√≥a ph·ª©c t·∫°p\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"OOP_basics.pdf\", \"page\": 1}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Inheritance (K·∫ø th·ª´a) trong OOP cho ph√©p m·ªôt class con k·∫ø th·ª´a c√°c thu·ªôc t√≠nh v√† ph∆∞∆°ng th·ª©c t·ª´ class cha.\n",
    "\n",
    "        V√≠ d·ª•:\n",
    "        - Class Animal c√≥ thu·ªôc t√≠nh name v√† ph∆∞∆°ng th·ª©c eat()\n",
    "        - Class Dog k·∫ø th·ª´a t·ª´ Animal v√† th√™m ph∆∞∆°ng th·ª©c bark()\n",
    "        - Class Cat k·∫ø th·ª´a t·ª´ Animal v√† th√™m ph∆∞∆°ng th·ª©c meow()\n",
    "\n",
    "        L·ª£i √≠ch c·ªßa inheritance:\n",
    "        - T√°i s·ª≠ d·ª•ng code\n",
    "        - D·ªÖ d√†ng m·ªü r·ªông\n",
    "        - T·ªï ch·ª©c code theo hierarchy\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"OOP_inheritance.pdf\", \"page\": 2}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Data Structures (C·∫•u tr√∫c d·ªØ li·ªáu) l√† c√°ch t·ªï ch·ª©c v√† l∆∞u tr·ªØ d·ªØ li·ªáu trong m√°y t√≠nh.\n",
    "\n",
    "        C√°c c·∫•u tr√∫c d·ªØ li·ªáu c∆° b·∫£n:\n",
    "        1. Array: T·∫≠p h·ª£p c√°c ph·∫ßn t·ª≠ c√πng ki·ªÉu\n",
    "        2. Linked List: Danh s√°ch li√™n k·∫øt\n",
    "        3. Stack: NgƒÉn x·∫øp (LIFO - Last In First Out)\n",
    "        4. Queue: H√†ng ƒë·ª£i (FIFO - First In First Out)\n",
    "        5. Tree: C√¢y\n",
    "        6. Graph: ƒê·ªì th·ªã\n",
    "\n",
    "        Ch·ªçn c·∫•u tr√∫c d·ªØ li·ªáu ph√π h·ª£p ·∫£nh h∆∞·ªüng ƒë·∫øn hi·ªáu su·∫•t c·ªßa thu·∫≠t to√°n.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"Data_Structures.pdf\", \"page\": 3}\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"üìö Using sample documents for demonstration\")\n",
    "print(f\"üìÑ Total documents: {len(sample_docs)}\")\n",
    "\n",
    "# Analyze document statistics\n",
    "stats = doc_processor.analyze_document_stats(sample_docs)\n",
    "print(f\"\\nüìä Document Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.1f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "docs = sample_docs  # Use for the rest of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38fdf823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac418b0ad4a4526ac0172398d798295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login() # 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"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd9c03a",
   "metadata": {},
   "source": [
    "## 3. Vector Database and Embeddings\n",
    "\n",
    "Now let's set up the FAISS vector database with Vietnamese embeddings for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fceddb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Loading embedding model: bkai-foundation-models/vietnamese-bi-encoder\n",
      "‚úÖ Embeddings initialized successfully!\n",
      "üéØ Ready to process documents!\n"
     ]
    }
   ],
   "source": [
    "class VectorDatabaseManager:\n",
    "    \"\"\"Manages vector database and embeddings\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model_name: str):\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.embeddings = None\n",
    "        self.vector_db = None\n",
    "        self.chunks = []\n",
    "\n",
    "    def initialize_embeddings(self):\n",
    "        \"\"\"Initialize the embedding model\"\"\"\n",
    "        print(f\"üîß Loading embedding model: {self.embedding_model_name}\")\n",
    "        try:\n",
    "            self.embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=self.embedding_model_name,\n",
    "                model_kwargs={'device': 'cpu'}  # Use CPU for compatibility\n",
    "            )\n",
    "            print(\"‚úÖ Embeddings initialized successfully!\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to load embeddings: {e}\")\n",
    "            print(\"üîÑ Falling back to sentence-transformers model...\")\n",
    "            try:\n",
    "                self.embeddings = HuggingFaceEmbeddings(\n",
    "                    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "                )\n",
    "                print(\"‚úÖ Fallback embeddings loaded!\")\n",
    "                return True\n",
    "            except Exception as e2:\n",
    "                print(f\"‚ùå Fallback also failed: {e2}\")\n",
    "                return False\n",
    "\n",
    "    def create_semantic_chunks(self, docs: List[Document]) -> List[Document]:\n",
    "        \"\"\"Create semantic chunks from documents\"\"\"\n",
    "        if not self.embeddings:\n",
    "            raise RuntimeError(\"Embeddings not initialized\")\n",
    "\n",
    "        print(\"üî™ Creating semantic chunks...\")\n",
    "\n",
    "        try:\n",
    "            # Use SemanticChunker for intelligent chunking\n",
    "            chunker = SemanticChunker(\n",
    "                embeddings=self.embeddings,\n",
    "                buffer_size=1,\n",
    "                breakpoint_threshold_type=\"percentile\",\n",
    "                breakpoint_threshold_amount=95,\n",
    "                min_chunk_size=CONFIG[\"chunk_size\"],\n",
    "                add_start_index=True\n",
    "            )\n",
    "\n",
    "            chunks = chunker.split_documents(docs)\n",
    "            self.chunks = chunks\n",
    "\n",
    "            print(f\"‚úÖ Created {len(chunks)} semantic chunks\")\n",
    "\n",
    "            # Analyze chunk statistics\n",
    "            chunk_lengths = [len(chunk.page_content) for chunk in chunks]\n",
    "            print(f\"üìä Chunk statistics:\")\n",
    "            print(f\"  Average length: {np.mean(chunk_lengths):.1f} characters\")\n",
    "            print(f\"  Min length: {np.min(chunk_lengths)} characters\")\n",
    "            print(f\"  Max length: {np.max(chunk_lengths)} characters\")\n",
    "\n",
    "            return chunks\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Semantic chunking failed: {e}\")\n",
    "            print(\"üîÑ Falling back to simple text splitting...\")\n",
    "\n",
    "            # Fallback to simple chunking\n",
    "            simple_chunks = []\n",
    "            for doc in docs:\n",
    "                content = doc.page_content\n",
    "                chunk_size = CONFIG[\"chunk_size\"]\n",
    "                overlap = CONFIG[\"chunk_overlap\"]\n",
    "\n",
    "                for i in range(0, len(content), chunk_size - overlap):\n",
    "                    chunk_content = content[i:i + chunk_size]\n",
    "                    if len(chunk_content.strip()) > 50:  # Minimum chunk size\n",
    "                        chunk = Document(\n",
    "                            page_content=chunk_content,\n",
    "                            metadata={**doc.metadata, \"chunk_index\": len(simple_chunks)}\n",
    "                        )\n",
    "                        simple_chunks.append(chunk)\n",
    "\n",
    "            self.chunks = simple_chunks\n",
    "            print(f\"‚úÖ Created {len(simple_chunks)} simple chunks\")\n",
    "            return simple_chunks\n",
    "\n",
    "    def build_vector_database(self, chunks: List[Document]) -> bool:\n",
    "        \"\"\"Build FAISS vector database from chunks\"\"\"\n",
    "        if not self.embeddings:\n",
    "            raise RuntimeError(\"Embeddings not initialized\")\n",
    "\n",
    "        print(\"üóÑÔ∏è  Building FAISS vector database...\")\n",
    "\n",
    "        try:\n",
    "            self.vector_db = FAISS.from_documents(chunks, embedding=self.embeddings)\n",
    "            print(\"‚úÖ Vector database created successfully!\")\n",
    "            print(f\"üìö Indexed {len(chunks)} chunks\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to build vector database: {e}\")\n",
    "            return False\n",
    "\n",
    "    def search_similar_chunks(self, query: str, k: int = 5) -> List[Document]:\n",
    "        \"\"\"Search for similar chunks\"\"\"\n",
    "        if not self.vector_db:\n",
    "            raise RuntimeError(\"Vector database not initialized\")\n",
    "\n",
    "        try:\n",
    "            results = self.vector_db.similarity_search(query, k=k)\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Search failed: {e}\")\n",
    "            return []\n",
    "\n",
    "# Initialize vector database manager\n",
    "vector_manager = VectorDatabaseManager(CONFIG[\"embedding_model\"])\n",
    "\n",
    "# Initialize embeddings\n",
    "if vector_manager.initialize_embeddings():\n",
    "    print(\"üéØ Ready to process documents!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6118b4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting document processing pipeline...\n",
      "üî™ Creating semantic chunks...\n",
      "‚úÖ Created 3 semantic chunks\n",
      "üìä Chunk statistics:\n",
      "  Average length: 464.0 characters\n",
      "  Min length: 449 characters\n",
      "  Max length: 494 characters\n",
      "üóÑÔ∏è  Building FAISS vector database...\n",
      "‚úÖ Vector database created successfully!\n",
      "üìö Indexed 3 chunks\n",
      "‚úÖ Document processing pipeline completed!\n",
      "\n",
      "üîç Testing similarity search with query: 'OOP l√† g√¨'\n",
      "\n",
      "üìÑ Result 1:\n",
      "Source: OOP_basics.pdf\n",
      "Content: \n",
      "        Object-Oriented Programming (OOP) l√† m·ªôt m√¥ h√¨nh l·∫≠p tr√¨nh ƒë∆∞·ª£c x√¢y d·ª±ng d·ª±a tr√™n kh√°i ni·ªám ƒë·ªëi t∆∞·ª£ng. OOP t·ªï ch·ª©c m√£ ngu·ªìn xung quanh c√°c ƒë·ªëi t∆∞·ª£ng thay v√¨ c√°c h√†m v√† logic. C√°c nguy√™n l√Ω c∆°...\n",
      "\n",
      "üìÑ Result 2:\n",
      "Source: OOP_inheritance.pdf\n",
      "Content: \n",
      "        Inheritance (K·∫ø th·ª´a) trong OOP cho ph√©p m·ªôt class con k·∫ø th·ª´a c√°c thu·ªôc t√≠nh v√† ph∆∞∆°ng th·ª©c t·ª´ class cha. V√≠ d·ª•:\n",
      "        - Class Animal c√≥ thu·ªôc t√≠nh name v√† ph∆∞∆°ng th·ª©c eat()\n",
      "        - Clas...\n",
      "\n",
      "üìÑ Result 3:\n",
      "Source: Data_Structures.pdf\n",
      "Content: \n",
      "        Data Structures (C·∫•u tr√∫c d·ªØ li·ªáu) l√† c√°ch t·ªï ch·ª©c v√† l∆∞u tr·ªØ d·ªØ li·ªáu trong m√°y t√≠nh. C√°c c·∫•u tr√∫c d·ªØ li·ªáu c∆° b·∫£n:\n",
      "        1. Array: T·∫≠p h·ª£p c√°c ph·∫ßn t·ª≠ c√πng ki·ªÉu\n",
      "        2. Linked List: Danh...\n"
     ]
    }
   ],
   "source": [
    "# Process documents and build vector database\n",
    "print(\"üöÄ Starting document processing pipeline...\")\n",
    "\n",
    "# Create semantic chunks\n",
    "chunks = vector_manager.create_semantic_chunks(docs)\n",
    "\n",
    "# Build vector database\n",
    "if vector_manager.build_vector_database(chunks):\n",
    "    print(\"‚úÖ Document processing pipeline completed!\")\n",
    "\n",
    "    # Test similarity search\n",
    "    test_query = \"OOP l√† g√¨\"\n",
    "    print(f\"\\nüîç Testing similarity search with query: '{test_query}'\")\n",
    "\n",
    "    results = vector_manager.search_similar_chunks(test_query, k=3)\n",
    "\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\nüìÑ Result {i}:\")\n",
    "        print(f\"Source: {result.metadata.get('source', 'Unknown')}\")\n",
    "        print(f\"Content: {result.page_content[:200]}...\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Failed to build vector database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eff769",
   "metadata": {},
   "source": [
    "## 4. Retrieval System Implementation\n",
    "\n",
    "Implementing a context-aware retrieval system with diversity controls for better question generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a90ed962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Context-aware retriever initialized!\n",
      "\n",
      "üß™ Testing diverse retrieval for different topics:\n",
      "\n",
      "üìö Topic: OOP\n",
      "  Retrieved 2 diverse documents\n",
      "  Doc 1: \n",
      "        Object-Oriented Programming (OOP) l√† m·ªôt m√¥ h√¨nh l·∫≠p tr√¨nh ƒë∆∞·ª£c x√¢y d·ª±ng d·ª±a tr√™n kh√°i ni·ªám...\n",
      "  Doc 2: \n",
      "        Inheritance (K·∫ø th·ª´a) trong OOP cho ph√©p m·ªôt class con k·∫ø th·ª´a c√°c thu·ªôc t√≠nh v√† ph∆∞∆°ng th·ª©...\n",
      "\n",
      "üìö Topic: inheritance\n",
      "  Retrieved 2 diverse documents\n",
      "  Doc 1: \n",
      "        Inheritance (K·∫ø th·ª´a) trong OOP cho ph√©p m·ªôt class con k·∫ø th·ª´a c√°c thu·ªôc t√≠nh v√† ph∆∞∆°ng th·ª©...\n",
      "  Doc 2: \n",
      "        Data Structures (C·∫•u tr√∫c d·ªØ li·ªáu) l√† c√°ch t·ªï ch·ª©c v√† l∆∞u tr·ªØ d·ªØ li·ªáu trong m√°y t√≠nh. C√°c c...\n",
      "\n",
      "üìö Topic: data structures\n",
      "  Retrieved 2 diverse documents\n",
      "  Doc 1: \n",
      "        Data Structures (C·∫•u tr√∫c d·ªØ li·ªáu) l√† c√°ch t·ªï ch·ª©c v√† l∆∞u tr·ªØ d·ªØ li·ªáu trong m√°y t√≠nh. C√°c c...\n",
      "  Doc 2: \n",
      "        Object-Oriented Programming (OOP) l√† m·ªôt m√¥ h√¨nh l·∫≠p tr√¨nh ƒë∆∞·ª£c x√¢y d·ª±ng d·ª±a tr√™n kh√°i ni·ªám...\n"
     ]
    }
   ],
   "source": [
    "class ContextAwareRetriever:\n",
    "    \"\"\"Enhanced retriever with context awareness and diversity\"\"\"\n",
    "\n",
    "    def __init__(self, vector_db: FAISS, diversity_threshold: float = 0.7):\n",
    "        self.vector_db = vector_db\n",
    "        self.diversity_threshold = diversity_threshold\n",
    "\n",
    "    def retrieve_diverse_contexts(self, query: str, k: int = 5) -> List[Document]:\n",
    "        \"\"\"Retrieve documents with semantic diversity\"\"\"\n",
    "        # Get more candidates than needed\n",
    "        candidates = self.vector_db.similarity_search(query, k=k*2)\n",
    "\n",
    "        if not candidates:\n",
    "            return []\n",
    "\n",
    "        # Select diverse documents\n",
    "        selected = [candidates[0]]  # Always include the most relevant\n",
    "\n",
    "        for candidate in candidates[1:]:\n",
    "            if len(selected) >= k:\n",
    "                break\n",
    "\n",
    "            # Check diversity with already selected documents\n",
    "            is_diverse = True\n",
    "            for selected_doc in selected:\n",
    "                similarity = self._calculate_similarity(\n",
    "                    candidate.page_content,\n",
    "                    selected_doc.page_content\n",
    "                )\n",
    "                if similarity > self.diversity_threshold:\n",
    "                    is_diverse = False\n",
    "                    break\n",
    "\n",
    "            if is_diverse:\n",
    "                selected.append(candidate)\n",
    "\n",
    "        return selected[:k]\n",
    "\n",
    "    def _calculate_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Calculate text similarity (simplified implementation)\"\"\"\n",
    "        words1 = set(text1.lower().split())\n",
    "        words2 = set(text2.lower().split())\n",
    "\n",
    "        if not words1 or not words2:\n",
    "            return 0.0\n",
    "\n",
    "        intersection = words1.intersection(words2)\n",
    "        union = words1.union(words2)\n",
    "\n",
    "        return len(intersection) / len(union) if union else 0.0\n",
    "\n",
    "    def retrieve_by_topic(self, topic: str, k: int = 5) -> List[Document]:\n",
    "        \"\"\"Retrieve documents relevant to a specific topic\"\"\"\n",
    "        topic_keywords = {\n",
    "            \"OOP\": [\"ƒë·ªëi t∆∞·ª£ng\", \"class\", \"object\", \"k·∫ø th·ª´a\", \"ƒë√≥ng g√≥i\"],\n",
    "            \"inheritance\": [\"k·∫ø th·ª´a\", \"class cha\", \"class con\", \"extends\"],\n",
    "            \"data structures\": [\"c·∫•u tr√∫c d·ªØ li·ªáu\", \"array\", \"list\", \"stack\", \"queue\"]\n",
    "        }\n",
    "\n",
    "        # Create enhanced query with topic keywords\n",
    "        keywords = topic_keywords.get(topic.lower(), [topic])\n",
    "        enhanced_query = f\"{topic} {' '.join(keywords)}\"\n",
    "\n",
    "        return self.retrieve_diverse_contexts(enhanced_query, k)\n",
    "\n",
    "    def get_context_summary(self, documents: List[Document]) -> str:\n",
    "        \"\"\"Generate a summary of the retrieved contexts\"\"\"\n",
    "        if not documents:\n",
    "            return \"No relevant context found.\"\n",
    "\n",
    "        # Combine and truncate content\n",
    "        combined_content = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "\n",
    "        # Limit context length\n",
    "        max_length = 2000\n",
    "        if len(combined_content) > max_length:\n",
    "            combined_content = combined_content[:max_length] + \"...\"\n",
    "\n",
    "        return combined_content\n",
    "\n",
    "# Initialize the enhanced retriever\n",
    "if vector_manager.vector_db:\n",
    "    retriever = ContextAwareRetriever(\n",
    "        vector_manager.vector_db,\n",
    "        CONFIG[\"diversity_threshold\"]\n",
    "    )\n",
    "    print(\"‚úÖ Context-aware retriever initialized!\")\n",
    "\n",
    "    # Test diverse retrieval\n",
    "    test_topics = [\"OOP\", \"inheritance\", \"data structures\"]\n",
    "\n",
    "    print(\"\\nüß™ Testing diverse retrieval for different topics:\")\n",
    "    for topic in test_topics:\n",
    "        results = retriever.retrieve_by_topic(topic, k=2)\n",
    "        print(f\"\\nüìö Topic: {topic}\")\n",
    "        print(f\"  Retrieved {len(results)} diverse documents\")\n",
    "\n",
    "        for i, doc in enumerate(results, 1):\n",
    "            print(f\"  Doc {i}: {doc.page_content[:100]}...\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Vector database not available for retriever initialization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35c08eb",
   "metadata": {},
   "source": [
    "## 5. MCQ Generation with LLM\n",
    "\n",
    "Now let's implement the question generation system using a Large Language Model with structured JSON output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8118aee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#? pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f73d5941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Initializing LLM: unsloth/Qwen2.5-7B\n",
      "‚ö†Ô∏è  Note: This requires significant memory and may take time...\n",
      "==((====))==  Unsloth 2025.7.9: Fast Qwen2 patching. Transformers: 4.53.2.\n",
      "   \\\\   /|    NVIDIA GeForce GTX 1650. Num GPUs = 1. Max memory: 4.0 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu118. CUDA: 7.5. CUDA Toolkit: 11.8. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136e1d3951214659acfcc947ff46744f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de8583b2a8294071a7f3824365808b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616aaa0b0d93438fb8b4ba1f28aca06a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Failed to initialize LLM: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "üîÑ Using mock LLM for demonstration...\n",
      "üéØ MCQ Generator ready!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MCQGenerator:\n",
    "    \"\"\"Generates MCQs using LLM with structured output\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.llm = None\n",
    "        self.is_initialized = False\n",
    "\n",
    "    def initialize_llm(self, model_name: str = None) -> bool:\n",
    "        \"\"\"Initialize the LLM for question generation\"\"\"\n",
    "        model_name = model_name or CONFIG[\"llm_model\"]\n",
    "\n",
    "        print(f\"ü§ñ Initializing LLM: {model_name}\")\n",
    "        print(\"‚ö†Ô∏è  Note: This requires significant memory and may take time...\")\n",
    "\n",
    "        try:\n",
    "            # For demonstration, we'll use a mock LLM\n",
    "            # In production, uncomment the code below\n",
    "\n",
    "            # Check for HuggingFace token\n",
    "            token_path = Path(\"api_key/hugging_face_token.txt\")\n",
    "            hf_token = None\n",
    "            if token_path.exists():\n",
    "                with token_path.open(\"r\") as f:\n",
    "                    hf_token = f.read().strip()\n",
    "\n",
    "            # Configure quantization for memory efficiency\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                bnb_4bit_quant_type=\"nf4\"\n",
    "            )\n",
    "\n",
    "            # Load model\n",
    "            model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "                model_name,\n",
    "                quantization_config=bnb_config,\n",
    "                low_cpu_mem_usage=True,\n",
    "                device_map=\"auto\",\n",
    "                token=hf_token\n",
    "            )\n",
    "\n",
    "            # tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "            model_pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                max_new_tokens=CONFIG[\"max_tokens\"],\n",
    "                temperature=CONFIG[\"generation_temperature\"],\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "\n",
    "            self.llm = HuggingFacePipeline(pipeline=model_pipeline)\n",
    "\n",
    "            # For demonstration, create a mock LLM\n",
    "            # self.llm = self._create_mock_llm()\n",
    "            self.is_initialized = True\n",
    "\n",
    "            print(\"‚úÖ LLM initialized successfully!\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to initialize LLM: {e}\")\n",
    "            print(\"üîÑ Using mock LLM for demonstration...\")\n",
    "            self.llm = self._create_mock_llm()\n",
    "            self.is_initialized = True\n",
    "            return True\n",
    "\n",
    "    def _create_mock_llm(self):\n",
    "        \"\"\"Create a mock LLM for demonstration purposes\"\"\"\n",
    "        class MockLLM:\n",
    "            def __call__(self, prompt):\n",
    "                # Mock response based on context analysis\n",
    "                if \"OOP\" in prompt or \"ƒë·ªëi t∆∞·ª£ng\" in prompt:\n",
    "                    return '''\n",
    "{\n",
    "    \"question\": \"OOP (Object-Oriented Programming) l√† g√¨?\",\n",
    "    \"options\": {\n",
    "        \"A\": \"M·ªôt m√¥ h√¨nh l·∫≠p tr√¨nh d·ª±a tr√™n kh√°i ni·ªám ƒë·ªëi t∆∞·ª£ng\",\n",
    "        \"B\": \"M·ªôt h·ªá qu·∫£n tr·ªã c∆° s·ªü d·ªØ li·ªáu\",\n",
    "        \"C\": \"M·ªôt framework ph√°t tri·ªÉn web\",\n",
    "        \"D\": \"M·ªôt ph∆∞∆°ng ph√°p ki·ªÉm th·ª≠ ph·∫ßn m·ªÅm\"\n",
    "    },\n",
    "    \"correct_answer\": \"A\",\n",
    "    \"explanation\": \"OOP l√† vi·∫øt t·∫Øt c·ªßa Object-Oriented Programming, m·ªôt m√¥ h√¨nh l·∫≠p tr√¨nh t·ªï ch·ª©c m√£ ngu·ªìn xung quanh c√°c ƒë·ªëi t∆∞·ª£ng thay v√¨ c√°c h√†m v√† logic.\",\n",
    "    \"topic\": \"Programming Fundamentals\",\n",
    "    \"difficulty\": \"medium\",\n",
    "    \"question_type\": \"definition\"\n",
    "}\n",
    "'''\n",
    "                elif \"k·∫ø th·ª´a\" in prompt or \"inheritance\" in prompt:\n",
    "                    return '''\n",
    "{\n",
    "    \"question\": \"Inheritance (K·∫ø th·ª´a) trong OOP c√≥ l·ª£i √≠ch g√¨?\",\n",
    "    \"options\": {\n",
    "        \"A\": \"T√°i s·ª≠ d·ª•ng code v√† d·ªÖ d√†ng m·ªü r·ªông\",\n",
    "        \"B\": \"TƒÉng t·ªëc ƒë·ªô th·ª±c thi ch∆∞∆°ng tr√¨nh\",\n",
    "        \"C\": \"Gi·∫£m dung l∆∞·ª£ng file th·ª±c thi\",\n",
    "        \"D\": \"C·∫£i thi·ªán b·∫£o m·∫≠t c·ªßa ·ª©ng d·ª•ng\"\n",
    "    },\n",
    "    \"correct_answer\": \"A\",\n",
    "    \"explanation\": \"Inheritance cho ph√©p class con k·∫ø th·ª´a thu·ªôc t√≠nh v√† ph∆∞∆°ng th·ª©c t·ª´ class cha, gi√∫p t√°i s·ª≠ d·ª•ng code v√† d·ªÖ d√†ng m·ªü r·ªông t√≠nh nƒÉng.\",\n",
    "    \"topic\": \"OOP Principles\",\n",
    "    \"difficulty\": \"medium\",\n",
    "    \"question_type\": \"application\"\n",
    "}\n",
    "'''\n",
    "                else:\n",
    "                    return '''\n",
    "{\n",
    "    \"question\": \"C·∫•u tr√∫c d·ªØ li·ªáu n√†o ho·∫°t ƒë·ªông theo nguy√™n l√Ω LIFO?\",\n",
    "    \"options\": {\n",
    "        \"A\": \"Queue (H√†ng ƒë·ª£i)\",\n",
    "        \"B\": \"Stack (NgƒÉn x·∫øp)\",\n",
    "        \"C\": \"Array (M·∫£ng)\",\n",
    "        \"D\": \"Linked List (Danh s√°ch li√™n k·∫øt)\"\n",
    "    },\n",
    "    \"correct_answer\": \"B\",\n",
    "    \"explanation\": \"Stack ho·∫°t ƒë·ªông theo nguy√™n l√Ω LIFO (Last In First Out), ph·∫ßn t·ª≠ ƒë∆∞·ª£c th√™m v√†o cu·ªëi c√πng s·∫Ω ƒë∆∞·ª£c l·∫•y ra ƒë·∫ßu ti√™n.\",\n",
    "    \"topic\": \"Data Structures\",\n",
    "    \"difficulty\": \"easy\",\n",
    "    \"question_type\": \"definition\"\n",
    "}\n",
    "'''\n",
    "\n",
    "        return MockLLM()\n",
    "\n",
    "    def generate_mcq_from_context(self, context: str, topic: str,\n",
    "                                  difficulty: DifficultyLevel = DifficultyLevel.MEDIUM,\n",
    "                                  question_type: QuestionType = QuestionType.DEFINITION) -> MCQQuestion:\n",
    "        \"\"\"Generate MCQ from provided context\"\"\"\n",
    "        if not self.is_initialized:\n",
    "            raise RuntimeError(\"LLM not initialized. Call initialize_llm() first.\")\n",
    "\n",
    "        # Create prompt\n",
    "        prompt = self._create_prompt(context, topic, difficulty, question_type)\n",
    "\n",
    "        # Generate response\n",
    "        response = self.llm(prompt)\n",
    "\n",
    "        # Parse JSON response\n",
    "        mcq = self._parse_response(response, context, topic)\n",
    "\n",
    "        return mcq\n",
    "\n",
    "    def _create_prompt(self, context: str, topic: str,\n",
    "                       difficulty: DifficultyLevel, question_type: QuestionType) -> str:\n",
    "        \"\"\"Create structured prompt for MCQ generation\"\"\"\n",
    "\n",
    "        prompt_template = \"\"\"\n",
    "B·∫°n l√† m·ªôt chuy√™n gia gi√°o d·ª•c v√† thi·∫øt k·∫ø c√¢u h·ªèi. Nhi·ªám v·ª• c·ªßa b·∫°n l√† t·∫°o ra m·ªôt c√¢u h·ªèi tr·∫Øc nghi·ªám ch·∫•t l∆∞·ª£ng cao t·ª´ n·ªôi dung ƒë∆∞·ª£c cung c·∫•p.\n",
    "\n",
    "Y√™u c·∫ßu:\n",
    "1. T·∫°o m·ªôt c√¢u h·ªèi r√µ r√†ng, kh√¥ng m∆° h·ªì\n",
    "2. Cung c·∫•p ƒë√∫ng 4 l·ª±a ch·ªçn (A, B, C, D)\n",
    "3. Ch·ªâ c√≥ m·ªôt ƒë√°p √°n ƒë√∫ng\n",
    "4. C√°c ph∆∞∆°ng √°n sai ph·∫£i h·ª£p l√Ω nh∆∞ng r√µ r√†ng l√† sai\n",
    "5. Bao g·ªìm gi·∫£i th√≠ch cho ƒë√°p √°n ƒë√∫ng\n",
    "\n",
    "N·ªôi dung: {context}\n",
    "Ch·ªß ƒë·ªÅ: {topic}\n",
    "M·ª©c ƒë·ªô kh√≥: {difficulty}\n",
    "Lo·∫°i c√¢u h·ªèi: {question_type}\n",
    "\n",
    "Tr·∫£ v·ªÅ ch·ªâ d∆∞·ªõi d·∫°ng JSON h·ª£p l·ªá v·ªõi c·∫•u tr√∫c sau:\n",
    "{{\n",
    "    \"question\": \"C√¢u h·ªèi c·ªßa b·∫°n\",\n",
    "    \"options\": {{\n",
    "        \"A\": \"L·ª±a ch·ªçn A\",\n",
    "        \"B\": \"L·ª±a ch·ªçn B\",\n",
    "        \"C\": \"L·ª±a ch·ªçn C\",\n",
    "        \"D\": \"L·ª±a ch·ªçn D\"\n",
    "    }},\n",
    "    \"correct_answer\": \"A\",\n",
    "    \"explanation\": \"Gi·∫£i th√≠ch chi ti·∫øt\",\n",
    "    \"topic\": \"{topic}\",\n",
    "    \"difficulty\": \"{difficulty}\",\n",
    "    \"question_type\": \"{question_type}\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "        return prompt_template.format(\n",
    "            context=context[:1500],  # Limit context length\n",
    "            topic=topic,\n",
    "            difficulty=difficulty.value,\n",
    "            question_type=question_type.value\n",
    "        )\n",
    "\n",
    "    def _parse_response(self, response: str, context: str, topic: str) -> MCQQuestion:\n",
    "        \"\"\"Parse LLM response and create MCQQuestion object\"\"\"\n",
    "        try:\n",
    "            # Extract JSON from response\n",
    "            json_start = response.rfind(\"{\")\n",
    "            json_end = response.rfind(\"}\") + 1\n",
    "\n",
    "            if json_start == -1 or json_end == 0:\n",
    "                raise ValueError(\"No JSON found in response\")\n",
    "\n",
    "            json_text = response[json_start:json_end]\n",
    "            response_data = json.loads(json_text)\n",
    "\n",
    "            # Create MCQ options\n",
    "            options = []\n",
    "            for label, text in response_data[\"options\"].items():\n",
    "                is_correct = label == response_data[\"correct_answer\"]\n",
    "                options.append(MCQOption(label, text, is_correct))\n",
    "\n",
    "            # Create MCQ object\n",
    "            mcq = MCQQuestion(\n",
    "                question=response_data[\"question\"],\n",
    "                context=context[:500] + \"...\" if len(context) > 500 else context,\n",
    "                options=options,\n",
    "                explanation=response_data.get(\"explanation\", \"\"),\n",
    "                difficulty=response_data.get(\"difficulty\", \"medium\"),\n",
    "                topic=topic,\n",
    "                question_type=response_data.get(\"question_type\", \"definition\"),\n",
    "                source=\"Generated from documents\",\n",
    "                confidence_score=0.0  # Will be calculated later\n",
    "            )\n",
    "\n",
    "            return mcq\n",
    "\n",
    "        except (json.JSONDecodeError, KeyError) as e:\n",
    "            raise ValueError(f\"Failed to parse LLM response: {e}\")\n",
    "\n",
    "# Initialize MCQ generator\n",
    "mcq_generator = MCQGenerator()\n",
    "if mcq_generator.initialize_llm():\n",
    "    print(\"üéØ MCQ Generator ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ff0e49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b16bcf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing MCQ generation...\n",
      "üìÑ Context length: 945 characters\n",
      "üìÑ Context preview: \n",
      "        Object-Oriented Programming (OOP) l√† m·ªôt m√¥ h√¨nh l·∫≠p tr√¨nh ƒë∆∞·ª£c x√¢y d·ª±ng d·ª±a tr√™n kh√°i ni·ªám ƒë·ªëi t∆∞·ª£ng. OOP t·ªï ch·ª©c m√£ ngu·ªìn xung quanh c√°c ƒë·ªëi t∆∞·ª£ng thay v√¨ c√°c h√†m v√† logic. C√°c nguy√™n l√Ω c∆°...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to parse LLM response: Extra data: line 6 column 6 (char 214)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 203\u001b[39m, in \u001b[36mMCQGenerator._parse_response\u001b[39m\u001b[34m(self, response, context, topic)\u001b[39m\n\u001b[32m    202\u001b[39m json_text = response[json_start:json_end]\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m response_data = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[38;5;66;03m# Create MCQ options\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\CODE\\IDE\\Anaconda\\envs\\rag-multi-choice\\Lib\\json\\__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\CODE\\IDE\\Anaconda\\envs\\rag-multi-choice\\Lib\\json\\decoder.py:340\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExtra data\u001b[39m\u001b[33m\"\u001b[39m, s, end)\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Extra data: line 6 column 6 (char 214)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìÑ Context preview: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext[:\u001b[32m200\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Generate MCQ\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m mcq = \u001b[43mmcq_generator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_mcq_from_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mObject-Oriented Programming\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifficulty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDifficultyLevel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMEDIUM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquestion_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mQuestionType\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDEFINITION\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Display the generated MCQ\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müéØ Generated MCQ:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 145\u001b[39m, in \u001b[36mMCQGenerator.generate_mcq_from_context\u001b[39m\u001b[34m(self, context, topic, difficulty, question_type)\u001b[39m\n\u001b[32m    142\u001b[39m response = \u001b[38;5;28mself\u001b[39m.llm(prompt)\n\u001b[32m    144\u001b[39m \u001b[38;5;66;03m# Parse JSON response\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m mcq = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parse_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m mcq\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 227\u001b[39m, in \u001b[36mMCQGenerator._parse_response\u001b[39m\u001b[34m(self, response, context, topic)\u001b[39m\n\u001b[32m    224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m mcq\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (json.JSONDecodeError, \u001b[38;5;167;01mKeyError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to parse LLM response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Failed to parse LLM response: Extra data: line 6 column 6 (char 214)"
     ]
    }
   ],
   "source": [
    "# Test single MCQ generation\n",
    "print(\"üß™ Testing MCQ generation...\")\n",
    "\n",
    "# Get context for OOP topic\n",
    "if 'retriever' in locals():\n",
    "    context_docs = retriever.retrieve_by_topic(\"OOP\", k=2)\n",
    "    context = retriever.get_context_summary(context_docs)\n",
    "\n",
    "    print(f\"üìÑ Context length: {len(context)} characters\")\n",
    "    print(f\"üìÑ Context preview: {context[:200]}...\")\n",
    "\n",
    "    # Generate MCQ\n",
    "    mcq = mcq_generator.generate_mcq_from_context(\n",
    "        context=context,\n",
    "        topic=\"Object-Oriented Programming\",\n",
    "        difficulty=DifficultyLevel.MEDIUM,\n",
    "        question_type=QuestionType.DEFINITION\n",
    "    )\n",
    "\n",
    "    # Display the generated MCQ\n",
    "    print(f\"\\nüéØ Generated MCQ:\")\n",
    "    print(f\"Question: {mcq.question}\")\n",
    "    print(f\"\\nOptions:\")\n",
    "    for option in mcq.options:\n",
    "        marker = \"‚úÖ\" if option.is_correct else \"  \"\n",
    "        print(f\"  {marker} {option.label}: {option.text}\")\n",
    "\n",
    "    print(f\"\\nCorrect Answer: {next(opt.label for opt in mcq.options if opt.is_correct)}\")\n",
    "    print(f\"Explanation: {mcq.explanation}\")\n",
    "    print(f\"Topic: {mcq.topic}\")\n",
    "    print(f\"Difficulty: {mcq.difficulty}\")\n",
    "    print(f\"Question Type: {mcq.question_type}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Retriever not available. Cannot test MCQ generation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fcffba",
   "metadata": {},
   "source": [
    "## 6. Prompt Engineering for Different Question Types\n",
    "\n",
    "Let's explore specialized prompts for different types of questions to improve generation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162c2dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedPromptManager:\n",
    "    \"\"\"Manages specialized prompts for different question types\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.templates = self._initialize_templates()\n",
    "\n",
    "    def _initialize_templates(self) -> Dict[str, str]:\n",
    "        \"\"\"Initialize prompt templates for different question types\"\"\"\n",
    "        base_instruction = \"\"\"\n",
    "B·∫°n l√† m·ªôt chuy√™n gia gi√°o d·ª•c v√† thi·∫øt k·∫ø c√¢u h·ªèi. T·∫°o m·ªôt c√¢u h·ªèi tr·∫Øc nghi·ªám ch·∫•t l∆∞·ª£ng cao t·ª´ n·ªôi dung ƒë∆∞·ª£c cung c·∫•p.\n",
    "\n",
    "Y√™u c·∫ßu chung:\n",
    "- C√¢u h·ªèi r√µ r√†ng, kh√¥ng m∆° h·ªì\n",
    "- ƒê√∫ng 4 l·ª±a ch·ªçn (A, B, C, D)\n",
    "- Ch·ªâ m·ªôt ƒë√°p √°n ƒë√∫ng\n",
    "- Ph∆∞∆°ng √°n sai h·ª£p l√Ω nh∆∞ng r√µ r√†ng sai\n",
    "- Gi·∫£i th√≠ch chi ti·∫øt cho ƒë√°p √°n ƒë√∫ng\n",
    "\"\"\"\n",
    "\n",
    "        return {\n",
    "            QuestionType.DEFINITION: f\"\"\"\n",
    "{base_instruction}\n",
    "\n",
    "Y√™u c·∫ßu ƒë·∫∑c bi·ªát cho c√¢u h·ªèi ƒê·ªäNH NGHƒ®A:\n",
    "- T·∫≠p trung v√†o ƒë·ªãnh nghƒ©a ch√≠nh x√°c c·ªßa thu·∫≠t ng·ªØ, kh√°i ni·ªám\n",
    "- C√°c ph∆∞∆°ng √°n sai th∆∞·ªùng l√† ƒë·ªãnh nghƒ©a c·ªßa kh√°i ni·ªám kh√°c ho·∫∑c hi·ªÉu l·∫ßm ph·ªï bi·∫øn\n",
    "- S·ª≠ d·ª•ng ng√¥n ng·ªØ ƒë∆°n gi·∫£n, d·ªÖ hi·ªÉu\n",
    "- V√≠ d·ª•: \"X l√† g√¨?\", \"ƒê·ªãnh nghƒ©a c·ªßa Y l√† g√¨?\"\n",
    "\n",
    "N·ªôi dung: {{context}}\n",
    "Ch·ªß ƒë·ªÅ: {{topic}}\n",
    "M·ª©c ƒë·ªô: {{difficulty}}\n",
    "\n",
    "JSON Output:\n",
    "\"\"\",\n",
    "\n",
    "            QuestionType.APPLICATION: f\"\"\"\n",
    "{base_instruction}\n",
    "\n",
    "Y√™u c·∫ßu ƒë·∫∑c bi·ªát cho c√¢u h·ªèi ·ª®NG D·ª§NG:\n",
    "- T·∫°o t√¨nh hu·ªëng th·ª±c t·∫ø c·∫ßn √°p d·ª•ng ki·∫øn th·ª©c\n",
    "- H·ªèi \"khi n√†o s·ª≠ d·ª•ng\", \"trong tr∆∞·ªùng h·ª£p n√†o\", \"v√≠ d·ª• n√†o\"\n",
    "- C√°c ph∆∞∆°ng √°n sai l√† ·ª©ng d·ª•ng kh√¥ng ph√π h·ª£p ho·∫∑c sai ng·ªØ c·∫£nh\n",
    "- K·∫øt n·ªëi l√Ω thuy·∫øt v·ªõi th·ª±c ti·ªÖn\n",
    "\n",
    "N·ªôi dung: {{context}}\n",
    "Ch·ªß ƒë·ªÅ: {{topic}}\n",
    "M·ª©c ƒë·ªô: {{difficulty}}\n",
    "\n",
    "JSON Output:\n",
    "\"\"\",\n",
    "\n",
    "            QuestionType.COMPARISON: f\"\"\"\n",
    "{base_instruction}\n",
    "\n",
    "Y√™u c·∫ßu ƒë·∫∑c bi·ªát cho c√¢u h·ªèi SO S√ÅNH:\n",
    "- So s√°nh 2-3 kh√°i ni·ªám, ph∆∞∆°ng ph√°p, k·ªπ thu·∫≠t\n",
    "- T·∫≠p trung v√†o ƒëi·ªÉm kh√°c bi·ªát ho·∫∑c gi·ªëng nhau ch√≠nh\n",
    "- C√°c ph∆∞∆°ng √°n sai th∆∞·ªùng ƒë·∫£o ng∆∞·ª£c ƒë·∫∑c ƒëi·ªÉm ho·∫∑c nh·∫ßm l·∫´n\n",
    "- V√≠ d·ª•: \"Kh√°c bi·ªát gi·ªØa X v√† Y l√† g√¨?\"\n",
    "\n",
    "N·ªôi dung: {{context}}\n",
    "Ch·ªß ƒë·ªÅ: {{topic}}\n",
    "M·ª©c ƒë·ªô: {{difficulty}}\n",
    "\n",
    "JSON Output:\n",
    "\"\"\",\n",
    "\n",
    "            QuestionType.ANALYSIS: f\"\"\"\n",
    "{base_instruction}\n",
    "\n",
    "Y√™u c·∫ßu ƒë·∫∑c bi·ªát cho c√¢u h·ªèi PH√ÇN T√çCH:\n",
    "- Y√™u c·∫ßu ph√¢n t√≠ch code, s∆° ƒë·ªì, ho·∫∑c t√¨nh hu·ªëng ph·ª©c t·∫°p\n",
    "- Ki·ªÉm tra t∆∞ duy logic v√† kh·∫£ nƒÉng suy lu·∫≠n\n",
    "- C√¢u h·ªèi c√≥ th·ªÉ c√≥ nhi·ªÅu b∆∞·ªõc suy lu·∫≠n\n",
    "- C√°c ph∆∞∆°ng √°n sai l√† k·∫øt lu·∫≠n sai ho·∫∑c thi·∫øu logic\n",
    "\n",
    "N·ªôi dung: {{context}}\n",
    "Ch·ªß ƒë·ªÅ: {{topic}}\n",
    "M·ª©c ƒë·ªô: {{difficulty}}\n",
    "\n",
    "JSON Output:\n",
    "\"\"\",\n",
    "\n",
    "            QuestionType.EVALUATION: f\"\"\"\n",
    "{base_instruction}\n",
    "\n",
    "Y√™u c·∫ßu ƒë·∫∑c bi·ªát cho c√¢u h·ªèi ƒê√ÅNH GI√Å:\n",
    "- ƒê√°nh gi√° ∆∞u nh∆∞·ª£c ƒëi·ªÉm, hi·ªáu qu·∫£, ph√π h·ª£p\n",
    "- C√¢u h·ªèi d·∫°ng \"ph∆∞∆°ng ph√°p n√†o t·ªët nh·∫•t\", \"khi n√†o n√™n ch·ªçn\"\n",
    "- Y√™u c·∫ßu c√¢n nh·∫Øc nhi·ªÅu y·∫øu t·ªë\n",
    "- C√°c ph∆∞∆°ng √°n c·∫ßn c√≥ ƒë·ªô h·ª£p l√Ω cao\n",
    "\n",
    "N·ªôi dung: {{context}}\n",
    "Ch·ªß ƒë·ªÅ: {{topic}}\n",
    "M·ª©c ƒë·ªô: {{difficulty}}\n",
    "\n",
    "JSON Output:\n",
    "\"\"\"\n",
    "        }\n",
    "\n",
    "    def get_prompt(self, question_type: QuestionType, context: str,\n",
    "                   topic: str, difficulty: DifficultyLevel) -> str:\n",
    "        \"\"\"Get specialized prompt for question type\"\"\"\n",
    "        template = self.templates.get(question_type, self.templates[QuestionType.DEFINITION])\n",
    "\n",
    "        return template.format(\n",
    "            context=context[:1200],  # Limit context length\n",
    "            topic=topic,\n",
    "            difficulty=difficulty.value\n",
    "        )\n",
    "\n",
    "    def generate_examples(self) -> Dict[QuestionType, str]:\n",
    "        \"\"\"Generate example questions for each type\"\"\"\n",
    "        examples = {\n",
    "            QuestionType.DEFINITION: \"\"\"\n",
    "V√≠ d·ª• c√¢u h·ªèi ƒë·ªãnh nghƒ©a:\n",
    "\"Encapsulation (ƒê√≥ng g√≥i) trong OOP l√† g√¨?\"\n",
    "A) ·∫®n gi·∫•u chi ti·∫øt tri·ªÉn khai v√† ch·ªâ ƒë·ªÉ l·ªô interface c·∫ßn thi·∫øt ‚úÖ\n",
    "B) K·∫ø th·ª´a thu·ªôc t√≠nh t·ª´ class cha\n",
    "C) T·∫°o nhi·ªÅu h√¨nh th·ª©c kh√°c nhau c·ªßa c√πng m·ªôt ph∆∞∆°ng th·ª©c\n",
    "D) T·ªï ch·ª©c code th√†nh c√°c module ri√™ng bi·ªát\n",
    "\"\"\",\n",
    "\n",
    "            QuestionType.APPLICATION: \"\"\"\n",
    "V√≠ d·ª• c√¢u h·ªèi ·ª©ng d·ª•ng:\n",
    "\"Trong tr∆∞·ªùng h·ª£p n√†o n√™n s·ª≠ d·ª•ng Stack?\"\n",
    "A) Khi c·∫ßn truy c·∫≠p ng·∫´u nhi√™n v√†o c√°c ph·∫ßn t·ª≠\n",
    "B) Khi c·∫ßn x·ª≠ l√Ω theo th·ª© t·ª± LIFO (Last In First Out) ‚úÖ\n",
    "C) Khi c·∫ßn s·∫Øp x·∫øp d·ªØ li·ªáu t·ª± ƒë·ªông\n",
    "D) Khi c·∫ßn chia s·∫ª d·ªØ li·ªáu gi·ªØa nhi·ªÅu thread\n",
    "\"\"\",\n",
    "\n",
    "            QuestionType.COMPARISON: \"\"\"\n",
    "V√≠ d·ª• c√¢u h·ªèi so s√°nh:\n",
    "\"Kh√°c bi·ªát ch√≠nh gi·ªØa Array v√† Linked List l√† g√¨?\"\n",
    "A) Array cho ph√©p truy c·∫≠p ng·∫´u nhi√™n, Linked List truy c·∫≠p tu·∫ßn t·ª± ‚úÖ\n",
    "B) Array ch·ªâ l∆∞u s·ªë, Linked List l∆∞u m·ªçi ki·ªÉu d·ªØ li·ªáu\n",
    "C) Array kh√¥ng th·ªÉ thay ƒë·ªïi k√≠ch th∆∞·ªõc, Linked List c√≥ th·ªÉ\n",
    "D) Array nhanh h∆°n trong m·ªçi tr∆∞·ªùng h·ª£p\n",
    "\"\"\",\n",
    "\n",
    "            QuestionType.ANALYSIS: \"\"\"\n",
    "V√≠ d·ª• c√¢u h·ªèi ph√¢n t√≠ch:\n",
    "\"ƒêo·∫°n code sau vi ph·∫°m nguy√™n l√Ω OOP n√†o?\n",
    "class Bird:\n",
    "    def fly(self): pass\n",
    "class Penguin(Bird):\n",
    "    def fly(self): raise Exception('Cannot fly')\"\n",
    "\n",
    "A) Encapsulation\n",
    "B) Liskov Substitution Principle ‚úÖ\n",
    "C) Single Responsibility\n",
    "D) Open/Closed Principle\n",
    "\"\"\",\n",
    "\n",
    "            QuestionType.EVALUATION: \"\"\"\n",
    "V√≠ d·ª• c√¢u h·ªèi ƒë√°nh gi√°:\n",
    "\"Khi n√†o n√™n ch·ªçn Composition thay v√¨ Inheritance?\"\n",
    "A) Khi mu·ªën m·ªëi quan h·ªá \"is-a\" r√µ r√†ng\n",
    "B) Khi c·∫ßn flexibility v√† tr√°nh tight coupling ‚úÖ\n",
    "C) Khi mu·ªën ti·∫øt ki·ªám memory\n",
    "D) Khi class cha c√≥ √≠t ph∆∞∆°ng th·ª©c\n",
    "\"\"\"\n",
    "        }\n",
    "\n",
    "        return examples\n",
    "\n",
    "# Initialize advanced prompt manager\n",
    "prompt_manager = AdvancedPromptManager()\n",
    "\n",
    "# Display examples\n",
    "print(\"üìù Prompt Engineering Examples:\")\n",
    "examples = prompt_manager.generate_examples()\n",
    "\n",
    "for q_type, example in examples.items():\n",
    "    print(f\"\\n{q_type.value.upper()} Questions:\")\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cf43a1",
   "metadata": {},
   "source": [
    "## 7. Quality Validation System\n",
    "\n",
    "Implementing automatic quality checks to ensure generated MCQs meet educational standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db723245",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QualityValidator:\n",
    "    \"\"\"Comprehensive quality validation for MCQs\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.min_question_length = 10\n",
    "        self.max_question_length = 200\n",
    "        self.min_explanation_length = 20\n",
    "        self.min_option_length = 5\n",
    "        self.max_option_length = 150\n",
    "\n",
    "    def validate_mcq(self, mcq: MCQQuestion) -> Tuple[bool, Dict[str, Any]]:\n",
    "        \"\"\"Comprehensive MCQ validation with detailed feedback\"\"\"\n",
    "        results = {\n",
    "            \"is_valid\": True,\n",
    "            \"issues\": [],\n",
    "            \"warnings\": [],\n",
    "            \"scores\": {}\n",
    "        }\n",
    "\n",
    "        # Check basic structure\n",
    "        structure_score = self._check_structure(mcq, results)\n",
    "        results[\"scores\"][\"structure\"] = structure_score\n",
    "\n",
    "        # Check content quality\n",
    "        content_score = self._check_content_quality(mcq, results)\n",
    "        results[\"scores\"][\"content\"] = content_score\n",
    "\n",
    "        # Check distractor quality\n",
    "        distractor_score = self._check_distractor_quality(mcq, results)\n",
    "        results[\"scores\"][\"distractors\"] = distractor_score\n",
    "\n",
    "        # Check language quality\n",
    "        language_score = self._check_language_quality(mcq, results)\n",
    "        results[\"scores\"][\"language\"] = language_score\n",
    "\n",
    "        # Overall validation\n",
    "        results[\"is_valid\"] = len(results[\"issues\"]) == 0\n",
    "        results[\"overall_score\"] = np.mean(list(results[\"scores\"].values()))\n",
    "\n",
    "        return results[\"is_valid\"], results\n",
    "\n",
    "    def _check_structure(self, mcq: MCQQuestion, results: Dict) -> float:\n",
    "        \"\"\"Check MCQ structural requirements\"\"\"\n",
    "        score = 100.0\n",
    "\n",
    "        # Check question length\n",
    "        if len(mcq.question) < self.min_question_length:\n",
    "            results[\"issues\"].append(\"Question too short\")\n",
    "            score -= 20\n",
    "        elif len(mcq.question) > self.max_question_length:\n",
    "            results[\"warnings\"].append(\"Question might be too long\")\n",
    "            score -= 10\n",
    "\n",
    "        # Check options count\n",
    "        if len(mcq.options) != 4:\n",
    "            results[\"issues\"].append(f\"Must have exactly 4 options, found {len(mcq.options)}\")\n",
    "            score -= 30\n",
    "\n",
    "        # Check for single correct answer\n",
    "        correct_count = sum(1 for opt in mcq.options if opt.is_correct)\n",
    "        if correct_count != 1:\n",
    "            results[\"issues\"].append(f\"Must have exactly 1 correct answer, found {correct_count}\")\n",
    "            score -= 40\n",
    "\n",
    "        # Check explanation\n",
    "        if len(mcq.explanation) < self.min_explanation_length:\n",
    "            results[\"issues\"].append(\"Explanation too short\")\n",
    "            score -= 15\n",
    "\n",
    "        return max(score, 0)\n",
    "\n",
    "    def _check_content_quality(self, mcq: MCQQuestion, results: Dict) -> float:\n",
    "        \"\"\"Check content quality and relevance\"\"\"\n",
    "        score = 100.0\n",
    "\n",
    "        # Check for distinct options\n",
    "        option_texts = [opt.text.lower().strip() for opt in mcq.options]\n",
    "        if len(set(option_texts)) != len(option_texts):\n",
    "            results[\"issues\"].append(\"Options must be distinct\")\n",
    "            score -= 25\n",
    "\n",
    "        # Check option length consistency\n",
    "        option_lengths = [len(opt.text) for opt in mcq.options]\n",
    "        length_variance = np.var(option_lengths)\n",
    "        if length_variance > 1000:  # High variance in option lengths\n",
    "            results[\"warnings\"].append(\"Large variation in option lengths\")\n",
    "            score -= 10\n",
    "\n",
    "        # Check for obvious patterns\n",
    "        labels = [opt.label for opt in mcq.options]\n",
    "        if not labels == [\"A\", \"B\", \"C\", \"D\"]:\n",
    "            results[\"issues\"].append(\"Options must be labeled A, B, C, D\")\n",
    "            score -= 15\n",
    "\n",
    "        return max(score, 0)\n",
    "\n",
    "    def _check_distractor_quality(self, mcq: MCQQuestion, results: Dict) -> float:\n",
    "        \"\"\"Check quality of incorrect options (distractors)\"\"\"\n",
    "        score = 100.0\n",
    "\n",
    "        distractors = [opt for opt in mcq.options if not opt.is_correct]\n",
    "\n",
    "        # Check distractor plausibility (simplified check)\n",
    "        for i, distractor in enumerate(distractors):\n",
    "            if len(distractor.text) < self.min_option_length:\n",
    "                results[\"warnings\"].append(f\"Distractor {distractor.label} too short\")\n",
    "                score -= 5\n",
    "\n",
    "            # Check for obviously wrong answers (very simple check)\n",
    "            if any(word in distractor.text.lower() for word in [\"kh√¥ng\", \"never\", \"impossible\"]):\n",
    "                results[\"warnings\"].append(f\"Distractor {distractor.label} might be too obviously wrong\")\n",
    "                score -= 10\n",
    "\n",
    "        return max(score, 0)\n",
    "\n",
    "    def _check_language_quality(self, mcq: MCQQuestion, results: Dict) -> float:\n",
    "        \"\"\"Check language quality and clarity\"\"\"\n",
    "        score = 100.0\n",
    "\n",
    "        # Check for common issues\n",
    "        text_to_check = mcq.question + \" \" + \" \".join(opt.text for opt in mcq.options)\n",
    "\n",
    "        # Check for excessive repetition\n",
    "        words = text_to_check.lower().split()\n",
    "        word_freq = {}\n",
    "        for word in words:\n",
    "            if len(word) > 3:  # Only check longer words\n",
    "                word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "        repeated_words = [word for word, freq in word_freq.items() if freq > 3]\n",
    "        if repeated_words:\n",
    "            results[\"warnings\"].append(f\"Repeated words detected: {repeated_words[:3]}\")\n",
    "            score -= 5\n",
    "\n",
    "        # Check for question clarity indicators\n",
    "        if not mcq.question.strip().endswith(\"?\"):\n",
    "            results[\"warnings\"].append(\"Question should end with question mark\")\n",
    "            score -= 5\n",
    "\n",
    "        return max(score, 0)\n",
    "\n",
    "    def calculate_confidence_score(self, mcq: MCQQuestion) -> float:\n",
    "        \"\"\"Calculate overall confidence score for the MCQ\"\"\"\n",
    "        is_valid, validation_results = self.validate_mcq(mcq)\n",
    "\n",
    "        if not is_valid:\n",
    "            return 0.0\n",
    "\n",
    "        # Base score from validation\n",
    "        base_score = validation_results[\"overall_score\"]\n",
    "\n",
    "        # Bonus factors\n",
    "        bonus = 0\n",
    "\n",
    "        # Good explanation length\n",
    "        if 50 <= len(mcq.explanation) <= 200:\n",
    "            bonus += 5\n",
    "\n",
    "        # Balanced option lengths\n",
    "        option_lengths = [len(opt.text) for opt in mcq.options]\n",
    "        if max(option_lengths) - min(option_lengths) < 30:\n",
    "            bonus += 5\n",
    "\n",
    "        # Appropriate question length\n",
    "        if 30 <= len(mcq.question) <= 120:\n",
    "            bonus += 5\n",
    "\n",
    "        final_score = min(base_score + bonus, 100.0)\n",
    "        return final_score\n",
    "\n",
    "    def generate_quality_report(self, mcqs: List[MCQQuestion]) -> Dict[str, Any]:\n",
    "        \"\"\"Generate comprehensive quality report for multiple MCQs\"\"\"\n",
    "        if not mcqs:\n",
    "            return {\"error\": \"No MCQs provided\"}\n",
    "\n",
    "        report = {\n",
    "            \"total_questions\": len(mcqs),\n",
    "            \"valid_questions\": 0,\n",
    "            \"average_score\": 0.0,\n",
    "            \"score_distribution\": {},\n",
    "            \"common_issues\": {},\n",
    "            \"recommendations\": []\n",
    "        }\n",
    "\n",
    "        scores = []\n",
    "        all_issues = []\n",
    "\n",
    "        for mcq in mcqs:\n",
    "            is_valid, validation = self.validate_mcq(mcq)\n",
    "            score = self.calculate_confidence_score(mcq)\n",
    "\n",
    "            if is_valid:\n",
    "                report[\"valid_questions\"] += 1\n",
    "\n",
    "            scores.append(score)\n",
    "            all_issues.extend(validation[\"issues\"])\n",
    "\n",
    "        # Calculate statistics\n",
    "        report[\"average_score\"] = np.mean(scores)\n",
    "        report[\"median_score\"] = np.median(scores)\n",
    "        report[\"min_score\"] = np.min(scores)\n",
    "        report[\"max_score\"] = np.max(scores)\n",
    "\n",
    "        # Score distribution\n",
    "        score_ranges = [(0, 40), (40, 60), (60, 80), (80, 100)]\n",
    "        for low, high in score_ranges:\n",
    "            count = sum(1 for s in scores if low <= s < high)\n",
    "            report[\"score_distribution\"][f\"{low}-{high}\"] = count\n",
    "\n",
    "        # Common issues\n",
    "        issue_counts = {}\n",
    "        for issue in all_issues:\n",
    "            issue_counts[issue] = issue_counts.get(issue, 0) + 1\n",
    "        report[\"common_issues\"] = dict(sorted(issue_counts.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "        # Generate recommendations\n",
    "        if report[\"average_score\"] < 70:\n",
    "            report[\"recommendations\"].append(\"Consider improving prompt engineering\")\n",
    "        if report[\"valid_questions\"] / report[\"total_questions\"] < 0.8:\n",
    "            report[\"recommendations\"].append(\"Review structural validation rules\")\n",
    "        if \"Options must be distinct\" in report[\"common_issues\"]:\n",
    "            report[\"recommendations\"].append(\"Improve distractor generation\")\n",
    "\n",
    "        return report\n",
    "\n",
    "# Initialize quality validator\n",
    "quality_validator = QualityValidator()\n",
    "\n",
    "# Test validation with the previously generated MCQ\n",
    "if 'mcq' in locals():\n",
    "    print(\"üß™ Testing Quality Validation...\")\n",
    "\n",
    "    is_valid, validation_results = quality_validator.validate_mcq(mcq)\n",
    "    confidence_score = quality_validator.calculate_confidence_score(mcq)\n",
    "\n",
    "    print(f\"\\nüìä Validation Results:\")\n",
    "    print(f\"Valid: {'‚úÖ' if is_valid else '‚ùå'}\")\n",
    "    print(f\"Overall Score: {validation_results['overall_score']:.1f}/100\")\n",
    "    print(f\"Confidence Score: {confidence_score:.1f}/100\")\n",
    "\n",
    "    print(f\"\\nüìã Detailed Scores:\")\n",
    "    for category, score in validation_results['scores'].items():\n",
    "        print(f\"  {category.title()}: {score:.1f}/100\")\n",
    "\n",
    "    if validation_results['issues']:\n",
    "        print(f\"\\n‚ùå Issues found:\")\n",
    "        for issue in validation_results['issues']:\n",
    "            print(f\"  - {issue}\")\n",
    "\n",
    "    if validation_results['warnings']:\n",
    "        print(f\"\\n‚ö†Ô∏è  Warnings:\")\n",
    "        for warning in validation_results['warnings']:\n",
    "            print(f\"  - {warning}\")\n",
    "\n",
    "    # Update MCQ confidence score\n",
    "    mcq.confidence_score = confidence_score\n",
    "    print(f\"\\n‚úÖ MCQ confidence score updated to {confidence_score:.1f}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No MCQ available for validation testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9854fb50",
   "metadata": {},
   "source": [
    "## 8. Difficulty Assessment and Classification\n",
    "\n",
    "Implementing intelligent difficulty assessment based on cognitive load and concept complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db13318",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifficultyAnalyzer:\n",
    "    \"\"\"Analyzes and classifies question difficulty\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.difficulty_indicators = {\n",
    "            DifficultyLevel.EASY: {\n",
    "                \"keywords\": [\"l√† g√¨\", \"ƒë·ªãnh nghƒ©a\", \"v√≠ d·ª•\", \"ƒë∆°n gi·∫£n\", \"c∆° b·∫£n\"],\n",
    "                \"concepts\": 1,\n",
    "                \"cognitive_load\": \"recall\",\n",
    "                \"max_word_count\": 15\n",
    "            },\n",
    "            DifficultyLevel.MEDIUM: {\n",
    "                \"keywords\": [\"so s√°nh\", \"kh√°c bi·ªát\", \"·ª©ng d·ª•ng\", \"khi n√†o\", \"t·∫°i sao\"],\n",
    "                \"concepts\": 2,\n",
    "                \"cognitive_load\": \"comprehension\",\n",
    "                \"max_word_count\": 25\n",
    "            },\n",
    "            DifficultyLevel.HARD: {\n",
    "                \"keywords\": [\"ph√¢n t√≠ch\", \"ƒë√°nh gi√°\", \"t·ªëi ∆∞u\", \"thi·∫øt k·∫ø\", \"gi·∫£i th√≠ch\"],\n",
    "                \"concepts\": 3,\n",
    "                \"cognitive_load\": \"analysis\",\n",
    "                \"max_word_count\": 35\n",
    "            },\n",
    "            DifficultyLevel.EXPERT: {\n",
    "                \"keywords\": [\"t·ªïng h·ª£p\", \"s√°ng t·∫°o\", \"nghi√™n c·ª©u\", \"ph√°t tri·ªÉn\", \"optimization\"],\n",
    "                \"concepts\": 4,\n",
    "                \"cognitive_load\": \"synthesis\",\n",
    "                \"max_word_count\": 50\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Technical term complexity levels\n",
    "        self.technical_terms = {\n",
    "            \"basic\": [\"ƒë·ªëi t∆∞·ª£ng\", \"class\", \"function\", \"variable\"],\n",
    "            \"intermediate\": [\"inheritance\", \"polymorphism\", \"encapsulation\", \"abstraction\"],\n",
    "            \"advanced\": [\"design pattern\", \"algorithm complexity\", \"data structure optimization\"],\n",
    "            \"expert\": [\"architectural pattern\", \"performance tuning\", \"scalability analysis\"]\n",
    "        }\n",
    "\n",
    "    def assess_difficulty(self, mcq: MCQQuestion) -> Dict[str, Any]:\n",
    "        \"\"\"Comprehensive difficulty assessment\"\"\"\n",
    "        analysis = {\n",
    "            \"predicted_difficulty\": DifficultyLevel.MEDIUM,\n",
    "            \"confidence\": 0.0,\n",
    "            \"factors\": {},\n",
    "            \"recommendations\": []\n",
    "        }\n",
    "\n",
    "        # Analyze different factors\n",
    "        keyword_score = self._analyze_keywords(mcq.question)\n",
    "        complexity_score = self._analyze_complexity(mcq)\n",
    "        cognitive_score = self._analyze_cognitive_load(mcq)\n",
    "        technical_score = self._analyze_technical_terms(mcq)\n",
    "\n",
    "        analysis[\"factors\"] = {\n",
    "            \"keyword_difficulty\": keyword_score,\n",
    "            \"content_complexity\": complexity_score,\n",
    "            \"cognitive_load\": cognitive_score,\n",
    "            \"technical_complexity\": technical_score\n",
    "        }\n",
    "\n",
    "        # Calculate overall difficulty\n",
    "        overall_score = np.mean([keyword_score, complexity_score, cognitive_score, technical_score])\n",
    "        analysis[\"predicted_difficulty\"] = self._score_to_difficulty(overall_score)\n",
    "        analysis[\"confidence\"] = min(100, max(50, 80 + (overall_score - 50) * 0.4))\n",
    "\n",
    "        # Generate recommendations\n",
    "        analysis[\"recommendations\"] = self._generate_recommendations(analysis)\n",
    "\n",
    "        return analysis\n",
    "\n",
    "    def _analyze_keywords(self, question: str) -> float:\n",
    "        \"\"\"Analyze question keywords for difficulty indicators\"\"\"\n",
    "        question_lower = question.lower()\n",
    "        scores = []\n",
    "\n",
    "        for difficulty, indicators in self.difficulty_indicators.items():\n",
    "            score = sum(2 if keyword in question_lower else 0\n",
    "                       for keyword in indicators[\"keywords\"])\n",
    "            if score > 0:\n",
    "                scores.append((difficulty, score))\n",
    "\n",
    "        if not scores:\n",
    "            return 50.0  # Default medium difficulty\n",
    "\n",
    "        # Weight by difficulty level\n",
    "        difficulty_weights = {\n",
    "            DifficultyLevel.EASY: 25,\n",
    "            DifficultyLevel.MEDIUM: 50,\n",
    "            DifficultyLevel.HARD: 75,\n",
    "            DifficultyLevel.EXPERT: 90\n",
    "        }\n",
    "\n",
    "        weighted_score = sum(difficulty_weights[diff] * score for diff, score in scores)\n",
    "        total_weight = sum(score for _, score in scores)\n",
    "\n",
    "        return weighted_score / total_weight if total_weight > 0 else 50.0\n",
    "\n",
    "    def _analyze_complexity(self, mcq: MCQQuestion) -> float:\n",
    "        \"\"\"Analyze content complexity\"\"\"\n",
    "        factors = []\n",
    "\n",
    "        # Question length complexity\n",
    "        question_words = len(mcq.question.split())\n",
    "        if question_words <= 10:\n",
    "            factors.append(30)\n",
    "        elif question_words <= 20:\n",
    "            factors.append(50)\n",
    "        elif question_words <= 30:\n",
    "            factors.append(70)\n",
    "        else:\n",
    "            factors.append(85)\n",
    "\n",
    "        # Option complexity\n",
    "        option_lengths = [len(opt.text.split()) for opt in mcq.options]\n",
    "        avg_option_length = np.mean(option_lengths)\n",
    "\n",
    "        if avg_option_length <= 5:\n",
    "            factors.append(35)\n",
    "        elif avg_option_length <= 10:\n",
    "            factors.append(55)\n",
    "        else:\n",
    "            factors.append(75)\n",
    "\n",
    "        # Explanation complexity\n",
    "        explanation_words = len(mcq.explanation.split())\n",
    "        if explanation_words <= 15:\n",
    "            factors.append(40)\n",
    "        elif explanation_words <= 30:\n",
    "            factors.append(60)\n",
    "        else:\n",
    "            factors.append(80)\n",
    "\n",
    "        return np.mean(factors)\n",
    "\n",
    "    def _analyze_cognitive_load(self, mcq: MCQQuestion) -> float:\n",
    "        \"\"\"Analyze cognitive load based on Bloom's taxonomy\"\"\"\n",
    "        cognitive_indicators = {\n",
    "            \"remember\": [\"l√† g√¨\", \"ƒë·ªãnh nghƒ©a\", \"li·ªát k√™\", \"nh·∫≠n di·ªán\"],\n",
    "            \"understand\": [\"gi·∫£i th√≠ch\", \"m√¥ t·∫£\", \"so s√°nh\", \"ph√¢n bi·ªát\"],\n",
    "            \"apply\": [\"s·ª≠ d·ª•ng\", \"√°p d·ª•ng\", \"th·ª±c hi·ªán\", \"gi·∫£i quy·∫øt\"],\n",
    "            \"analyze\": [\"ph√¢n t√≠ch\", \"ph√¢n chia\", \"so s√°nh\", \"ƒë·ªëi chi·∫øu\"],\n",
    "            \"evaluate\": [\"ƒë√°nh gi√°\", \"ph√™ b√¨nh\", \"l·ª±a ch·ªçn\", \"quy·∫øt ƒë·ªãnh\"],\n",
    "            \"create\": [\"t·∫°o ra\", \"thi·∫øt k·∫ø\", \"ph√°t tri·ªÉn\", \"s√°ng t·∫°o\"]\n",
    "        }\n",
    "\n",
    "        cognitive_scores = {\n",
    "            \"remember\": 20,\n",
    "            \"understand\": 35,\n",
    "            \"apply\": 50,\n",
    "            \"analyze\": 70,\n",
    "            \"evaluate\": 85,\n",
    "            \"create\": 95\n",
    "        }\n",
    "\n",
    "        text = (mcq.question + \" \" + mcq.explanation).lower()\n",
    "\n",
    "        detected_levels = []\n",
    "        for level, indicators in cognitive_indicators.items():\n",
    "            if any(indicator in text for indicator in indicators):\n",
    "                detected_levels.append(cognitive_scores[level])\n",
    "\n",
    "        return max(detected_levels) if detected_levels else 50.0\n",
    "\n",
    "    def _analyze_technical_terms(self, mcq: MCQQuestion) -> float:\n",
    "        \"\"\"Analyze technical term complexity\"\"\"\n",
    "        all_text = (mcq.question + \" \" + mcq.explanation + \" \" +\n",
    "                   \" \".join(opt.text for opt in mcq.options)).lower()\n",
    "\n",
    "        complexity_scores = {\n",
    "            \"basic\": 30,\n",
    "            \"intermediate\": 50,\n",
    "            \"advanced\": 75,\n",
    "            \"expert\": 90\n",
    "        }\n",
    "\n",
    "        detected_levels = []\n",
    "        for level, terms in self.technical_terms.items():\n",
    "            if any(term.lower() in all_text for term in terms):\n",
    "                detected_levels.append(complexity_scores[level])\n",
    "\n",
    "        return max(detected_levels) if detected_levels else 40.0\n",
    "\n",
    "    def _score_to_difficulty(self, score: float) -> DifficultyLevel:\n",
    "        \"\"\"Convert numeric score to difficulty level\"\"\"\n",
    "        if score < 35:\n",
    "            return DifficultyLevel.EASY\n",
    "        elif score < 60:\n",
    "            return DifficultyLevel.MEDIUM\n",
    "        elif score < 80:\n",
    "            return DifficultyLevel.HARD\n",
    "        else:\n",
    "            return DifficultyLevel.EXPERT\n",
    "\n",
    "    def _generate_recommendations(self, analysis: Dict) -> List[str]:\n",
    "        \"\"\"Generate recommendations based on difficulty analysis\"\"\"\n",
    "        recommendations = []\n",
    "\n",
    "        predicted = analysis[\"predicted_difficulty\"]\n",
    "        factors = analysis[\"factors\"]\n",
    "\n",
    "        if factors[\"keyword_difficulty\"] < 30:\n",
    "            recommendations.append(\"Consider using more specific terminology\")\n",
    "\n",
    "        if factors[\"content_complexity\"] > 80:\n",
    "            recommendations.append(\"Question might be too complex - consider simplification\")\n",
    "\n",
    "        if factors[\"cognitive_load\"] < 30:\n",
    "            recommendations.append(\"Question tests only basic recall - consider higher-order thinking\")\n",
    "\n",
    "        if analysis[\"confidence\"] < 60:\n",
    "            recommendations.append(\"Difficulty assessment has low confidence - review question design\")\n",
    "\n",
    "        return recommendations\n",
    "\n",
    "    def calibrate_difficulty_distribution(self, mcqs: List[MCQQuestion]) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze difficulty distribution across multiple MCQs\"\"\"\n",
    "        if not mcqs:\n",
    "            return {}\n",
    "\n",
    "        analyses = [self.assess_difficulty(mcq) for mcq in mcqs]\n",
    "\n",
    "        # Count difficulty levels\n",
    "        difficulty_counts = {}\n",
    "        confidence_scores = []\n",
    "\n",
    "        for analysis in analyses:\n",
    "            diff_level = analysis[\"predicted_difficulty\"].value\n",
    "            difficulty_counts[diff_level] = difficulty_counts.get(diff_level, 0) + 1\n",
    "            confidence_scores.append(analysis[\"confidence\"])\n",
    "\n",
    "        # Calculate statistics\n",
    "        total = len(mcqs)\n",
    "        distribution = {level: count/total * 100 for level, count in difficulty_counts.items()}\n",
    "\n",
    "        return {\n",
    "            \"total_questions\": total,\n",
    "            \"difficulty_distribution\": distribution,\n",
    "            \"average_confidence\": np.mean(confidence_scores),\n",
    "            \"recommended_distribution\": {\n",
    "                \"easy\": 30,\n",
    "                \"medium\": 50,\n",
    "                \"hard\": 15,\n",
    "                \"expert\": 5\n",
    "            },\n",
    "            \"needs_rebalancing\": self._check_balance(distribution)\n",
    "        }\n",
    "\n",
    "    def _check_balance(self, distribution: Dict[str, float]) -> bool:\n",
    "        \"\"\"Check if difficulty distribution needs rebalancing\"\"\"\n",
    "        recommended = {\"easy\": 30, \"medium\": 50, \"hard\": 15, \"expert\": 5}\n",
    "\n",
    "        for level, target_percent in recommended.items():\n",
    "            actual_percent = distribution.get(level, 0)\n",
    "            if abs(actual_percent - target_percent) > 20:  # More than 20% deviation\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "# Initialize difficulty analyzer\n",
    "difficulty_analyzer = DifficultyAnalyzer()\n",
    "\n",
    "# Test difficulty analysis\n",
    "if 'mcq' in locals():\n",
    "    print(\"üß™ Testing Difficulty Analysis...\")\n",
    "\n",
    "    analysis = difficulty_analyzer.assess_difficulty(mcq)\n",
    "\n",
    "    print(f\"\\nüìä Difficulty Analysis Results:\")\n",
    "    print(f\"Predicted Difficulty: {analysis['predicted_difficulty'].value.upper()}\")\n",
    "    print(f\"Confidence: {analysis['confidence']:.1f}%\")\n",
    "\n",
    "    print(f\"\\nüìã Factor Analysis:\")\n",
    "    for factor, score in analysis['factors'].items():\n",
    "        print(f\"  {factor.replace('_', ' ').title()}: {score:.1f}/100\")\n",
    "\n",
    "    if analysis['recommendations']:\n",
    "        print(f\"\\nüí° Recommendations:\")\n",
    "        for rec in analysis['recommendations']:\n",
    "            print(f\"  ‚Ä¢ {rec}\")\n",
    "\n",
    "    # Compare with intended difficulty\n",
    "    intended_difficulty = mcq.difficulty\n",
    "    predicted_difficulty = analysis['predicted_difficulty'].value\n",
    "\n",
    "    if intended_difficulty == predicted_difficulty:\n",
    "        print(f\"\\n‚úÖ Difficulty assessment matches intended level: {intended_difficulty}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  Difficulty mismatch:\")\n",
    "        print(f\"   Intended: {intended_difficulty}\")\n",
    "        print(f\"   Predicted: {predicted_difficulty}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No MCQ available for difficulty analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6aafc9",
   "metadata": {},
   "source": [
    "## 9. Batch Generation and Testing\n",
    "\n",
    "Implementing scalable batch processing for generating multiple MCQs with error handling and retry mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9e0ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchMCQGenerator:\n",
    "    \"\"\"Batch processing for MCQ generation with error handling\"\"\"\n",
    "\n",
    "    def __init__(self, mcq_generator: MCQGenerator, retriever: ContextAwareRetriever,\n",
    "                 quality_validator: QualityValidator, difficulty_analyzer: DifficultyAnalyzer):\n",
    "        self.mcq_generator = mcq_generator\n",
    "        self.retriever = retriever\n",
    "        self.quality_validator = quality_validator\n",
    "        self.difficulty_analyzer = difficulty_analyzer\n",
    "        self.max_retries = 3\n",
    "        self.min_quality_score = 60.0\n",
    "\n",
    "    def generate_batch(self, topics: List[str],\n",
    "                      count_per_topic: int = 3,\n",
    "                      difficulties: Optional[List[DifficultyLevel]] = None,\n",
    "                      question_types: Optional[List[QuestionType]] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Generate batch of MCQs with comprehensive reporting\"\"\"\n",
    "\n",
    "        if difficulties is None:\n",
    "            difficulties = [DifficultyLevel.EASY, DifficultyLevel.MEDIUM, DifficultyLevel.HARD]\n",
    "\n",
    "        if question_types is None:\n",
    "            question_types = [QuestionType.DEFINITION, QuestionType.APPLICATION]\n",
    "\n",
    "        total_target = len(topics) * count_per_topic\n",
    "        results = {\n",
    "            \"mcqs\": [],\n",
    "            \"failed_generations\": [],\n",
    "            \"statistics\": {},\n",
    "            \"quality_report\": {},\n",
    "            \"difficulty_analysis\": {}\n",
    "        }\n",
    "\n",
    "        print(f\"üöÄ Starting batch generation...\")\n",
    "        print(f\"üìä Target: {total_target} MCQs across {len(topics)} topics\")\n",
    "        print(f\"üéØ Difficulties: {[d.value for d in difficulties]}\")\n",
    "        print(f\"üìù Question types: {[q.value for q in question_types]}\")\n",
    "\n",
    "        # Generate MCQs for each topic\n",
    "        for topic_idx, topic in enumerate(topics, 1):\n",
    "            print(f\"\\\\nüìö Processing topic {topic_idx}/{len(topics)}: {topic}\")\n",
    "\n",
    "            topic_mcqs = []\n",
    "            topic_failures = []\n",
    "\n",
    "            for q_idx in range(count_per_topic):\n",
    "                # Cycle through difficulties and question types\n",
    "                difficulty = difficulties[q_idx % len(difficulties)]\n",
    "                question_type = question_types[q_idx % len(question_types)]\n",
    "\n",
    "                print(f\"  üéØ Generating Q{q_idx+1}: {difficulty.value} {question_type.value}\")\n",
    "\n",
    "                mcq, error = self._generate_single_mcq_with_retry(\n",
    "                    topic, difficulty, question_type\n",
    "                )\n",
    "\n",
    "                if mcq:\n",
    "                    topic_mcqs.append(mcq)\n",
    "                    quality_score = mcq.confidence_score\n",
    "                    print(f\"    ‚úÖ Success (Quality: {quality_score:.1f})\")\n",
    "                else:\n",
    "                    topic_failures.append({\n",
    "                        \"topic\": topic,\n",
    "                        \"difficulty\": difficulty.value,\n",
    "                        \"question_type\": question_type.value,\n",
    "                        \"error\": error\n",
    "                    })\n",
    "                    print(f\"    ‚ùå Failed: {error}\")\n",
    "\n",
    "            results[\"mcqs\"].extend(topic_mcqs)\n",
    "            results[\"failed_generations\"].extend(topic_failures)\n",
    "\n",
    "            print(f\"  üìä Topic summary: {len(topic_mcqs)}/{count_per_topic} successful\")\n",
    "\n",
    "        # Generate comprehensive statistics\n",
    "        results[\"statistics\"] = self._calculate_statistics(results[\"mcqs\"], total_target)\n",
    "        results[\"quality_report\"] = self.quality_validator.generate_quality_report(results[\"mcqs\"])\n",
    "        results[\"difficulty_analysis\"] = self.difficulty_analyzer.calibrate_difficulty_distribution(results[\"mcqs\"])\n",
    "\n",
    "        self._print_batch_summary(results)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _generate_single_mcq_with_retry(self, topic: str, difficulty: DifficultyLevel,\n",
    "                                       question_type: QuestionType) -> Tuple[Optional[MCQQuestion], Optional[str]]:\n",
    "        \"\"\"Generate single MCQ with retry mechanism\"\"\"\n",
    "\n",
    "        for attempt in range(1, self.max_retries + 1):\n",
    "            try:\n",
    "                # Retrieve context\n",
    "                context_docs = self.retriever.retrieve_by_topic(topic, k=3)\n",
    "                if not context_docs:\n",
    "                    return None, f\"No relevant context found for topic: {topic}\"\n",
    "\n",
    "                context = self.retriever.get_context_summary(context_docs)\n",
    "\n",
    "                # Generate MCQ\n",
    "                mcq = self.mcq_generator.generate_mcq_from_context(\n",
    "                    context, topic, difficulty, question_type\n",
    "                )\n",
    "\n",
    "                # Validate quality\n",
    "                confidence_score = self.quality_validator.calculate_confidence_score(mcq)\n",
    "                mcq.confidence_score = confidence_score\n",
    "\n",
    "                if confidence_score >= self.min_quality_score:\n",
    "                    return mcq, None\n",
    "                else:\n",
    "                    if attempt < self.max_retries:\n",
    "                        print(f\"    üîÑ Retry {attempt}: Low quality score ({confidence_score:.1f})\")\n",
    "                        continue\n",
    "                    else:\n",
    "                        return None, f\"Quality too low after {self.max_retries} attempts\"\n",
    "\n",
    "            except Exception as e:\n",
    "                if attempt < self.max_retries:\n",
    "                    print(f\"    üîÑ Retry {attempt}: {str(e)[:50]}...\")\n",
    "                    continue\n",
    "                else:\n",
    "                    return None, f\"Generation failed: {str(e)}\"\n",
    "\n",
    "        return None, \"Max retries exceeded\"\n",
    "\n",
    "    def _calculate_statistics(self, mcqs: List[MCQQuestion], target_count: int) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate generation statistics\"\"\"\n",
    "        if not mcqs:\n",
    "            return {\"error\": \"No MCQs generated\"}\n",
    "\n",
    "        # Basic statistics\n",
    "        stats = {\n",
    "            \"total_generated\": len(mcqs),\n",
    "            \"target_count\": target_count,\n",
    "            \"success_rate\": len(mcqs) / target_count * 100,\n",
    "            \"average_quality\": np.mean([mcq.confidence_score for mcq in mcqs]),\n",
    "            \"quality_distribution\": {}\n",
    "        }\n",
    "\n",
    "        # Quality distribution\n",
    "        quality_ranges = [(0, 40), (40, 60), (60, 80), (80, 100)]\n",
    "        for low, high in quality_ranges:\n",
    "            count = sum(1 for mcq in mcqs if low <= mcq.confidence_score < high)\n",
    "            stats[\"quality_distribution\"][f\"{low}-{high}\"] = count\n",
    "\n",
    "        # Topic distribution\n",
    "        topic_counts = {}\n",
    "        for mcq in mcqs:\n",
    "            topic_counts[mcq.topic] = topic_counts.get(mcq.topic, 0) + 1\n",
    "        stats[\"topic_distribution\"] = topic_counts\n",
    "\n",
    "        # Difficulty distribution\n",
    "        difficulty_counts = {}\n",
    "        for mcq in mcqs:\n",
    "            difficulty_counts[mcq.difficulty] = difficulty_counts.get(mcq.difficulty, 0) + 1\n",
    "        stats[\"difficulty_distribution\"] = difficulty_counts\n",
    "\n",
    "        # Question type distribution\n",
    "        type_counts = {}\n",
    "        for mcq in mcqs:\n",
    "            type_counts[mcq.question_type] = type_counts.get(mcq.question_type, 0) + 1\n",
    "        stats[\"question_type_distribution\"] = type_counts\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def _print_batch_summary(self, results: Dict[str, Any]):\n",
    "        \"\"\"Print comprehensive batch summary\"\"\"\n",
    "        stats = results[\"statistics\"]\n",
    "        quality_report = results[\"quality_report\"]\n",
    "\n",
    "        print(f\"\\\\nüéâ Batch Generation Complete!\")\n",
    "        print(f\"={'='*50}\")\n",
    "\n",
    "        print(f\"üìä Generation Statistics:\")\n",
    "        print(f\"  Total Generated: {stats['total_generated']}/{stats['target_count']}\")\n",
    "        print(f\"  Success Rate: {stats['success_rate']:.1f}%\")\n",
    "        print(f\"  Average Quality: {stats['average_quality']:.1f}/100\")\n",
    "\n",
    "        print(f\"\\\\nüìà Quality Distribution:\")\n",
    "        for range_str, count in stats['quality_distribution'].items():\n",
    "            print(f\"  {range_str}: {count} questions\")\n",
    "\n",
    "        print(f\"\\\\nüéØ Difficulty Distribution:\")\n",
    "        for difficulty, count in stats['difficulty_distribution'].items():\n",
    "            print(f\"  {difficulty.title()}: {count} questions\")\n",
    "\n",
    "        if results[\"failed_generations\"]:\n",
    "            print(f\"\\\\n‚ùå Failed Generations: {len(results['failed_generations'])}\")\n",
    "            failure_reasons = {}\n",
    "            for failure in results[\"failed_generations\"]:\n",
    "                reason = failure[\"error\"]\n",
    "                failure_reasons[reason] = failure_reasons.get(reason, 0) + 1\n",
    "\n",
    "            for reason, count in failure_reasons.items():\n",
    "                print(f\"  {reason}: {count} times\")\n",
    "\n",
    "        print(f\"\\\\nüí° Recommendations:\")\n",
    "        if quality_report.get(\"recommendations\"):\n",
    "            for rec in quality_report[\"recommendations\"]:\n",
    "                print(f\"  ‚Ä¢ {rec}\")\n",
    "\n",
    "        if stats['success_rate'] < 80:\n",
    "            print(f\"  ‚Ä¢ Consider adjusting generation parameters\")\n",
    "        if stats['average_quality'] < 70:\n",
    "            print(f\"  ‚Ä¢ Review prompt engineering and validation criteria\")\n",
    "\n",
    "    def export_results(self, results: Dict[str, Any], output_file: str):\n",
    "        \"\"\"Export results to JSON file\"\"\"\n",
    "        export_data = {\n",
    "            \"metadata\": {\n",
    "                \"generation_timestamp\": time.time(),\n",
    "                \"total_questions\": len(results[\"mcqs\"]),\n",
    "                \"success_rate\": results[\"statistics\"][\"success_rate\"],\n",
    "                \"average_quality\": results[\"statistics\"][\"average_quality\"]\n",
    "            },\n",
    "            \"questions\": [mcq.to_dict() for mcq in results[\"mcqs\"]],\n",
    "            \"statistics\": results[\"statistics\"],\n",
    "            \"quality_report\": results[\"quality_report\"],\n",
    "            \"difficulty_analysis\": results[\"difficulty_analysis\"],\n",
    "            \"failed_generations\": results[\"failed_generations\"]\n",
    "        }\n",
    "\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(export_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"üìÅ Results exported to: {output_file}\")\n",
    "\n",
    "# Initialize batch generator\n",
    "if all(var in locals() for var in ['mcq_generator', 'retriever', 'quality_validator', 'difficulty_analyzer']):\n",
    "    batch_generator = BatchMCQGenerator(\n",
    "        mcq_generator, retriever, quality_validator, difficulty_analyzer\n",
    "    )\n",
    "    print(\"‚úÖ Batch MCQ Generator initialized!\")\n",
    "else:\n",
    "    print(\"‚ùå Required components not available for batch generator initialization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004c4c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run batch generation test\n",
    "if 'batch_generator' in locals():\n",
    "    print(\"üß™ Testing Batch MCQ Generation...\")\n",
    "\n",
    "    # Define test parameters\n",
    "    test_topics = [\n",
    "        \"Object-Oriented Programming\",\n",
    "        \"Inheritance and Polymorphism\",\n",
    "        \"Data Structures\"\n",
    "    ]\n",
    "\n",
    "    test_difficulties = [\n",
    "        DifficultyLevel.EASY,\n",
    "        DifficultyLevel.MEDIUM,\n",
    "        DifficultyLevel.HARD\n",
    "    ]\n",
    "\n",
    "    test_question_types = [\n",
    "        QuestionType.DEFINITION,\n",
    "        QuestionType.APPLICATION\n",
    "    ]\n",
    "\n",
    "    # Generate batch\n",
    "    batch_results = batch_generator.generate_batch(\n",
    "        topics=test_topics,\n",
    "        count_per_topic=2,  # Generate 2 questions per topic\n",
    "        difficulties=test_difficulties,\n",
    "        question_types=test_question_types\n",
    "    )\n",
    "\n",
    "    # Display sample generated MCQs\n",
    "    print(f\"\\\\nüìù Sample Generated MCQs:\")\n",
    "    print(f\"=\"*60)\n",
    "\n",
    "    for i, mcq in enumerate(batch_results[\"mcqs\"][:3], 1):  # Show first 3 MCQs\n",
    "        print(f\"\\\\nüéØ Sample MCQ {i}:\")\n",
    "        print(f\"Topic: {mcq.topic}\")\n",
    "        print(f\"Difficulty: {mcq.difficulty}\")\n",
    "        print(f\"Type: {mcq.question_type}\")\n",
    "        print(f\"Quality Score: {mcq.confidence_score:.1f}/100\")\n",
    "        print(f\"\\\\nQuestion: {mcq.question}\")\n",
    "\n",
    "        print(f\"\\\\nOptions:\")\n",
    "        for option in mcq.options:\n",
    "            marker = \"‚úÖ\" if option.is_correct else \"  \"\n",
    "            print(f\"  {marker} {option.label}: {option.text}\")\n",
    "\n",
    "        print(f\"\\\\nExplanation: {mcq.explanation}\")\n",
    "        print(f\"-\" * 60)\n",
    "\n",
    "    # Export results\n",
    "    output_filename = f\"batch_mcq_results_{int(time.time())}.json\"\n",
    "    batch_generator.export_results(batch_results, output_filename)\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Batch generator not available for testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82437f85",
   "metadata": {},
   "source": [
    "## 10. Performance Evaluation Metrics\n",
    "\n",
    "Implementing comprehensive evaluation metrics for the RAG-MCQ system including relevance, clarity, generation speed, and success rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f99d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceEvaluator:\n",
    "    \"\"\"Comprehensive performance evaluation for RAG-MCQ system\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.metrics = {}\n",
    "        self.benchmarks = {\n",
    "            \"generation_time_per_question\": 30.0,  # seconds\n",
    "            \"minimum_success_rate\": 80.0,  # percentage\n",
    "            \"minimum_quality_score\": 70.0,  # 0-100\n",
    "            \"maximum_retry_rate\": 20.0  # percentage\n",
    "        }\n",
    "\n",
    "    def evaluate_system_performance(self, batch_results: Dict[str, Any],\n",
    "                                   generation_time: float) -> Dict[str, Any]:\n",
    "        \"\"\"Comprehensive system performance evaluation\"\"\"\n",
    "\n",
    "        mcqs = batch_results[\"mcqs\"]\n",
    "        stats = batch_results[\"statistics\"]\n",
    "\n",
    "        evaluation = {\n",
    "            \"performance_metrics\": {},\n",
    "            \"quality_metrics\": {},\n",
    "            \"efficiency_metrics\": {},\n",
    "            \"recommendations\": [],\n",
    "            \"overall_score\": 0.0\n",
    "        }\n",
    "\n",
    "        # Performance metrics\n",
    "        evaluation[\"performance_metrics\"] = self._calculate_performance_metrics(\n",
    "            mcqs, stats, generation_time\n",
    "        )\n",
    "\n",
    "        # Quality metrics\n",
    "        evaluation[\"quality_metrics\"] = self._calculate_quality_metrics(mcqs)\n",
    "\n",
    "        # Efficiency metrics\n",
    "        evaluation[\"efficiency_metrics\"] = self._calculate_efficiency_metrics(\n",
    "            batch_results, generation_time\n",
    "        )\n",
    "\n",
    "        # Generate recommendations\n",
    "        evaluation[\"recommendations\"] = self._generate_performance_recommendations(evaluation)\n",
    "\n",
    "        # Calculate overall score\n",
    "        evaluation[\"overall_score\"] = self._calculate_overall_score(evaluation)\n",
    "\n",
    "        return evaluation\n",
    "\n",
    "    def _calculate_performance_metrics(self, mcqs: List[MCQQuestion],\n",
    "                                     stats: Dict, generation_time: float) -> Dict[str, float]:\n",
    "        \"\"\"Calculate core performance metrics\"\"\"\n",
    "        total_target = stats.get(\"target_count\", len(mcqs))\n",
    "\n",
    "        metrics = {\n",
    "            \"success_rate\": len(mcqs) / total_target * 100 if total_target > 0 else 0,\n",
    "            \"average_generation_time\": generation_time / len(mcqs) if mcqs else 0,\n",
    "            \"throughput_questions_per_minute\": len(mcqs) / (generation_time / 60) if generation_time > 0 else 0,\n",
    "            \"validity_rate\": sum(1 for mcq in mcqs if mcq.confidence_score >= 60) / len(mcqs) * 100 if mcqs else 0\n",
    "        }\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def _calculate_quality_metrics(self, mcqs: List[MCQQuestion]) -> Dict[str, float]:\n",
    "        \"\"\"Calculate quality-related metrics\"\"\"\n",
    "        if not mcqs:\n",
    "            return {}\n",
    "\n",
    "        confidence_scores = [mcq.confidence_score for mcq in mcqs]\n",
    "\n",
    "        # Content diversity metrics\n",
    "        unique_questions = len(set(mcq.question for mcq in mcqs))\n",
    "        question_diversity = unique_questions / len(mcqs) * 100\n",
    "\n",
    "        # Option quality metrics\n",
    "        avg_option_lengths = []\n",
    "        for mcq in mcqs:\n",
    "            option_lengths = [len(opt.text) for opt in mcq.options]\n",
    "            avg_option_lengths.append(np.mean(option_lengths))\n",
    "\n",
    "        metrics = {\n",
    "            \"average_quality_score\": np.mean(confidence_scores),\n",
    "            \"quality_score_std\": np.std(confidence_scores),\n",
    "            \"min_quality_score\": np.min(confidence_scores),\n",
    "            \"max_quality_score\": np.max(confidence_scores),\n",
    "            \"high_quality_rate\": sum(1 for score in confidence_scores if score >= 80) / len(confidence_scores) * 100,\n",
    "            \"question_diversity\": question_diversity,\n",
    "            \"average_option_length\": np.mean(avg_option_lengths),\n",
    "            \"explanation_completeness\": sum(1 for mcq in mcqs if len(mcq.explanation) >= 50) / len(mcqs) * 100\n",
    "        }\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def _calculate_efficiency_metrics(self, batch_results: Dict, generation_time: float) -> Dict[str, float]:\n",
    "        \"\"\"Calculate efficiency and resource utilization metrics\"\"\"\n",
    "        mcqs = batch_results[\"mcqs\"]\n",
    "        failed_generations = batch_results.get(\"failed_generations\", [])\n",
    "\n",
    "        total_attempts = len(mcqs) + len(failed_generations)\n",
    "\n",
    "        metrics = {\n",
    "            \"retry_rate\": len(failed_generations) / total_attempts * 100 if total_attempts > 0 else 0,\n",
    "            \"resource_efficiency\": len(mcqs) / generation_time if generation_time > 0 else 0,\n",
    "            \"context_utilization\": self._calculate_context_utilization(mcqs),\n",
    "            \"prompt_efficiency\": self._calculate_prompt_efficiency(mcqs)\n",
    "        }\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def _calculate_context_utilization(self, mcqs: List[MCQQuestion]) -> float:\n",
    "        \"\"\"Calculate how well the context is utilized\"\"\"\n",
    "        if not mcqs:\n",
    "            return 0.0\n",
    "\n",
    "        # Simplified metric: average context length vs question relevance\n",
    "        context_lengths = [len(mcq.context) for mcq in mcqs]\n",
    "        quality_scores = [mcq.confidence_score for mcq in mcqs]\n",
    "\n",
    "        # Higher quality with reasonable context length indicates good utilization\n",
    "        avg_context_length = np.mean(context_lengths)\n",
    "        avg_quality = np.mean(quality_scores)\n",
    "\n",
    "        # Optimal context length range: 300-800 characters\n",
    "        if 300 <= avg_context_length <= 800:\n",
    "            length_score = 100\n",
    "        else:\n",
    "            length_score = max(0, 100 - abs(avg_context_length - 550) / 10)\n",
    "\n",
    "        # Combine with quality score\n",
    "        utilization_score = (length_score + avg_quality) / 2\n",
    "        return utilization_score\n",
    "\n",
    "    def _calculate_prompt_efficiency(self, mcqs: List[MCQQuestion]) -> float:\n",
    "        \"\"\"Calculate prompt efficiency based on output quality\"\"\"\n",
    "        if not mcqs:\n",
    "            return 0.0\n",
    "\n",
    "        # Measure consistency in output format and quality\n",
    "        format_consistency = self._check_format_consistency(mcqs)\n",
    "        quality_consistency = self._check_quality_consistency(mcqs)\n",
    "\n",
    "        return (format_consistency + quality_consistency) / 2\n",
    "\n",
    "    def _check_format_consistency(self, mcqs: List[MCQQuestion]) -> float:\n",
    "        \"\"\"Check consistency in MCQ format\"\"\"\n",
    "        if not mcqs:\n",
    "            return 0.0\n",
    "\n",
    "        consistent_count = 0\n",
    "        for mcq in mcqs:\n",
    "            # Check if MCQ follows expected format\n",
    "            has_4_options = len(mcq.options) == 4\n",
    "            has_correct_labels = all(opt.label in [\"A\", \"B\", \"C\", \"D\"] for opt in mcq.options)\n",
    "            has_one_correct = sum(1 for opt in mcq.options if opt.is_correct) == 1\n",
    "            has_explanation = len(mcq.explanation) > 10\n",
    "\n",
    "            if all([has_4_options, has_correct_labels, has_one_correct, has_explanation]):\n",
    "                consistent_count += 1\n",
    "\n",
    "        return consistent_count / len(mcqs) * 100\n",
    "\n",
    "    def _check_quality_consistency(self, mcqs: List[MCQQuestion]) -> float:\n",
    "        \"\"\"Check consistency in quality scores\"\"\"\n",
    "        if not mcqs:\n",
    "            return 0.0\n",
    "\n",
    "        quality_scores = [mcq.confidence_score for mcq in mcqs]\n",
    "        quality_std = np.std(quality_scores)\n",
    "\n",
    "        # Lower standard deviation indicates more consistent quality\n",
    "        # Scale to 0-100 where lower std = higher score\n",
    "        consistency_score = max(0, 100 - quality_std)\n",
    "        return consistency_score\n",
    "\n",
    "    def _generate_performance_recommendations(self, evaluation: Dict) -> List[str]:\n",
    "        \"\"\"Generate recommendations based on performance evaluation\"\"\"\n",
    "        recommendations = []\n",
    "\n",
    "        perf_metrics = evaluation[\"performance_metrics\"]\n",
    "        quality_metrics = evaluation[\"quality_metrics\"]\n",
    "        efficiency_metrics = evaluation[\"efficiency_metrics\"]\n",
    "\n",
    "        # Success rate recommendations\n",
    "        if perf_metrics.get(\"success_rate\", 0) < self.benchmarks[\"minimum_success_rate\"]:\n",
    "            recommendations.append(\"Improve generation stability - success rate below target\")\n",
    "\n",
    "        # Quality recommendations\n",
    "        if quality_metrics.get(\"average_quality_score\", 0) < self.benchmarks[\"minimum_quality_score\"]:\n",
    "            recommendations.append(\"Enhance prompt engineering to improve quality scores\")\n",
    "\n",
    "        # Efficiency recommendations\n",
    "        if perf_metrics.get(\"average_generation_time\", 0) > self.benchmarks[\"generation_time_per_question\"]:\n",
    "            recommendations.append(\"Optimize generation pipeline for better performance\")\n",
    "\n",
    "        if efficiency_metrics.get(\"retry_rate\", 0) > self.benchmarks[\"maximum_retry_rate\"]:\n",
    "            recommendations.append(\"Reduce retry rate by improving initial generation quality\")\n",
    "\n",
    "        # Diversity recommendations\n",
    "        if quality_metrics.get(\"question_diversity\", 0) < 90:\n",
    "            recommendations.append(\"Improve question diversity to avoid repetition\")\n",
    "\n",
    "        # Context utilization recommendations\n",
    "        if efficiency_metrics.get(\"context_utilization\", 0) < 70:\n",
    "            recommendations.append(\"Optimize context retrieval and utilization\")\n",
    "\n",
    "        return recommendations\n",
    "\n",
    "    def _calculate_overall_score(self, evaluation: Dict) -> float:\n",
    "        \"\"\"Calculate overall system performance score\"\"\"\n",
    "        perf_metrics = evaluation[\"performance_metrics\"]\n",
    "        quality_metrics = evaluation[\"quality_metrics\"]\n",
    "        efficiency_metrics = evaluation[\"efficiency_metrics\"]\n",
    "\n",
    "        # Weighted scoring\n",
    "        weights = {\n",
    "            \"success_rate\": 0.25,\n",
    "            \"quality_score\": 0.30,\n",
    "            \"generation_time\": 0.20,\n",
    "            \"efficiency\": 0.25\n",
    "        }\n",
    "\n",
    "        # Normalize metrics to 0-100 scale\n",
    "        success_score = min(100, perf_metrics.get(\"success_rate\", 0))\n",
    "        quality_score = quality_metrics.get(\"average_quality_score\", 0)\n",
    "\n",
    "        # Time score (inverse - lower time = higher score)\n",
    "        time_score = min(100, self.benchmarks[\"generation_time_per_question\"] /\n",
    "                        max(0.1, perf_metrics.get(\"average_generation_time\", 30)) * 100)\n",
    "\n",
    "        efficiency_score = efficiency_metrics.get(\"context_utilization\", 0)\n",
    "\n",
    "        overall_score = (\n",
    "            weights[\"success_rate\"] * success_score +\n",
    "            weights[\"quality_score\"] * quality_score +\n",
    "            weights[\"generation_time\"] * time_score +\n",
    "            weights[\"efficiency\"] * efficiency_score\n",
    "        )\n",
    "\n",
    "        return overall_score\n",
    "\n",
    "    def visualize_performance(self, evaluation: Dict):\n",
    "        \"\"\"Create performance visualization\"\"\"\n",
    "        if not evaluation:\n",
    "            print(\"‚ùå No evaluation data available for visualization\")\n",
    "            return\n",
    "\n",
    "        # Create performance dashboard\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('RAG-MCQ System Performance Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "        # 1. Performance Metrics Bar Chart\n",
    "        perf_metrics = evaluation[\"performance_metrics\"]\n",
    "        metrics_names = list(perf_metrics.keys())\n",
    "        metrics_values = list(perf_metrics.values())\n",
    "\n",
    "        axes[0, 0].bar(range(len(metrics_names)), metrics_values, color='skyblue')\n",
    "        axes[0, 0].set_title('Performance Metrics')\n",
    "        axes[0, 0].set_xticks(range(len(metrics_names)))\n",
    "        axes[0, 0].set_xticklabels([name.replace('_', ' ').title() for name in metrics_names],\n",
    "                                  rotation=45, ha='right')\n",
    "        axes[0, 0].set_ylabel('Score/Rate')\n",
    "\n",
    "        # 2. Quality Distribution\n",
    "        quality_metrics = evaluation[\"quality_metrics\"]\n",
    "        quality_names = ['Avg Quality', 'Min Quality', 'Max Quality', 'High Quality Rate']\n",
    "        quality_values = [\n",
    "            quality_metrics.get(\"average_quality_score\", 0),\n",
    "            quality_metrics.get(\"min_quality_score\", 0),\n",
    "            quality_metrics.get(\"max_quality_score\", 0),\n",
    "            quality_metrics.get(\"high_quality_rate\", 0)\n",
    "        ]\n",
    "\n",
    "        axes[0, 1].bar(quality_names, quality_values, color='lightgreen')\n",
    "        axes[0, 1].set_title('Quality Metrics')\n",
    "        axes[0, 1].set_ylabel('Score')\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "        # 3. Efficiency Metrics\n",
    "        efficiency_metrics = evaluation[\"efficiency_metrics\"]\n",
    "        eff_names = list(efficiency_metrics.keys())\n",
    "        eff_values = list(efficiency_metrics.values())\n",
    "\n",
    "        axes[1, 0].bar(eff_names, eff_values, color='orange')\n",
    "        axes[1, 0].set_title('Efficiency Metrics')\n",
    "        axes[1, 0].set_xticks(range(len(eff_names)))\n",
    "        axes[1, 0].set_xticklabels([name.replace('_', ' ').title() for name in eff_names],\n",
    "                                  rotation=45, ha='right')\n",
    "        axes[1, 0].set_ylabel('Score/Rate')\n",
    "\n",
    "        # 4. Overall Score Gauge\n",
    "        overall_score = evaluation[\"overall_score\"]\n",
    "        axes[1, 1].pie([overall_score, 100-overall_score],\n",
    "                      labels=[f'Score: {overall_score:.1f}', ''],\n",
    "                      colors=['lightcoral', 'lightgray'],\n",
    "                      startangle=90)\n",
    "        axes[1, 1].set_title('Overall Performance Score')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Print performance summary\n",
    "        print(f\"\\\\nüéØ Performance Summary:\")\n",
    "        print(f\"Overall Score: {overall_score:.1f}/100\")\n",
    "\n",
    "        if overall_score >= 80:\n",
    "            print(\"‚úÖ Excellent performance!\")\n",
    "        elif overall_score >= 70:\n",
    "            print(\"üü° Good performance with room for improvement\")\n",
    "        else:\n",
    "            print(\"üî¥ Performance needs significant improvement\")\n",
    "\n",
    "# Initialize performance evaluator\n",
    "performance_evaluator = PerformanceEvaluator()\n",
    "\n",
    "# Evaluate system performance if batch results are available\n",
    "if 'batch_results' in locals():\n",
    "    print(\"üìä Evaluating System Performance...\")\n",
    "\n",
    "    # Simulate generation time (in real scenario, this would be measured)\n",
    "    simulated_generation_time = len(batch_results[\"mcqs\"]) * 5  # 5 seconds per question\n",
    "\n",
    "    evaluation = performance_evaluator.evaluate_system_performance(\n",
    "        batch_results, simulated_generation_time\n",
    "    )\n",
    "\n",
    "    # Display evaluation results\n",
    "    print(f\"\\\\nüìà Performance Evaluation Results:\")\n",
    "    print(f\"=\"*60)\n",
    "\n",
    "    print(f\"\\\\nüéØ Performance Metrics:\")\n",
    "    for metric, value in evaluation[\"performance_metrics\"].items():\n",
    "        print(f\"  {metric.replace('_', ' ').title()}: {value:.2f}\")\n",
    "\n",
    "    print(f\"\\\\nüèÜ Quality Metrics:\")\n",
    "    for metric, value in evaluation[\"quality_metrics\"].items():\n",
    "        print(f\"  {metric.replace('_', ' ').title()}: {value:.2f}\")\n",
    "\n",
    "    print(f\"\\\\n‚ö° Efficiency Metrics:\")\n",
    "    for metric, value in evaluation[\"efficiency_metrics\"].items():\n",
    "        print(f\"  {metric.replace('_', ' ').title()}: {value:.2f}\")\n",
    "\n",
    "    print(f\"\\\\nüíØ Overall Score: {evaluation['overall_score']:.1f}/100\")\n",
    "\n",
    "    if evaluation[\"recommendations\"]:\n",
    "        print(f\"\\\\nüí° Recommendations:\")\n",
    "        for rec in evaluation[\"recommendations\"]:\n",
    "            print(f\"  ‚Ä¢ {rec}\")\n",
    "\n",
    "    # Create visualization\n",
    "    performance_evaluator.visualize_performance(evaluation)\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No batch results available for performance evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537bd142",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "### üéâ What We've Accomplished\n",
    "\n",
    "This notebook demonstrated a comprehensive RAG system for Multiple Choice Question generation with the following key features:\n",
    "\n",
    "#### ‚úÖ Core Components Implemented\n",
    "1. **Document Processing Pipeline** - PDF loading, text extraction, and semantic chunking\n",
    "2. **Vector Database & Embeddings** - FAISS with Vietnamese language support\n",
    "3. **Context-Aware Retrieval** - Diverse document retrieval with similarity thresholds\n",
    "4. **LLM-Powered Generation** - Structured MCQ generation with JSON output\n",
    "5. **Advanced Prompt Engineering** - Specialized prompts for different question types\n",
    "6. **Quality Validation System** - Comprehensive validation with scoring\n",
    "7. **Difficulty Assessment** - Intelligent difficulty classification\n",
    "8. **Batch Processing** - Scalable generation with error handling\n",
    "9. **Performance Evaluation** - Comprehensive metrics and reporting\n",
    "\n",
    "#### üéØ Key Achievements\n",
    "- **Multi-language Support**: Vietnamese language optimization\n",
    "- **Educational Focus**: Question types aligned with learning objectives\n",
    "- **Quality Assurance**: Automatic validation and confidence scoring\n",
    "- **Scalability**: Batch processing capabilities\n",
    "- **Comprehensive Evaluation**: Multiple metrics for system assessment\n",
    "\n",
    "### üöÄ Next Steps for Production\n",
    "\n",
    "#### Phase 1: Enhancement & Optimization\n",
    "- [ ] **Real LLM Integration**: Replace mock LLM with actual models (Gemma, Vicuna, etc.)\n",
    "- [ ] **GPU Optimization**: Implement CUDA acceleration for faster processing\n",
    "- [ ] **Memory Management**: Optimize memory usage for large document collections\n",
    "- [ ] **Caching System**: Implement embedding and response caching\n",
    "\n",
    "#### Phase 2: Advanced Features\n",
    "- [ ] **Multi-Modal Support**: Add support for images, diagrams, and code snippets\n",
    "- [ ] **Adaptive Learning**: Implement difficulty adjustment based on user performance\n",
    "- [ ] **Human-in-the-Loop**: Add expert review and feedback mechanisms\n",
    "- [ ] **Multi-Language Expansion**: Support for English and other languages\n",
    "\n",
    "#### Phase 3: Production Deployment\n",
    "- [ ] **Web API Development**: Create REST API for system integration\n",
    "- [ ] **User Interface**: Build web interface for question management\n",
    "- [ ] **Database Integration**: Implement persistent storage for questions and metadata\n",
    "- [ ] **Authentication & Authorization**: Add user management and access control\n",
    "\n",
    "#### Phase 4: Advanced Analytics\n",
    "- [ ] **Learning Analytics**: Track question effectiveness and student performance\n",
    "- [ ] **Content Gap Analysis**: Identify areas needing more questions\n",
    "- [ ] **Automatic Curriculum Mapping**: Align questions with learning objectives\n",
    "- [ ] **Personalization**: Adaptive question selection based on learner profiles\n",
    "\n",
    "### üìä System Performance Summary\n",
    "\n",
    "Based on our demonstration:\n",
    "- **Generation Success Rate**: High (with proper configuration)\n",
    "- **Quality Validation**: Comprehensive multi-factor assessment\n",
    "- **Scalability**: Batch processing with error handling\n",
    "- **Flexibility**: Multiple question types and difficulty levels\n",
    "- **Educational Value**: Aligned with pedagogical best practices\n",
    "\n",
    "### üõ†Ô∏è Technical Requirements for Production\n",
    "\n",
    "#### Hardware Requirements\n",
    "- **GPU**: NVIDIA GPU with 8GB+ VRAM for model inference\n",
    "- **RAM**: 16GB+ system RAM for document processing\n",
    "- **Storage**: 100GB+ for models, embeddings, and document storage\n",
    "- **CPU**: Multi-core processor for parallel document processing\n",
    "\n",
    "#### Software Dependencies\n",
    "- **Python 3.8+** with virtual environment\n",
    "- **CUDA toolkit** for GPU acceleration\n",
    "- **LangChain ecosystem** for RAG pipeline\n",
    "- **Transformers library** for model inference\n",
    "- **FAISS** for vector similarity search\n",
    "- **FastAPI/Streamlit** for web interface\n",
    "\n",
    "### üìö Educational Impact\n",
    "\n",
    "This RAG-MCQ system can significantly impact education by:\n",
    "- **Reducing Teacher Workload**: Automated question generation\n",
    "- **Improving Assessment Quality**: Consistent, validated questions\n",
    "- **Personalizing Learning**: Adaptive difficulty and topics\n",
    "- **Scaling Education**: Support for large student populations\n",
    "- **Enhancing Learning**: Immediate feedback and explanations\n",
    "\n",
    "### üî¨ Research Opportunities\n",
    "\n",
    "- **Question Quality Metrics**: Develop better automatic quality assessment\n",
    "- **Distractor Generation**: Improve incorrect option generation\n",
    "- **Cognitive Load Theory**: Apply learning theory to difficulty assessment\n",
    "- **Multi-Document Synthesis**: Generate questions requiring multiple sources\n",
    "- **Real-Time Adaptation**: Dynamic question adjustment during assessment\n",
    "\n",
    "### üí° Final Recommendations\n",
    "\n",
    "1. **Start Small**: Begin with a limited domain and gradually expand\n",
    "2. **Validate Extensively**: Test with real educators and students\n",
    "3. **Iterate Quickly**: Use feedback to improve the system continuously\n",
    "4. **Focus on Quality**: Prioritize question quality over quantity\n",
    "5. **Monitor Performance**: Track all metrics for continuous improvement\n",
    "\n",
    "This demonstration provides a solid foundation for building a production-ready RAG system for MCQ generation that can serve educational institutions, online learning platforms, and assessment organizations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-multi-choice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
