{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e18e2357",
   "metadata": {},
   "source": [
    "# RAG System for Multiple Choice Question (MCQ) Generation\n",
    "\n",
    "## Comprehensive Demonstration and Implementation Guide\n",
    "\n",
    "This notebook demonstrates a complete implementation of a Retrieval-Augmented Generation (RAG) system specifically designed for generating high-quality Multiple Choice Questions from educational documents.\n",
    "\n",
    "### System Overview\n",
    "- **Document Processing**: PDF text extraction and semantic chunking\n",
    "- **Vector Database**: FAISS with Vietnamese language embeddings\n",
    "- **Question Generation**: LLM-powered MCQ creation with structured output\n",
    "- **Quality Assurance**: Automatic validation and difficulty assessment\n",
    "- **Batch Processing**: Scalable question generation capabilities\n",
    "\n",
    "### Key Features\n",
    "- ðŸŒ Vietnamese language support\n",
    "- ðŸ“š Multi-document processing\n",
    "- ðŸŽ¯ Multiple question types (definition, application, analysis)\n",
    "- ðŸ“Š Quality scoring and validation\n",
    "- âš¡ Optimized performance with quantized models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bac29a6",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies\n",
    "\n",
    "First, let's install and import all required libraries for our RAG-MCQ system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6583f9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install langchain langchain-community langchain-experimental langchain-huggingface\n",
    "# !pip install transformers torch accelerate bitsandbytes\n",
    "# !pip install faiss-cpu sentence-transformers\n",
    "# !pip install pypdf unstructured\n",
    "# !pip install numpy pandas matplotlib seaborn\n",
    "# !pip install nltk rouge-score\n",
    "\n",
    "# Check if CUDA is available\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5710fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Transformers imports\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"ðŸ“¦ LangChain version: {getattr(__import__('langchain'), '__version__', 'Unknown')}\")\n",
    "print(f\"ðŸ¤— Transformers version: {getattr(__import__('transformers'), '__version__', 'Unknown')}\")\n",
    "print(f\"ðŸ”¥ PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf39df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and Data Classes\n",
    "class QuestionType(Enum):\n",
    "    \"\"\"Enumeration of different question types\"\"\"\n",
    "    DEFINITION = \"definition\"\n",
    "    COMPARISON = \"comparison\"\n",
    "    APPLICATION = \"application\"\n",
    "    ANALYSIS = \"analysis\"\n",
    "    EVALUATION = \"evaluation\"\n",
    "\n",
    "class DifficultyLevel(Enum):\n",
    "    \"\"\"Enumeration of difficulty levels\"\"\"\n",
    "    EASY = \"easy\"\n",
    "    MEDIUM = \"medium\"\n",
    "    HARD = \"hard\"\n",
    "    EXPERT = \"expert\"\n",
    "\n",
    "@dataclass\n",
    "class MCQOption:\n",
    "    \"\"\"Data class for MCQ options\"\"\"\n",
    "    label: str\n",
    "    text: str\n",
    "    is_correct: bool\n",
    "\n",
    "@dataclass\n",
    "class MCQQuestion:\n",
    "    \"\"\"Data class for Multiple Choice Question\"\"\"\n",
    "    question: str\n",
    "    context: str\n",
    "    options: List[MCQOption]\n",
    "    explanation: str\n",
    "    difficulty: str\n",
    "    topic: str\n",
    "    question_type: str\n",
    "    source: str\n",
    "    confidence_score: float = 0.0\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert to dictionary format\"\"\"\n",
    "        return {\n",
    "            \"question\": self.question,\n",
    "            \"context\": self.context,\n",
    "            \"options\": {opt.label: opt.text for opt in self.options},\n",
    "            \"correct_answer\": next(opt.label for opt in self.options if opt.is_correct),\n",
    "            \"explanation\": self.explanation,\n",
    "            \"difficulty\": self.difficulty,\n",
    "            \"topic\": self.topic,\n",
    "            \"question_type\": self.question_type,\n",
    "            \"source\": self.source,\n",
    "            \"confidence_score\": self.confidence_score\n",
    "        }\n",
    "\n",
    "# System Configuration\n",
    "CONFIG = {\n",
    "    \"embedding_model\": \"bkai-foundation-models/vietnamese-bi-encoder\",\n",
    "    \"llm_model\": \"google/gemma-2b-it\",\n",
    "    \"chunk_size\": 500,\n",
    "    \"chunk_overlap\": 50,\n",
    "    \"retrieval_k\": 5,\n",
    "    \"generation_temperature\": 0.7,\n",
    "    \"max_tokens\": 512,\n",
    "    \"diversity_threshold\": 0.7\n",
    "}\n",
    "\n",
    "print(\"âœ… Configuration and data classes defined!\")\n",
    "print(f\"ðŸ“‹ Using embedding model: {CONFIG['embedding_model']}\")\n",
    "print(f\"ðŸ¤– Using LLM model: {CONFIG['llm_model']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c5d0ba",
   "metadata": {},
   "source": [
    "## 2. Document Processing Pipeline\n",
    "\n",
    "Let's implement the document loading and text extraction pipeline for PDF documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66df219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    \"\"\"Handles document loading and preprocessing\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.supported_formats = ['.pdf', '.txt']\n",
    "\n",
    "    def load_documents(self, folder_path: str) -> Tuple[List[Document], List[str]]:\n",
    "        \"\"\"Load and process documents from folder\"\"\"\n",
    "        folder = Path(folder_path)\n",
    "        if not folder.exists():\n",
    "            raise FileNotFoundError(f\"Folder not found: {folder}\")\n",
    "\n",
    "        pdf_files = list(folder.glob(\"*.pdf\"))\n",
    "        if not pdf_files:\n",
    "            print(f\"âš ï¸  No PDF files found in: {folder}\")\n",
    "            return [], []\n",
    "\n",
    "        all_docs, filenames = [], []\n",
    "        total_pages = 0\n",
    "\n",
    "        print(f\"ðŸ“ Processing {len(pdf_files)} PDF files...\")\n",
    "\n",
    "        for pdf_file in pdf_files:\n",
    "            try:\n",
    "                print(f\"ðŸ“„ Loading: {pdf_file.name}\")\n",
    "                loader = PyPDFLoader(str(pdf_file))\n",
    "                docs = loader.load()\n",
    "\n",
    "                # Add metadata\n",
    "                for doc in docs:\n",
    "                    doc.metadata['source_file'] = pdf_file.name\n",
    "                    doc.metadata['file_path'] = str(pdf_file)\n",
    "\n",
    "                all_docs.extend(docs)\n",
    "                filenames.append(pdf_file.name)\n",
    "                total_pages += len(docs)\n",
    "\n",
    "                print(f\"  âœ… Loaded {len(docs)} pages\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ Failed loading {pdf_file.name}: {e}\")\n",
    "\n",
    "        print(f\"\\nðŸ“Š Summary:\")\n",
    "        print(f\"  ðŸ“š Files loaded: {len(filenames)}\")\n",
    "        print(f\"  ðŸ“„ Total pages: {total_pages}\")\n",
    "        print(f\"  ðŸ“ Average pages per file: {total_pages/len(filenames):.1f}\")\n",
    "\n",
    "        return all_docs, filenames\n",
    "\n",
    "    def analyze_document_stats(self, docs: List[Document]) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze document statistics\"\"\"\n",
    "        if not docs:\n",
    "            return {}\n",
    "\n",
    "        # Calculate statistics\n",
    "        total_chars = sum(len(doc.page_content) for doc in docs)\n",
    "        total_words = sum(len(doc.page_content.split()) for doc in docs)\n",
    "\n",
    "        char_lengths = [len(doc.page_content) for doc in docs]\n",
    "        word_lengths = [len(doc.page_content.split()) for doc in docs]\n",
    "\n",
    "        stats = {\n",
    "            \"total_documents\": len(docs),\n",
    "            \"total_characters\": total_chars,\n",
    "            \"total_words\": total_words,\n",
    "            \"avg_chars_per_doc\": np.mean(char_lengths),\n",
    "            \"avg_words_per_doc\": np.mean(word_lengths),\n",
    "            \"min_chars\": np.min(char_lengths),\n",
    "            \"max_chars\": np.max(char_lengths),\n",
    "            \"min_words\": np.min(word_lengths),\n",
    "            \"max_words\": np.max(word_lengths)\n",
    "        }\n",
    "\n",
    "        return stats\n",
    "\n",
    "# Test the document processor\n",
    "doc_processor = DocumentProcessor()\n",
    "print(\"âœ… Document processor initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d4e56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample documents (update path as needed)\n",
    "# Uncomment and modify the path to your PDF folder\n",
    "\n",
    "# folder_path = \"../pdf_folder\"  # Update this path\n",
    "# docs, filenames = doc_processor.load_documents(folder_path)\n",
    "\n",
    "# For demonstration, let's create some sample documents\n",
    "sample_docs = [\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Object-Oriented Programming (OOP) lÃ  má»™t mÃ´ hÃ¬nh láº­p trÃ¬nh Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn khÃ¡i niá»‡m Ä‘á»‘i tÆ°á»£ng.\n",
    "        OOP tá»• chá»©c mÃ£ nguá»“n xung quanh cÃ¡c Ä‘á»‘i tÆ°á»£ng thay vÃ¬ cÃ¡c hÃ m vÃ  logic.\n",
    "\n",
    "        CÃ¡c nguyÃªn lÃ½ cÆ¡ báº£n cá»§a OOP bao gá»“m:\n",
    "        1. Encapsulation (ÄÃ³ng gÃ³i): áº¨n giáº¥u chi tiáº¿t triá»ƒn khai\n",
    "        2. Inheritance (Káº¿ thá»«a): TÃ¡i sá»­ dá»¥ng code tá»« class cha\n",
    "        3. Polymorphism (Äa hÃ¬nh): CÃ¹ng má»™t interface, nhiá»u implementation\n",
    "        4. Abstraction (Trá»«u tÆ°á»£ng): ÄÆ¡n giáº£n hÃ³a phá»©c táº¡p\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"OOP_basics.pdf\", \"page\": 1}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Inheritance (Káº¿ thá»«a) trong OOP cho phÃ©p má»™t class con káº¿ thá»«a cÃ¡c thuá»™c tÃ­nh vÃ  phÆ°Æ¡ng thá»©c tá»« class cha.\n",
    "\n",
    "        VÃ­ dá»¥:\n",
    "        - Class Animal cÃ³ thuá»™c tÃ­nh name vÃ  phÆ°Æ¡ng thá»©c eat()\n",
    "        - Class Dog káº¿ thá»«a tá»« Animal vÃ  thÃªm phÆ°Æ¡ng thá»©c bark()\n",
    "        - Class Cat káº¿ thá»«a tá»« Animal vÃ  thÃªm phÆ°Æ¡ng thá»©c meow()\n",
    "\n",
    "        Lá»£i Ã­ch cá»§a inheritance:\n",
    "        - TÃ¡i sá»­ dá»¥ng code\n",
    "        - Dá»… dÃ ng má»Ÿ rá»™ng\n",
    "        - Tá»• chá»©c code theo hierarchy\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"OOP_inheritance.pdf\", \"page\": 2}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Data Structures (Cáº¥u trÃºc dá»¯ liá»‡u) lÃ  cÃ¡ch tá»• chá»©c vÃ  lÆ°u trá»¯ dá»¯ liá»‡u trong mÃ¡y tÃ­nh.\n",
    "\n",
    "        CÃ¡c cáº¥u trÃºc dá»¯ liá»‡u cÆ¡ báº£n:\n",
    "        1. Array: Táº­p há»£p cÃ¡c pháº§n tá»­ cÃ¹ng kiá»ƒu\n",
    "        2. Linked List: Danh sÃ¡ch liÃªn káº¿t\n",
    "        3. Stack: NgÄƒn xáº¿p (LIFO - Last In First Out)\n",
    "        4. Queue: HÃ ng Ä‘á»£i (FIFO - First In First Out)\n",
    "        5. Tree: CÃ¢y\n",
    "        6. Graph: Äá»“ thá»‹\n",
    "\n",
    "        Chá»n cáº¥u trÃºc dá»¯ liá»‡u phÃ¹ há»£p áº£nh hÆ°á»Ÿng Ä‘áº¿n hiá»‡u suáº¥t cá»§a thuáº­t toÃ¡n.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"Data_Structures.pdf\", \"page\": 3}\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"ðŸ“š Using sample documents for demonstration\")\n",
    "print(f\"ðŸ“„ Total documents: {len(sample_docs)}\")\n",
    "\n",
    "# Analyze document statistics\n",
    "stats = doc_processor.analyze_document_stats(sample_docs)\n",
    "print(f\"\\nðŸ“Š Document Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.1f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "docs = sample_docs  # Use for the rest of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd9c03a",
   "metadata": {},
   "source": [
    "## 3. Vector Database and Embeddings\n",
    "\n",
    "Now let's set up the FAISS vector database with Vietnamese embeddings for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fceddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorDatabaseManager:\n",
    "    \"\"\"Manages vector database and embeddings\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model_name: str):\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.embeddings = None\n",
    "        self.vector_db = None\n",
    "        self.chunks = []\n",
    "\n",
    "    def initialize_embeddings(self):\n",
    "        \"\"\"Initialize the embedding model\"\"\"\n",
    "        print(f\"ðŸ”§ Loading embedding model: {self.embedding_model_name}\")\n",
    "        try:\n",
    "            self.embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=self.embedding_model_name,\n",
    "                model_kwargs={'device': 'cpu'}  # Use CPU for compatibility\n",
    "            )\n",
    "            print(\"âœ… Embeddings initialized successfully!\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to load embeddings: {e}\")\n",
    "            print(\"ðŸ”„ Falling back to sentence-transformers model...\")\n",
    "            try:\n",
    "                self.embeddings = HuggingFaceEmbeddings(\n",
    "                    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "                )\n",
    "                print(\"âœ… Fallback embeddings loaded!\")\n",
    "                return True\n",
    "            except Exception as e2:\n",
    "                print(f\"âŒ Fallback also failed: {e2}\")\n",
    "                return False\n",
    "\n",
    "    def create_semantic_chunks(self, docs: List[Document]) -> List[Document]:\n",
    "        \"\"\"Create semantic chunks from documents\"\"\"\n",
    "        if not self.embeddings:\n",
    "            raise RuntimeError(\"Embeddings not initialized\")\n",
    "\n",
    "        print(\"ðŸ”ª Creating semantic chunks...\")\n",
    "\n",
    "        try:\n",
    "            # Use SemanticChunker for intelligent chunking\n",
    "            chunker = SemanticChunker(\n",
    "                embeddings=self.embeddings,\n",
    "                buffer_size=1,\n",
    "                breakpoint_threshold_type=\"percentile\",\n",
    "                breakpoint_threshold_amount=95,\n",
    "                min_chunk_size=CONFIG[\"chunk_size\"],\n",
    "                add_start_index=True\n",
    "            )\n",
    "\n",
    "            chunks = chunker.split_documents(docs)\n",
    "            self.chunks = chunks\n",
    "\n",
    "            print(f\"âœ… Created {len(chunks)} semantic chunks\")\n",
    "\n",
    "            # Analyze chunk statistics\n",
    "            chunk_lengths = [len(chunk.page_content) for chunk in chunks]\n",
    "            print(f\"ðŸ“Š Chunk statistics:\")\n",
    "            print(f\"  Average length: {np.mean(chunk_lengths):.1f} characters\")\n",
    "            print(f\"  Min length: {np.min(chunk_lengths)} characters\")\n",
    "            print(f\"  Max length: {np.max(chunk_lengths)} characters\")\n",
    "\n",
    "            return chunks\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Semantic chunking failed: {e}\")\n",
    "            print(\"ðŸ”„ Falling back to simple text splitting...\")\n",
    "\n",
    "            # Fallback to simple chunking\n",
    "            simple_chunks = []\n",
    "            for doc in docs:\n",
    "                content = doc.page_content\n",
    "                chunk_size = CONFIG[\"chunk_size\"]\n",
    "                overlap = CONFIG[\"chunk_overlap\"]\n",
    "\n",
    "                for i in range(0, len(content), chunk_size - overlap):\n",
    "                    chunk_content = content[i:i + chunk_size]\n",
    "                    if len(chunk_content.strip()) > 50:  # Minimum chunk size\n",
    "                        chunk = Document(\n",
    "                            page_content=chunk_content,\n",
    "                            metadata={**doc.metadata, \"chunk_index\": len(simple_chunks)}\n",
    "                        )\n",
    "                        simple_chunks.append(chunk)\n",
    "\n",
    "            self.chunks = simple_chunks\n",
    "            print(f\"âœ… Created {len(simple_chunks)} simple chunks\")\n",
    "            return simple_chunks\n",
    "\n",
    "    def build_vector_database(self, chunks: List[Document]) -> bool:\n",
    "        \"\"\"Build FAISS vector database from chunks\"\"\"\n",
    "        if not self.embeddings:\n",
    "            raise RuntimeError(\"Embeddings not initialized\")\n",
    "\n",
    "        print(\"ðŸ—„ï¸  Building FAISS vector database...\")\n",
    "\n",
    "        try:\n",
    "            self.vector_db = FAISS.from_documents(chunks, embedding=self.embeddings)\n",
    "            print(\"âœ… Vector database created successfully!\")\n",
    "            print(f\"ðŸ“š Indexed {len(chunks)} chunks\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to build vector database: {e}\")\n",
    "            return False\n",
    "\n",
    "    def search_similar_chunks(self, query: str, k: int = 5) -> List[Document]:\n",
    "        \"\"\"Search for similar chunks\"\"\"\n",
    "        if not self.vector_db:\n",
    "            raise RuntimeError(\"Vector database not initialized\")\n",
    "\n",
    "        try:\n",
    "            results = self.vector_db.similarity_search(query, k=k)\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Search failed: {e}\")\n",
    "            return []\n",
    "\n",
    "# Initialize vector database manager\n",
    "vector_manager = VectorDatabaseManager(CONFIG[\"embedding_model\"])\n",
    "\n",
    "# Initialize embeddings\n",
    "if vector_manager.initialize_embeddings():\n",
    "    print(\"ðŸŽ¯ Ready to process documents!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6118b4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process documents and build vector database\n",
    "print(\"ðŸš€ Starting document processing pipeline...\")\n",
    "\n",
    "# Create semantic chunks\n",
    "chunks = vector_manager.create_semantic_chunks(docs)\n",
    "\n",
    "# Build vector database\n",
    "if vector_manager.build_vector_database(chunks):\n",
    "    print(\"âœ… Document processing pipeline completed!\")\n",
    "\n",
    "    # Test similarity search\n",
    "    test_query = \"OOP lÃ  gÃ¬\"\n",
    "    print(f\"\\nðŸ” Testing similarity search with query: '{test_query}'\")\n",
    "\n",
    "    results = vector_manager.search_similar_chunks(test_query, k=3)\n",
    "\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\nðŸ“„ Result {i}:\")\n",
    "        print(f\"Source: {result.metadata.get('source', 'Unknown')}\")\n",
    "        print(f\"Content: {result.page_content[:200]}...\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ Failed to build vector database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eff769",
   "metadata": {},
   "source": [
    "## 4. Retrieval System Implementation\n",
    "\n",
    "Implementing a context-aware retrieval system with diversity controls for better question generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90ed962",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextAwareRetriever:\n",
    "    \"\"\"Enhanced retriever with context awareness and diversity\"\"\"\n",
    "\n",
    "    def __init__(self, vector_db: FAISS, diversity_threshold: float = 0.7):\n",
    "        self.vector_db = vector_db\n",
    "        self.diversity_threshold = diversity_threshold\n",
    "\n",
    "    def retrieve_diverse_contexts(self, query: str, k: int = 5) -> List[Document]:\n",
    "        \"\"\"Retrieve documents with semantic diversity\"\"\"\n",
    "        # Get more candidates than needed\n",
    "        candidates = self.vector_db.similarity_search(query, k=k*2)\n",
    "\n",
    "        if not candidates:\n",
    "            return []\n",
    "\n",
    "        # Select diverse documents\n",
    "        selected = [candidates[0]]  # Always include the most relevant\n",
    "\n",
    "        for candidate in candidates[1:]:\n",
    "            if len(selected) >= k:\n",
    "                break\n",
    "\n",
    "            # Check diversity with already selected documents\n",
    "            is_diverse = True\n",
    "            for selected_doc in selected:\n",
    "                similarity = self._calculate_similarity(\n",
    "                    candidate.page_content,\n",
    "                    selected_doc.page_content\n",
    "                )\n",
    "                if similarity > self.diversity_threshold:\n",
    "                    is_diverse = False\n",
    "                    break\n",
    "\n",
    "            if is_diverse:\n",
    "                selected.append(candidate)\n",
    "\n",
    "        return selected[:k]\n",
    "\n",
    "    def _calculate_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Calculate text similarity (simplified implementation)\"\"\"\n",
    "        words1 = set(text1.lower().split())\n",
    "        words2 = set(text2.lower().split())\n",
    "\n",
    "        if not words1 or not words2:\n",
    "            return 0.0\n",
    "\n",
    "        intersection = words1.intersection(words2)\n",
    "        union = words1.union(words2)\n",
    "\n",
    "        return len(intersection) / len(union) if union else 0.0\n",
    "\n",
    "    def retrieve_by_topic(self, topic: str, k: int = 5) -> List[Document]:\n",
    "        \"\"\"Retrieve documents relevant to a specific topic\"\"\"\n",
    "        topic_keywords = {\n",
    "            \"OOP\": [\"Ä‘á»‘i tÆ°á»£ng\", \"class\", \"object\", \"káº¿ thá»«a\", \"Ä‘Ã³ng gÃ³i\"],\n",
    "            \"inheritance\": [\"káº¿ thá»«a\", \"class cha\", \"class con\", \"extends\"],\n",
    "            \"data structures\": [\"cáº¥u trÃºc dá»¯ liá»‡u\", \"array\", \"list\", \"stack\", \"queue\"]\n",
    "        }\n",
    "\n",
    "        # Create enhanced query with topic keywords\n",
    "        keywords = topic_keywords.get(topic.lower(), [topic])\n",
    "        enhanced_query = f\"{topic} {' '.join(keywords)}\"\n",
    "\n",
    "        return self.retrieve_diverse_contexts(enhanced_query, k)\n",
    "\n",
    "    def get_context_summary(self, documents: List[Document]) -> str:\n",
    "        \"\"\"Generate a summary of the retrieved contexts\"\"\"\n",
    "        if not documents:\n",
    "            return \"No relevant context found.\"\n",
    "\n",
    "        # Combine and truncate content\n",
    "        combined_content = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "\n",
    "        # Limit context length\n",
    "        max_length = 2000\n",
    "        if len(combined_content) > max_length:\n",
    "            combined_content = combined_content[:max_length] + \"...\"\n",
    "\n",
    "        return combined_content\n",
    "\n",
    "# Initialize the enhanced retriever\n",
    "if vector_manager.vector_db:\n",
    "    retriever = ContextAwareRetriever(\n",
    "        vector_manager.vector_db,\n",
    "        CONFIG[\"diversity_threshold\"]\n",
    "    )\n",
    "    print(\"âœ… Context-aware retriever initialized!\")\n",
    "\n",
    "    # Test diverse retrieval\n",
    "    test_topics = [\"OOP\", \"inheritance\", \"data structures\"]\n",
    "\n",
    "    print(\"\\nðŸ§ª Testing diverse retrieval for different topics:\")\n",
    "    for topic in test_topics:\n",
    "        results = retriever.retrieve_by_topic(topic, k=2)\n",
    "        print(f\"\\nðŸ“š Topic: {topic}\")\n",
    "        print(f\"  Retrieved {len(results)} diverse documents\")\n",
    "\n",
    "        for i, doc in enumerate(results, 1):\n",
    "            print(f\"  Doc {i}: {doc.page_content[:100]}...\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ Vector database not available for retriever initialization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35c08eb",
   "metadata": {},
   "source": [
    "## 5. MCQ Generation with LLM\n",
    "\n",
    "Now let's implement the question generation system using a Large Language Model with structured JSON output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73d5941",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCQGenerator:\n",
    "    \"\"\"Generates MCQs using LLM with structured output\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.llm = None\n",
    "        self.is_initialized = False\n",
    "\n",
    "    def initialize_llm(self, model_name: str = None) -> bool:\n",
    "        \"\"\"Initialize the LLM for question generation\"\"\"\n",
    "        model_name = model_name or CONFIG[\"llm_model\"]\n",
    "\n",
    "        print(f\"ðŸ¤– Initializing LLM: {model_name}\")\n",
    "        print(\"âš ï¸  Note: This requires significant memory and may take time...\")\n",
    "\n",
    "        try:\n",
    "            # For demonstration, we'll use a mock LLM\n",
    "            # In production, uncomment the code below\n",
    "\n",
    "            # # Check for HuggingFace token\n",
    "            # token_path = Path(\"../api_key/hugging_face_token.txt\")\n",
    "            # hf_token = None\n",
    "            # if token_path.exists():\n",
    "            #     with token_path.open(\"r\") as f:\n",
    "            #         hf_token = f.read().strip()\n",
    "\n",
    "            # # Configure quantization for memory efficiency\n",
    "            # bnb_config = BitsAndBytesConfig(\n",
    "            #     load_in_4bit=True,\n",
    "            #     bnb_4bit_use_double_quant=True,\n",
    "            #     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            #     bnb_4bit_quant_type=\"nf4\"\n",
    "            # )\n",
    "\n",
    "            # # Load model\n",
    "            # model = AutoModelForCausalLM.from_pretrained(\n",
    "            #     model_name,\n",
    "            #     quantization_config=bnb_config,\n",
    "            #     low_cpu_mem_usage=True,\n",
    "            #     device_map=\"auto\",\n",
    "            #     token=hf_token\n",
    "            # )\n",
    "\n",
    "            # tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            # tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "            # model_pipeline = pipeline(\n",
    "            #     \"text-generation\",\n",
    "            #     model=model,\n",
    "            #     tokenizer=tokenizer,\n",
    "            #     max_new_tokens=CONFIG[\"max_tokens\"],\n",
    "            #     temperature=CONFIG[\"generation_temperature\"],\n",
    "            #     pad_token_id=tokenizer.eos_token_id,\n",
    "            #     device_map=\"auto\"\n",
    "            # )\n",
    "\n",
    "            # self.llm = HuggingFacePipeline(pipeline=model_pipeline)\n",
    "\n",
    "            # For demonstration, create a mock LLM\n",
    "            self.llm = self._create_mock_llm()\n",
    "            self.is_initialized = True\n",
    "\n",
    "            print(\"âœ… LLM initialized successfully!\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to initialize LLM: {e}\")\n",
    "            print(\"ðŸ”„ Using mock LLM for demonstration...\")\n",
    "            self.llm = self._create_mock_llm()\n",
    "            self.is_initialized = True\n",
    "            return True\n",
    "\n",
    "    def _create_mock_llm(self):\n",
    "        \"\"\"Create a mock LLM for demonstration purposes\"\"\"\n",
    "        class MockLLM:\n",
    "            def __call__(self, prompt):\n",
    "                # Mock response based on context analysis\n",
    "                if \"OOP\" in prompt or \"Ä‘á»‘i tÆ°á»£ng\" in prompt:\n",
    "                    return '''\n",
    "{\n",
    "    \"question\": \"OOP (Object-Oriented Programming) lÃ  gÃ¬?\",\n",
    "    \"options\": {\n",
    "        \"A\": \"Má»™t mÃ´ hÃ¬nh láº­p trÃ¬nh dá»±a trÃªn khÃ¡i niá»‡m Ä‘á»‘i tÆ°á»£ng\",\n",
    "        \"B\": \"Má»™t há»‡ quáº£n trá»‹ cÆ¡ sá»Ÿ dá»¯ liá»‡u\",\n",
    "        \"C\": \"Má»™t framework phÃ¡t triá»ƒn web\",\n",
    "        \"D\": \"Má»™t phÆ°Æ¡ng phÃ¡p kiá»ƒm thá»­ pháº§n má»m\"\n",
    "    },\n",
    "    \"correct_answer\": \"A\",\n",
    "    \"explanation\": \"OOP lÃ  viáº¿t táº¯t cá»§a Object-Oriented Programming, má»™t mÃ´ hÃ¬nh láº­p trÃ¬nh tá»• chá»©c mÃ£ nguá»“n xung quanh cÃ¡c Ä‘á»‘i tÆ°á»£ng thay vÃ¬ cÃ¡c hÃ m vÃ  logic.\",\n",
    "    \"topic\": \"Programming Fundamentals\",\n",
    "    \"difficulty\": \"medium\",\n",
    "    \"question_type\": \"definition\"\n",
    "}\n",
    "'''\n",
    "                elif \"káº¿ thá»«a\" in prompt or \"inheritance\" in prompt:\n",
    "                    return '''\n",
    "{\n",
    "    \"question\": \"Inheritance (Káº¿ thá»«a) trong OOP cÃ³ lá»£i Ã­ch gÃ¬?\",\n",
    "    \"options\": {\n",
    "        \"A\": \"TÃ¡i sá»­ dá»¥ng code vÃ  dá»… dÃ ng má»Ÿ rá»™ng\",\n",
    "        \"B\": \"TÄƒng tá»‘c Ä‘á»™ thá»±c thi chÆ°Æ¡ng trÃ¬nh\",\n",
    "        \"C\": \"Giáº£m dung lÆ°á»£ng file thá»±c thi\",\n",
    "        \"D\": \"Cáº£i thiá»‡n báº£o máº­t cá»§a á»©ng dá»¥ng\"\n",
    "    },\n",
    "    \"correct_answer\": \"A\",\n",
    "    \"explanation\": \"Inheritance cho phÃ©p class con káº¿ thá»«a thuá»™c tÃ­nh vÃ  phÆ°Æ¡ng thá»©c tá»« class cha, giÃºp tÃ¡i sá»­ dá»¥ng code vÃ  dá»… dÃ ng má»Ÿ rá»™ng tÃ­nh nÄƒng.\",\n",
    "    \"topic\": \"OOP Principles\",\n",
    "    \"difficulty\": \"medium\",\n",
    "    \"question_type\": \"application\"\n",
    "}\n",
    "'''\n",
    "                else:\n",
    "                    return '''\n",
    "{\n",
    "    \"question\": \"Cáº¥u trÃºc dá»¯ liá»‡u nÃ o hoáº¡t Ä‘á»™ng theo nguyÃªn lÃ½ LIFO?\",\n",
    "    \"options\": {\n",
    "        \"A\": \"Queue (HÃ ng Ä‘á»£i)\",\n",
    "        \"B\": \"Stack (NgÄƒn xáº¿p)\",\n",
    "        \"C\": \"Array (Máº£ng)\",\n",
    "        \"D\": \"Linked List (Danh sÃ¡ch liÃªn káº¿t)\"\n",
    "    },\n",
    "    \"correct_answer\": \"B\",\n",
    "    \"explanation\": \"Stack hoáº¡t Ä‘á»™ng theo nguyÃªn lÃ½ LIFO (Last In First Out), pháº§n tá»­ Ä‘Æ°á»£c thÃªm vÃ o cuá»‘i cÃ¹ng sáº½ Ä‘Æ°á»£c láº¥y ra Ä‘áº§u tiÃªn.\",\n",
    "    \"topic\": \"Data Structures\",\n",
    "    \"difficulty\": \"easy\",\n",
    "    \"question_type\": \"definition\"\n",
    "}\n",
    "'''\n",
    "\n",
    "        return MockLLM()\n",
    "\n",
    "    def generate_mcq_from_context(self, context: str, topic: str,\n",
    "                                 difficulty: DifficultyLevel = DifficultyLevel.MEDIUM,\n",
    "                                 question_type: QuestionType = QuestionType.DEFINITION) -> MCQQuestion:\n",
    "        \"\"\"Generate MCQ from provided context\"\"\"\n",
    "        if not self.is_initialized:\n",
    "            raise RuntimeError(\"LLM not initialized. Call initialize_llm() first.\")\n",
    "\n",
    "        # Create prompt\n",
    "        prompt = self._create_prompt(context, topic, difficulty, question_type)\n",
    "\n",
    "        # Generate response\n",
    "        response = self.llm(prompt)\n",
    "\n",
    "        # Parse JSON response\n",
    "        mcq = self._parse_response(response, context, topic)\n",
    "\n",
    "        return mcq\n",
    "\n",
    "    def _create_prompt(self, context: str, topic: str, difficulty: DifficultyLevel,\n",
    "                      question_type: QuestionType) -> str:\n",
    "        \"\"\"Create structured prompt for MCQ generation\"\"\"\n",
    "\n",
    "        prompt_template = \"\"\"\n",
    "Báº¡n lÃ  má»™t chuyÃªn gia giÃ¡o dá»¥c vÃ  thiáº¿t káº¿ cÃ¢u há»i. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  táº¡o ra má»™t cÃ¢u há»i tráº¯c nghiá»‡m cháº¥t lÆ°á»£ng cao tá»« ná»™i dung Ä‘Æ°á»£c cung cáº¥p.\n",
    "\n",
    "YÃªu cáº§u:\n",
    "1. Táº¡o má»™t cÃ¢u há»i rÃµ rÃ ng, khÃ´ng mÆ¡ há»“\n",
    "2. Cung cáº¥p Ä‘Ãºng 4 lá»±a chá»n (A, B, C, D)\n",
    "3. Chá»‰ cÃ³ má»™t Ä‘Ã¡p Ã¡n Ä‘Ãºng\n",
    "4. CÃ¡c phÆ°Æ¡ng Ã¡n sai pháº£i há»£p lÃ½ nhÆ°ng rÃµ rÃ ng lÃ  sai\n",
    "5. Bao gá»“m giáº£i thÃ­ch cho Ä‘Ã¡p Ã¡n Ä‘Ãºng\n",
    "\n",
    "Ná»™i dung: {context}\n",
    "Chá»§ Ä‘á»: {topic}\n",
    "Má»©c Ä‘á»™ khÃ³: {difficulty}\n",
    "Loáº¡i cÃ¢u há»i: {question_type}\n",
    "\n",
    "Tráº£ vá» chá»‰ dÆ°á»›i dáº¡ng JSON há»£p lá»‡ vá»›i cáº¥u trÃºc sau:\n",
    "{{\n",
    "    \"question\": \"CÃ¢u há»i cá»§a báº¡n\",\n",
    "    \"options\": {{\n",
    "        \"A\": \"Lá»±a chá»n A\",\n",
    "        \"B\": \"Lá»±a chá»n B\",\n",
    "        \"C\": \"Lá»±a chá»n C\",\n",
    "        \"D\": \"Lá»±a chá»n D\"\n",
    "    }},\n",
    "    \"correct_answer\": \"A\",\n",
    "    \"explanation\": \"Giáº£i thÃ­ch chi tiáº¿t\",\n",
    "    \"topic\": \"{topic}\",\n",
    "    \"difficulty\": \"{difficulty}\",\n",
    "    \"question_type\": \"{question_type}\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "        return prompt_template.format(\n",
    "            context=context[:1500],  # Limit context length\n",
    "            topic=topic,\n",
    "            difficulty=difficulty.value,\n",
    "            question_type=question_type.value\n",
    "        )\n",
    "\n",
    "    def _parse_response(self, response: str, context: str, topic: str) -> MCQQuestion:\n",
    "        \"\"\"Parse LLM response and create MCQQuestion object\"\"\"\n",
    "        try:\n",
    "            # Extract JSON from response\n",
    "            json_start = response.rfind(\"{\")\n",
    "            json_end = response.rfind(\"}\") + 1\n",
    "\n",
    "            if json_start == -1 or json_end == 0:\n",
    "                raise ValueError(\"No JSON found in response\")\n",
    "\n",
    "            json_text = response[json_start:json_end]\n",
    "            response_data = json.loads(json_text)\n",
    "\n",
    "            # Create MCQ options\n",
    "            options = []\n",
    "            for label, text in response_data[\"options\"].items():\n",
    "                is_correct = label == response_data[\"correct_answer\"]\n",
    "                options.append(MCQOption(label, text, is_correct))\n",
    "\n",
    "            # Create MCQ object\n",
    "            mcq = MCQQuestion(\n",
    "                question=response_data[\"question\"],\n",
    "                context=context[:500] + \"...\" if len(context) > 500 else context,\n",
    "                options=options,\n",
    "                explanation=response_data.get(\"explanation\", \"\"),\n",
    "                difficulty=response_data.get(\"difficulty\", \"medium\"),\n",
    "                topic=topic,\n",
    "                question_type=response_data.get(\"question_type\", \"definition\"),\n",
    "                source=\"Generated from documents\",\n",
    "                confidence_score=0.0  # Will be calculated later\n",
    "            )\n",
    "\n",
    "            return mcq\n",
    "\n",
    "        except (json.JSONDecodeError, KeyError) as e:\n",
    "            raise ValueError(f\"Failed to parse LLM response: {e}\")\n",
    "\n",
    "# Initialize MCQ generator\n",
    "mcq_generator = MCQGenerator()\n",
    "if mcq_generator.initialize_llm():\n",
    "    print(\"ðŸŽ¯ MCQ Generator ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16bcf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test single MCQ generation\n",
    "print(\"ðŸ§ª Testing MCQ generation...\")\n",
    "\n",
    "# Get context for OOP topic\n",
    "if 'retriever' in locals():\n",
    "    context_docs = retriever.retrieve_by_topic(\"OOP\", k=2)\n",
    "    context = retriever.get_context_summary(context_docs)\n",
    "\n",
    "    print(f\"ðŸ“„ Context length: {len(context)} characters\")\n",
    "    print(f\"ðŸ“„ Context preview: {context[:200]}...\")\n",
    "\n",
    "    # Generate MCQ\n",
    "    mcq = mcq_generator.generate_mcq_from_context(\n",
    "        context=context,\n",
    "        topic=\"Object-Oriented Programming\",\n",
    "        difficulty=DifficultyLevel.MEDIUM,\n",
    "        question_type=QuestionType.DEFINITION\n",
    "    )\n",
    "\n",
    "    # Display the generated MCQ\n",
    "    print(f\"\\nðŸŽ¯ Generated MCQ:\")\n",
    "    print(f\"Question: {mcq.question}\")\n",
    "    print(f\"\\nOptions:\")\n",
    "    for option in mcq.options:\n",
    "        marker = \"âœ…\" if option.is_correct else \"  \"\n",
    "        print(f\"  {marker} {option.label}: {option.text}\")\n",
    "\n",
    "    print(f\"\\nCorrect Answer: {next(opt.label for opt in mcq.options if opt.is_correct)}\")\n",
    "    print(f\"Explanation: {mcq.explanation}\")\n",
    "    print(f\"Topic: {mcq.topic}\")\n",
    "    print(f\"Difficulty: {mcq.difficulty}\")\n",
    "    print(f\"Question Type: {mcq.question_type}\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ Retriever not available. Cannot test MCQ generation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fcffba",
   "metadata": {},
   "source": [
    "## 6. Prompt Engineering for Different Question Types\n",
    "\n",
    "Let's explore specialized prompts for different types of questions to improve generation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162c2dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedPromptManager:\n",
    "    \"\"\"Manages specialized prompts for different question types\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.templates = self._initialize_templates()\n",
    "\n",
    "    def _initialize_templates(self) -> Dict[str, str]:\n",
    "        \"\"\"Initialize prompt templates for different question types\"\"\"\n",
    "        base_instruction = \"\"\"\n",
    "Báº¡n lÃ  má»™t chuyÃªn gia giÃ¡o dá»¥c vÃ  thiáº¿t káº¿ cÃ¢u há»i. Táº¡o má»™t cÃ¢u há»i tráº¯c nghiá»‡m cháº¥t lÆ°á»£ng cao tá»« ná»™i dung Ä‘Æ°á»£c cung cáº¥p.\n",
    "\n",
    "YÃªu cáº§u chung:\n",
    "- CÃ¢u há»i rÃµ rÃ ng, khÃ´ng mÆ¡ há»“\n",
    "- ÄÃºng 4 lá»±a chá»n (A, B, C, D)\n",
    "- Chá»‰ má»™t Ä‘Ã¡p Ã¡n Ä‘Ãºng\n",
    "- PhÆ°Æ¡ng Ã¡n sai há»£p lÃ½ nhÆ°ng rÃµ rÃ ng sai\n",
    "- Giáº£i thÃ­ch chi tiáº¿t cho Ä‘Ã¡p Ã¡n Ä‘Ãºng\n",
    "\"\"\"\n",
    "\n",
    "        return {\n",
    "            QuestionType.DEFINITION: f\"\"\"\n",
    "{base_instruction}\n",
    "\n",
    "YÃªu cáº§u Ä‘áº·c biá»‡t cho cÃ¢u há»i Äá»ŠNH NGHÄ¨A:\n",
    "- Táº­p trung vÃ o Ä‘á»‹nh nghÄ©a chÃ­nh xÃ¡c cá»§a thuáº­t ngá»¯, khÃ¡i niá»‡m\n",
    "- CÃ¡c phÆ°Æ¡ng Ã¡n sai thÆ°á»ng lÃ  Ä‘á»‹nh nghÄ©a cá»§a khÃ¡i niá»‡m khÃ¡c hoáº·c hiá»ƒu láº§m phá»• biáº¿n\n",
    "- Sá»­ dá»¥ng ngÃ´n ngá»¯ Ä‘Æ¡n giáº£n, dá»… hiá»ƒu\n",
    "- VÃ­ dá»¥: \"X lÃ  gÃ¬?\", \"Äá»‹nh nghÄ©a cá»§a Y lÃ  gÃ¬?\"\n",
    "\n",
    "Ná»™i dung: {{context}}\n",
    "Chá»§ Ä‘á»: {{topic}}\n",
    "Má»©c Ä‘á»™: {{difficulty}}\n",
    "\n",
    "JSON Output:\n",
    "\"\"\",\n",
    "\n",
    "            QuestionType.APPLICATION: f\"\"\"\n",
    "{base_instruction}\n",
    "\n",
    "YÃªu cáº§u Ä‘áº·c biá»‡t cho cÃ¢u há»i á»¨NG Dá»¤NG:\n",
    "- Táº¡o tÃ¬nh huá»‘ng thá»±c táº¿ cáº§n Ã¡p dá»¥ng kiáº¿n thá»©c\n",
    "- Há»i \"khi nÃ o sá»­ dá»¥ng\", \"trong trÆ°á»ng há»£p nÃ o\", \"vÃ­ dá»¥ nÃ o\"\n",
    "- CÃ¡c phÆ°Æ¡ng Ã¡n sai lÃ  á»©ng dá»¥ng khÃ´ng phÃ¹ há»£p hoáº·c sai ngá»¯ cáº£nh\n",
    "- Káº¿t ná»‘i lÃ½ thuyáº¿t vá»›i thá»±c tiá»…n\n",
    "\n",
    "Ná»™i dung: {{context}}\n",
    "Chá»§ Ä‘á»: {{topic}}\n",
    "Má»©c Ä‘á»™: {{difficulty}}\n",
    "\n",
    "JSON Output:\n",
    "\"\"\",\n",
    "\n",
    "            QuestionType.COMPARISON: f\"\"\"\n",
    "{base_instruction}\n",
    "\n",
    "YÃªu cáº§u Ä‘áº·c biá»‡t cho cÃ¢u há»i SO SÃNH:\n",
    "- So sÃ¡nh 2-3 khÃ¡i niá»‡m, phÆ°Æ¡ng phÃ¡p, ká»¹ thuáº­t\n",
    "- Táº­p trung vÃ o Ä‘iá»ƒm khÃ¡c biá»‡t hoáº·c giá»‘ng nhau chÃ­nh\n",
    "- CÃ¡c phÆ°Æ¡ng Ã¡n sai thÆ°á»ng Ä‘áº£o ngÆ°á»£c Ä‘áº·c Ä‘iá»ƒm hoáº·c nháº§m láº«n\n",
    "- VÃ­ dá»¥: \"KhÃ¡c biá»‡t giá»¯a X vÃ  Y lÃ  gÃ¬?\"\n",
    "\n",
    "Ná»™i dung: {{context}}\n",
    "Chá»§ Ä‘á»: {{topic}}\n",
    "Má»©c Ä‘á»™: {{difficulty}}\n",
    "\n",
    "JSON Output:\n",
    "\"\"\",\n",
    "\n",
    "            QuestionType.ANALYSIS: f\"\"\"\n",
    "{base_instruction}\n",
    "\n",
    "YÃªu cáº§u Ä‘áº·c biá»‡t cho cÃ¢u há»i PHÃ‚N TÃCH:\n",
    "- YÃªu cáº§u phÃ¢n tÃ­ch code, sÆ¡ Ä‘á»“, hoáº·c tÃ¬nh huá»‘ng phá»©c táº¡p\n",
    "- Kiá»ƒm tra tÆ° duy logic vÃ  kháº£ nÄƒng suy luáº­n\n",
    "- CÃ¢u há»i cÃ³ thá»ƒ cÃ³ nhiá»u bÆ°á»›c suy luáº­n\n",
    "- CÃ¡c phÆ°Æ¡ng Ã¡n sai lÃ  káº¿t luáº­n sai hoáº·c thiáº¿u logic\n",
    "\n",
    "Ná»™i dung: {{context}}\n",
    "Chá»§ Ä‘á»: {{topic}}\n",
    "Má»©c Ä‘á»™: {{difficulty}}\n",
    "\n",
    "JSON Output:\n",
    "\"\"\",\n",
    "\n",
    "            QuestionType.EVALUATION: f\"\"\"\n",
    "{base_instruction}\n",
    "\n",
    "YÃªu cáº§u Ä‘áº·c biá»‡t cho cÃ¢u há»i ÄÃNH GIÃ:\n",
    "- ÄÃ¡nh giÃ¡ Æ°u nhÆ°á»£c Ä‘iá»ƒm, hiá»‡u quáº£, phÃ¹ há»£p\n",
    "- CÃ¢u há»i dáº¡ng \"phÆ°Æ¡ng phÃ¡p nÃ o tá»‘t nháº¥t\", \"khi nÃ o nÃªn chá»n\"\n",
    "- YÃªu cáº§u cÃ¢n nháº¯c nhiá»u yáº¿u tá»‘\n",
    "- CÃ¡c phÆ°Æ¡ng Ã¡n cáº§n cÃ³ Ä‘á»™ há»£p lÃ½ cao\n",
    "\n",
    "Ná»™i dung: {{context}}\n",
    "Chá»§ Ä‘á»: {{topic}}\n",
    "Má»©c Ä‘á»™: {{difficulty}}\n",
    "\n",
    "JSON Output:\n",
    "\"\"\"\n",
    "        }\n",
    "\n",
    "    def get_prompt(self, question_type: QuestionType, context: str,\n",
    "                   topic: str, difficulty: DifficultyLevel) -> str:\n",
    "        \"\"\"Get specialized prompt for question type\"\"\"\n",
    "        template = self.templates.get(question_type, self.templates[QuestionType.DEFINITION])\n",
    "\n",
    "        return template.format(\n",
    "            context=context[:1200],  # Limit context length\n",
    "            topic=topic,\n",
    "            difficulty=difficulty.value\n",
    "        )\n",
    "\n",
    "    def generate_examples(self) -> Dict[QuestionType, str]:\n",
    "        \"\"\"Generate example questions for each type\"\"\"\n",
    "        examples = {\n",
    "            QuestionType.DEFINITION: \"\"\"\n",
    "VÃ­ dá»¥ cÃ¢u há»i Ä‘á»‹nh nghÄ©a:\n",
    "\"Encapsulation (ÄÃ³ng gÃ³i) trong OOP lÃ  gÃ¬?\"\n",
    "A) áº¨n giáº¥u chi tiáº¿t triá»ƒn khai vÃ  chá»‰ Ä‘á»ƒ lá»™ interface cáº§n thiáº¿t âœ…\n",
    "B) Káº¿ thá»«a thuá»™c tÃ­nh tá»« class cha\n",
    "C) Táº¡o nhiá»u hÃ¬nh thá»©c khÃ¡c nhau cá»§a cÃ¹ng má»™t phÆ°Æ¡ng thá»©c\n",
    "D) Tá»• chá»©c code thÃ nh cÃ¡c module riÃªng biá»‡t\n",
    "\"\"\",\n",
    "\n",
    "            QuestionType.APPLICATION: \"\"\"\n",
    "VÃ­ dá»¥ cÃ¢u há»i á»©ng dá»¥ng:\n",
    "\"Trong trÆ°á»ng há»£p nÃ o nÃªn sá»­ dá»¥ng Stack?\"\n",
    "A) Khi cáº§n truy cáº­p ngáº«u nhiÃªn vÃ o cÃ¡c pháº§n tá»­\n",
    "B) Khi cáº§n xá»­ lÃ½ theo thá»© tá»± LIFO (Last In First Out) âœ…\n",
    "C) Khi cáº§n sáº¯p xáº¿p dá»¯ liá»‡u tá»± Ä‘á»™ng\n",
    "D) Khi cáº§n chia sáº» dá»¯ liá»‡u giá»¯a nhiá»u thread\n",
    "\"\"\",\n",
    "\n",
    "            QuestionType.COMPARISON: \"\"\"\n",
    "VÃ­ dá»¥ cÃ¢u há»i so sÃ¡nh:\n",
    "\"KhÃ¡c biá»‡t chÃ­nh giá»¯a Array vÃ  Linked List lÃ  gÃ¬?\"\n",
    "A) Array cho phÃ©p truy cáº­p ngáº«u nhiÃªn, Linked List truy cáº­p tuáº§n tá»± âœ…\n",
    "B) Array chá»‰ lÆ°u sá»‘, Linked List lÆ°u má»i kiá»ƒu dá»¯ liá»‡u\n",
    "C) Array khÃ´ng thá»ƒ thay Ä‘á»•i kÃ­ch thÆ°á»›c, Linked List cÃ³ thá»ƒ\n",
    "D) Array nhanh hÆ¡n trong má»i trÆ°á»ng há»£p\n",
    "\"\"\",\n",
    "\n",
    "            QuestionType.ANALYSIS: \"\"\"\n",
    "VÃ­ dá»¥ cÃ¢u há»i phÃ¢n tÃ­ch:\n",
    "\"Äoáº¡n code sau vi pháº¡m nguyÃªn lÃ½ OOP nÃ o?\n",
    "class Bird:\n",
    "    def fly(self): pass\n",
    "class Penguin(Bird):\n",
    "    def fly(self): raise Exception('Cannot fly')\"\n",
    "\n",
    "A) Encapsulation\n",
    "B) Liskov Substitution Principle âœ…\n",
    "C) Single Responsibility\n",
    "D) Open/Closed Principle\n",
    "\"\"\",\n",
    "\n",
    "            QuestionType.EVALUATION: \"\"\"\n",
    "VÃ­ dá»¥ cÃ¢u há»i Ä‘Ã¡nh giÃ¡:\n",
    "\"Khi nÃ o nÃªn chá»n Composition thay vÃ¬ Inheritance?\"\n",
    "A) Khi muá»‘n má»‘i quan há»‡ \"is-a\" rÃµ rÃ ng\n",
    "B) Khi cáº§n flexibility vÃ  trÃ¡nh tight coupling âœ…\n",
    "C) Khi muá»‘n tiáº¿t kiá»‡m memory\n",
    "D) Khi class cha cÃ³ Ã­t phÆ°Æ¡ng thá»©c\n",
    "\"\"\"\n",
    "        }\n",
    "\n",
    "        return examples\n",
    "\n",
    "# Initialize advanced prompt manager\n",
    "prompt_manager = AdvancedPromptManager()\n",
    "\n",
    "# Display examples\n",
    "print(\"ðŸ“ Prompt Engineering Examples:\")\n",
    "examples = prompt_manager.generate_examples()\n",
    "\n",
    "for q_type, example in examples.items():\n",
    "    print(f\"\\n{q_type.value.upper()} Questions:\")\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cf43a1",
   "metadata": {},
   "source": [
    "## 7. Quality Validation System\n",
    "\n",
    "Implementing automatic quality checks to ensure generated MCQs meet educational standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db723245",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QualityValidator:\n",
    "    \"\"\"Comprehensive quality validation for MCQs\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.min_question_length = 10\n",
    "        self.max_question_length = 200\n",
    "        self.min_explanation_length = 20\n",
    "        self.min_option_length = 5\n",
    "        self.max_option_length = 150\n",
    "\n",
    "    def validate_mcq(self, mcq: MCQQuestion) -> Tuple[bool, Dict[str, Any]]:\n",
    "        \"\"\"Comprehensive MCQ validation with detailed feedback\"\"\"\n",
    "        results = {\n",
    "            \"is_valid\": True,\n",
    "            \"issues\": [],\n",
    "            \"warnings\": [],\n",
    "            \"scores\": {}\n",
    "        }\n",
    "\n",
    "        # Check basic structure\n",
    "        structure_score = self._check_structure(mcq, results)\n",
    "        results[\"scores\"][\"structure\"] = structure_score\n",
    "\n",
    "        # Check content quality\n",
    "        content_score = self._check_content_quality(mcq, results)\n",
    "        results[\"scores\"][\"content\"] = content_score\n",
    "\n",
    "        # Check distractor quality\n",
    "        distractor_score = self._check_distractor_quality(mcq, results)\n",
    "        results[\"scores\"][\"distractors\"] = distractor_score\n",
    "\n",
    "        # Check language quality\n",
    "        language_score = self._check_language_quality(mcq, results)\n",
    "        results[\"scores\"][\"language\"] = language_score\n",
    "\n",
    "        # Overall validation\n",
    "        results[\"is_valid\"] = len(results[\"issues\"]) == 0\n",
    "        results[\"overall_score\"] = np.mean(list(results[\"scores\"].values()))\n",
    "\n",
    "        return results[\"is_valid\"], results\n",
    "\n",
    "    def _check_structure(self, mcq: MCQQuestion, results: Dict) -> float:\n",
    "        \"\"\"Check MCQ structural requirements\"\"\"\n",
    "        score = 100.0\n",
    "\n",
    "        # Check question length\n",
    "        if len(mcq.question) < self.min_question_length:\n",
    "            results[\"issues\"].append(\"Question too short\")\n",
    "            score -= 20\n",
    "        elif len(mcq.question) > self.max_question_length:\n",
    "            results[\"warnings\"].append(\"Question might be too long\")\n",
    "            score -= 10\n",
    "\n",
    "        # Check options count\n",
    "        if len(mcq.options) != 4:\n",
    "            results[\"issues\"].append(f\"Must have exactly 4 options, found {len(mcq.options)}\")\n",
    "            score -= 30\n",
    "\n",
    "        # Check for single correct answer\n",
    "        correct_count = sum(1 for opt in mcq.options if opt.is_correct)\n",
    "        if correct_count != 1:\n",
    "            results[\"issues\"].append(f\"Must have exactly 1 correct answer, found {correct_count}\")\n",
    "            score -= 40\n",
    "\n",
    "        # Check explanation\n",
    "        if len(mcq.explanation) < self.min_explanation_length:\n",
    "            results[\"issues\"].append(\"Explanation too short\")\n",
    "            score -= 15\n",
    "\n",
    "        return max(score, 0)\n",
    "\n",
    "    def _check_content_quality(self, mcq: MCQQuestion, results: Dict) -> float:\n",
    "        \"\"\"Check content quality and relevance\"\"\"\n",
    "        score = 100.0\n",
    "\n",
    "        # Check for distinct options\n",
    "        option_texts = [opt.text.lower().strip() for opt in mcq.options]\n",
    "        if len(set(option_texts)) != len(option_texts):\n",
    "            results[\"issues\"].append(\"Options must be distinct\")\n",
    "            score -= 25\n",
    "\n",
    "        # Check option length consistency\n",
    "        option_lengths = [len(opt.text) for opt in mcq.options]\n",
    "        length_variance = np.var(option_lengths)\n",
    "        if length_variance > 1000:  # High variance in option lengths\n",
    "            results[\"warnings\"].append(\"Large variation in option lengths\")\n",
    "            score -= 10\n",
    "\n",
    "        # Check for obvious patterns\n",
    "        labels = [opt.label for opt in mcq.options]\n",
    "        if not labels == [\"A\", \"B\", \"C\", \"D\"]:\n",
    "            results[\"issues\"].append(\"Options must be labeled A, B, C, D\")\n",
    "            score -= 15\n",
    "\n",
    "        return max(score, 0)\n",
    "\n",
    "    def _check_distractor_quality(self, mcq: MCQQuestion, results: Dict) -> float:\n",
    "        \"\"\"Check quality of incorrect options (distractors)\"\"\"\n",
    "        score = 100.0\n",
    "\n",
    "        distractors = [opt for opt in mcq.options if not opt.is_correct]\n",
    "\n",
    "        # Check distractor plausibility (simplified check)\n",
    "        for i, distractor in enumerate(distractors):\n",
    "            if len(distractor.text) < self.min_option_length:\n",
    "                results[\"warnings\"].append(f\"Distractor {distractor.label} too short\")\n",
    "                score -= 5\n",
    "\n",
    "            # Check for obviously wrong answers (very simple check)\n",
    "            if any(word in distractor.text.lower() for word in [\"khÃ´ng\", \"never\", \"impossible\"]):\n",
    "                results[\"warnings\"].append(f\"Distractor {distractor.label} might be too obviously wrong\")\n",
    "                score -= 10\n",
    "\n",
    "        return max(score, 0)\n",
    "\n",
    "    def _check_language_quality(self, mcq: MCQQuestion, results: Dict) -> float:\n",
    "        \"\"\"Check language quality and clarity\"\"\"\n",
    "        score = 100.0\n",
    "\n",
    "        # Check for common issues\n",
    "        text_to_check = mcq.question + \" \" + \" \".join(opt.text for opt in mcq.options)\n",
    "\n",
    "        # Check for excessive repetition\n",
    "        words = text_to_check.lower().split()\n",
    "        word_freq = {}\n",
    "        for word in words:\n",
    "            if len(word) > 3:  # Only check longer words\n",
    "                word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "        repeated_words = [word for word, freq in word_freq.items() if freq > 3]\n",
    "        if repeated_words:\n",
    "            results[\"warnings\"].append(f\"Repeated words detected: {repeated_words[:3]}\")\n",
    "            score -= 5\n",
    "\n",
    "        # Check for question clarity indicators\n",
    "        if not mcq.question.strip().endswith(\"?\"):\n",
    "            results[\"warnings\"].append(\"Question should end with question mark\")\n",
    "            score -= 5\n",
    "\n",
    "        return max(score, 0)\n",
    "\n",
    "    def calculate_confidence_score(self, mcq: MCQQuestion) -> float:\n",
    "        \"\"\"Calculate overall confidence score for the MCQ\"\"\"\n",
    "        is_valid, validation_results = self.validate_mcq(mcq)\n",
    "\n",
    "        if not is_valid:\n",
    "            return 0.0\n",
    "\n",
    "        # Base score from validation\n",
    "        base_score = validation_results[\"overall_score\"]\n",
    "\n",
    "        # Bonus factors\n",
    "        bonus = 0\n",
    "\n",
    "        # Good explanation length\n",
    "        if 50 <= len(mcq.explanation) <= 200:\n",
    "            bonus += 5\n",
    "\n",
    "        # Balanced option lengths\n",
    "        option_lengths = [len(opt.text) for opt in mcq.options]\n",
    "        if max(option_lengths) - min(option_lengths) < 30:\n",
    "            bonus += 5\n",
    "\n",
    "        # Appropriate question length\n",
    "        if 30 <= len(mcq.question) <= 120:\n",
    "            bonus += 5\n",
    "\n",
    "        final_score = min(base_score + bonus, 100.0)\n",
    "        return final_score\n",
    "\n",
    "    def generate_quality_report(self, mcqs: List[MCQQuestion]) -> Dict[str, Any]:\n",
    "        \"\"\"Generate comprehensive quality report for multiple MCQs\"\"\"\n",
    "        if not mcqs:\n",
    "            return {\"error\": \"No MCQs provided\"}\n",
    "\n",
    "        report = {\n",
    "            \"total_questions\": len(mcqs),\n",
    "            \"valid_questions\": 0,\n",
    "            \"average_score\": 0.0,\n",
    "            \"score_distribution\": {},\n",
    "            \"common_issues\": {},\n",
    "            \"recommendations\": []\n",
    "        }\n",
    "\n",
    "        scores = []\n",
    "        all_issues = []\n",
    "\n",
    "        for mcq in mcqs:\n",
    "            is_valid, validation = self.validate_mcq(mcq)\n",
    "            score = self.calculate_confidence_score(mcq)\n",
    "\n",
    "            if is_valid:\n",
    "                report[\"valid_questions\"] += 1\n",
    "\n",
    "            scores.append(score)\n",
    "            all_issues.extend(validation[\"issues\"])\n",
    "\n",
    "        # Calculate statistics\n",
    "        report[\"average_score\"] = np.mean(scores)\n",
    "        report[\"median_score\"] = np.median(scores)\n",
    "        report[\"min_score\"] = np.min(scores)\n",
    "        report[\"max_score\"] = np.max(scores)\n",
    "\n",
    "        # Score distribution\n",
    "        score_ranges = [(0, 40), (40, 60), (60, 80), (80, 100)]\n",
    "        for low, high in score_ranges:\n",
    "            count = sum(1 for s in scores if low <= s < high)\n",
    "            report[\"score_distribution\"][f\"{low}-{high}\"] = count\n",
    "\n",
    "        # Common issues\n",
    "        issue_counts = {}\n",
    "        for issue in all_issues:\n",
    "            issue_counts[issue] = issue_counts.get(issue, 0) + 1\n",
    "        report[\"common_issues\"] = dict(sorted(issue_counts.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "        # Generate recommendations\n",
    "        if report[\"average_score\"] < 70:\n",
    "            report[\"recommendations\"].append(\"Consider improving prompt engineering\")\n",
    "        if report[\"valid_questions\"] / report[\"total_questions\"] < 0.8:\n",
    "            report[\"recommendations\"].append(\"Review structural validation rules\")\n",
    "        if \"Options must be distinct\" in report[\"common_issues\"]:\n",
    "            report[\"recommendations\"].append(\"Improve distractor generation\")\n",
    "\n",
    "        return report\n",
    "\n",
    "# Initialize quality validator\n",
    "quality_validator = QualityValidator()\n",
    "\n",
    "# Test validation with the previously generated MCQ\n",
    "if 'mcq' in locals():\n",
    "    print(\"ðŸ§ª Testing Quality Validation...\")\n",
    "\n",
    "    is_valid, validation_results = quality_validator.validate_mcq(mcq)\n",
    "    confidence_score = quality_validator.calculate_confidence_score(mcq)\n",
    "\n",
    "    print(f\"\\nðŸ“Š Validation Results:\")\n",
    "    print(f\"Valid: {'âœ…' if is_valid else 'âŒ'}\")\n",
    "    print(f\"Overall Score: {validation_results['overall_score']:.1f}/100\")\n",
    "    print(f\"Confidence Score: {confidence_score:.1f}/100\")\n",
    "\n",
    "    print(f\"\\nðŸ“‹ Detailed Scores:\")\n",
    "    for category, score in validation_results['scores'].items():\n",
    "        print(f\"  {category.title()}: {score:.1f}/100\")\n",
    "\n",
    "    if validation_results['issues']:\n",
    "        print(f\"\\nâŒ Issues found:\")\n",
    "        for issue in validation_results['issues']:\n",
    "            print(f\"  - {issue}\")\n",
    "\n",
    "    if validation_results['warnings']:\n",
    "        print(f\"\\nâš ï¸  Warnings:\")\n",
    "        for warning in validation_results['warnings']:\n",
    "            print(f\"  - {warning}\")\n",
    "\n",
    "    # Update MCQ confidence score\n",
    "    mcq.confidence_score = confidence_score\n",
    "    print(f\"\\nâœ… MCQ confidence score updated to {confidence_score:.1f}\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ No MCQ available for validation testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9854fb50",
   "metadata": {},
   "source": [
    "## 8. Difficulty Assessment and Classification\n",
    "\n",
    "Implementing intelligent difficulty assessment based on cognitive load and concept complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db13318",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifficultyAnalyzer:\n",
    "    \"\"\"Analyzes and classifies question difficulty\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.difficulty_indicators = {\n",
    "            DifficultyLevel.EASY: {\n",
    "                \"keywords\": [\"lÃ  gÃ¬\", \"Ä‘á»‹nh nghÄ©a\", \"vÃ­ dá»¥\", \"Ä‘Æ¡n giáº£n\", \"cÆ¡ báº£n\"],\n",
    "                \"concepts\": 1,\n",
    "                \"cognitive_load\": \"recall\",\n",
    "                \"max_word_count\": 15\n",
    "            },\n",
    "            DifficultyLevel.MEDIUM: {\n",
    "                \"keywords\": [\"so sÃ¡nh\", \"khÃ¡c biá»‡t\", \"á»©ng dá»¥ng\", \"khi nÃ o\", \"táº¡i sao\"],\n",
    "                \"concepts\": 2,\n",
    "                \"cognitive_load\": \"comprehension\",\n",
    "                \"max_word_count\": 25\n",
    "            },\n",
    "            DifficultyLevel.HARD: {\n",
    "                \"keywords\": [\"phÃ¢n tÃ­ch\", \"Ä‘Ã¡nh giÃ¡\", \"tá»‘i Æ°u\", \"thiáº¿t káº¿\", \"giáº£i thÃ­ch\"],\n",
    "                \"concepts\": 3,\n",
    "                \"cognitive_load\": \"analysis\",\n",
    "                \"max_word_count\": 35\n",
    "            },\n",
    "            DifficultyLevel.EXPERT: {\n",
    "                \"keywords\": [\"tá»•ng há»£p\", \"sÃ¡ng táº¡o\", \"nghiÃªn cá»©u\", \"phÃ¡t triá»ƒn\", \"optimization\"],\n",
    "                \"concepts\": 4,\n",
    "                \"cognitive_load\": \"synthesis\",\n",
    "                \"max_word_count\": 50\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Technical term complexity levels\n",
    "        self.technical_terms = {\n",
    "            \"basic\": [\"Ä‘á»‘i tÆ°á»£ng\", \"class\", \"function\", \"variable\"],\n",
    "            \"intermediate\": [\"inheritance\", \"polymorphism\", \"encapsulation\", \"abstraction\"],\n",
    "            \"advanced\": [\"design pattern\", \"algorithm complexity\", \"data structure optimization\"],\n",
    "            \"expert\": [\"architectural pattern\", \"performance tuning\", \"scalability analysis\"]\n",
    "        }\n",
    "\n",
    "    def assess_difficulty(self, mcq: MCQQuestion) -> Dict[str, Any]:\n",
    "        \"\"\"Comprehensive difficulty assessment\"\"\"\n",
    "        analysis = {\n",
    "            \"predicted_difficulty\": DifficultyLevel.MEDIUM,\n",
    "            \"confidence\": 0.0,\n",
    "            \"factors\": {},\n",
    "            \"recommendations\": []\n",
    "        }\n",
    "\n",
    "        # Analyze different factors\n",
    "        keyword_score = self._analyze_keywords(mcq.question)\n",
    "        complexity_score = self._analyze_complexity(mcq)\n",
    "        cognitive_score = self._analyze_cognitive_load(mcq)\n",
    "        technical_score = self._analyze_technical_terms(mcq)\n",
    "\n",
    "        analysis[\"factors\"] = {\n",
    "            \"keyword_difficulty\": keyword_score,\n",
    "            \"content_complexity\": complexity_score,\n",
    "            \"cognitive_load\": cognitive_score,\n",
    "            \"technical_complexity\": technical_score\n",
    "        }\n",
    "\n",
    "        # Calculate overall difficulty\n",
    "        overall_score = np.mean([keyword_score, complexity_score, cognitive_score, technical_score])\n",
    "        analysis[\"predicted_difficulty\"] = self._score_to_difficulty(overall_score)\n",
    "        analysis[\"confidence\"] = min(100, max(50, 80 + (overall_score - 50) * 0.4))\n",
    "\n",
    "        # Generate recommendations\n",
    "        analysis[\"recommendations\"] = self._generate_recommendations(analysis)\n",
    "\n",
    "        return analysis\n",
    "\n",
    "    def _analyze_keywords(self, question: str) -> float:\n",
    "        \"\"\"Analyze question keywords for difficulty indicators\"\"\"\n",
    "        question_lower = question.lower()\n",
    "        scores = []\n",
    "\n",
    "        for difficulty, indicators in self.difficulty_indicators.items():\n",
    "            score = sum(2 if keyword in question_lower else 0\n",
    "                       for keyword in indicators[\"keywords\"])\n",
    "            if score > 0:\n",
    "                scores.append((difficulty, score))\n",
    "\n",
    "        if not scores:\n",
    "            return 50.0  # Default medium difficulty\n",
    "\n",
    "        # Weight by difficulty level\n",
    "        difficulty_weights = {\n",
    "            DifficultyLevel.EASY: 25,\n",
    "            DifficultyLevel.MEDIUM: 50,\n",
    "            DifficultyLevel.HARD: 75,\n",
    "            DifficultyLevel.EXPERT: 90\n",
    "        }\n",
    "\n",
    "        weighted_score = sum(difficulty_weights[diff] * score for diff, score in scores)\n",
    "        total_weight = sum(score for _, score in scores)\n",
    "\n",
    "        return weighted_score / total_weight if total_weight > 0 else 50.0\n",
    "\n",
    "    def _analyze_complexity(self, mcq: MCQQuestion) -> float:\n",
    "        \"\"\"Analyze content complexity\"\"\"\n",
    "        factors = []\n",
    "\n",
    "        # Question length complexity\n",
    "        question_words = len(mcq.question.split())\n",
    "        if question_words <= 10:\n",
    "            factors.append(30)\n",
    "        elif question_words <= 20:\n",
    "            factors.append(50)\n",
    "        elif question_words <= 30:\n",
    "            factors.append(70)\n",
    "        else:\n",
    "            factors.append(85)\n",
    "\n",
    "        # Option complexity\n",
    "        option_lengths = [len(opt.text.split()) for opt in mcq.options]\n",
    "        avg_option_length = np.mean(option_lengths)\n",
    "\n",
    "        if avg_option_length <= 5:\n",
    "            factors.append(35)\n",
    "        elif avg_option_length <= 10:\n",
    "            factors.append(55)\n",
    "        else:\n",
    "            factors.append(75)\n",
    "\n",
    "        # Explanation complexity\n",
    "        explanation_words = len(mcq.explanation.split())\n",
    "        if explanation_words <= 15:\n",
    "            factors.append(40)\n",
    "        elif explanation_words <= 30:\n",
    "            factors.append(60)\n",
    "        else:\n",
    "            factors.append(80)\n",
    "\n",
    "        return np.mean(factors)\n",
    "\n",
    "    def _analyze_cognitive_load(self, mcq: MCQQuestion) -> float:\n",
    "        \"\"\"Analyze cognitive load based on Bloom's taxonomy\"\"\"\n",
    "        cognitive_indicators = {\n",
    "            \"remember\": [\"lÃ  gÃ¬\", \"Ä‘á»‹nh nghÄ©a\", \"liá»‡t kÃª\", \"nháº­n diá»‡n\"],\n",
    "            \"understand\": [\"giáº£i thÃ­ch\", \"mÃ´ táº£\", \"so sÃ¡nh\", \"phÃ¢n biá»‡t\"],\n",
    "            \"apply\": [\"sá»­ dá»¥ng\", \"Ã¡p dá»¥ng\", \"thá»±c hiá»‡n\", \"giáº£i quyáº¿t\"],\n",
    "            \"analyze\": [\"phÃ¢n tÃ­ch\", \"phÃ¢n chia\", \"so sÃ¡nh\", \"Ä‘á»‘i chiáº¿u\"],\n",
    "            \"evaluate\": [\"Ä‘Ã¡nh giÃ¡\", \"phÃª bÃ¬nh\", \"lá»±a chá»n\", \"quyáº¿t Ä‘á»‹nh\"],\n",
    "            \"create\": [\"táº¡o ra\", \"thiáº¿t káº¿\", \"phÃ¡t triá»ƒn\", \"sÃ¡ng táº¡o\"]\n",
    "        }\n",
    "\n",
    "        cognitive_scores = {\n",
    "            \"remember\": 20,\n",
    "            \"understand\": 35,\n",
    "            \"apply\": 50,\n",
    "            \"analyze\": 70,\n",
    "            \"evaluate\": 85,\n",
    "            \"create\": 95\n",
    "        }\n",
    "\n",
    "        text = (mcq.question + \" \" + mcq.explanation).lower()\n",
    "\n",
    "        detected_levels = []\n",
    "        for level, indicators in cognitive_indicators.items():\n",
    "            if any(indicator in text for indicator in indicators):\n",
    "                detected_levels.append(cognitive_scores[level])\n",
    "\n",
    "        return max(detected_levels) if detected_levels else 50.0\n",
    "\n",
    "    def _analyze_technical_terms(self, mcq: MCQQuestion) -> float:\n",
    "        \"\"\"Analyze technical term complexity\"\"\"\n",
    "        all_text = (mcq.question + \" \" + mcq.explanation + \" \" +\n",
    "                   \" \".join(opt.text for opt in mcq.options)).lower()\n",
    "\n",
    "        complexity_scores = {\n",
    "            \"basic\": 30,\n",
    "            \"intermediate\": 50,\n",
    "            \"advanced\": 75,\n",
    "            \"expert\": 90\n",
    "        }\n",
    "\n",
    "        detected_levels = []\n",
    "        for level, terms in self.technical_terms.items():\n",
    "            if any(term.lower() in all_text for term in terms):\n",
    "                detected_levels.append(complexity_scores[level])\n",
    "\n",
    "        return max(detected_levels) if detected_levels else 40.0\n",
    "\n",
    "    def _score_to_difficulty(self, score: float) -> DifficultyLevel:\n",
    "        \"\"\"Convert numeric score to difficulty level\"\"\"\n",
    "        if score < 35:\n",
    "            return DifficultyLevel.EASY\n",
    "        elif score < 60:\n",
    "            return DifficultyLevel.MEDIUM\n",
    "        elif score < 80:\n",
    "            return DifficultyLevel.HARD\n",
    "        else:\n",
    "            return DifficultyLevel.EXPERT\n",
    "\n",
    "    def _generate_recommendations(self, analysis: Dict) -> List[str]:\n",
    "        \"\"\"Generate recommendations based on difficulty analysis\"\"\"\n",
    "        recommendations = []\n",
    "\n",
    "        predicted = analysis[\"predicted_difficulty\"]\n",
    "        factors = analysis[\"factors\"]\n",
    "\n",
    "        if factors[\"keyword_difficulty\"] < 30:\n",
    "            recommendations.append(\"Consider using more specific terminology\")\n",
    "\n",
    "        if factors[\"content_complexity\"] > 80:\n",
    "            recommendations.append(\"Question might be too complex - consider simplification\")\n",
    "\n",
    "        if factors[\"cognitive_load\"] < 30:\n",
    "            recommendations.append(\"Question tests only basic recall - consider higher-order thinking\")\n",
    "\n",
    "        if analysis[\"confidence\"] < 60:\n",
    "            recommendations.append(\"Difficulty assessment has low confidence - review question design\")\n",
    "\n",
    "        return recommendations\n",
    "\n",
    "    def calibrate_difficulty_distribution(self, mcqs: List[MCQQuestion]) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze difficulty distribution across multiple MCQs\"\"\"\n",
    "        if not mcqs:\n",
    "            return {}\n",
    "\n",
    "        analyses = [self.assess_difficulty(mcq) for mcq in mcqs]\n",
    "\n",
    "        # Count difficulty levels\n",
    "        difficulty_counts = {}\n",
    "        confidence_scores = []\n",
    "\n",
    "        for analysis in analyses:\n",
    "            diff_level = analysis[\"predicted_difficulty\"].value\n",
    "            difficulty_counts[diff_level] = difficulty_counts.get(diff_level, 0) + 1\n",
    "            confidence_scores.append(analysis[\"confidence\"])\n",
    "\n",
    "        # Calculate statistics\n",
    "        total = len(mcqs)\n",
    "        distribution = {level: count/total * 100 for level, count in difficulty_counts.items()}\n",
    "\n",
    "        return {\n",
    "            \"total_questions\": total,\n",
    "            \"difficulty_distribution\": distribution,\n",
    "            \"average_confidence\": np.mean(confidence_scores),\n",
    "            \"recommended_distribution\": {\n",
    "                \"easy\": 30,\n",
    "                \"medium\": 50,\n",
    "                \"hard\": 15,\n",
    "                \"expert\": 5\n",
    "            },\n",
    "            \"needs_rebalancing\": self._check_balance(distribution)\n",
    "        }\n",
    "\n",
    "    def _check_balance(self, distribution: Dict[str, float]) -> bool:\n",
    "        \"\"\"Check if difficulty distribution needs rebalancing\"\"\"\n",
    "        recommended = {\"easy\": 30, \"medium\": 50, \"hard\": 15, \"expert\": 5}\n",
    "\n",
    "        for level, target_percent in recommended.items():\n",
    "            actual_percent = distribution.get(level, 0)\n",
    "            if abs(actual_percent - target_percent) > 20:  # More than 20% deviation\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "# Initialize difficulty analyzer\n",
    "difficulty_analyzer = DifficultyAnalyzer()\n",
    "\n",
    "# Test difficulty analysis\n",
    "if 'mcq' in locals():\n",
    "    print(\"ðŸ§ª Testing Difficulty Analysis...\")\n",
    "\n",
    "    analysis = difficulty_analyzer.assess_difficulty(mcq)\n",
    "\n",
    "    print(f\"\\nðŸ“Š Difficulty Analysis Results:\")\n",
    "    print(f\"Predicted Difficulty: {analysis['predicted_difficulty'].value.upper()}\")\n",
    "    print(f\"Confidence: {analysis['confidence']:.1f}%\")\n",
    "\n",
    "    print(f\"\\nðŸ“‹ Factor Analysis:\")\n",
    "    for factor, score in analysis['factors'].items():\n",
    "        print(f\"  {factor.replace('_', ' ').title()}: {score:.1f}/100\")\n",
    "\n",
    "    if analysis['recommendations']:\n",
    "        print(f\"\\nðŸ’¡ Recommendations:\")\n",
    "        for rec in analysis['recommendations']:\n",
    "            print(f\"  â€¢ {rec}\")\n",
    "\n",
    "    # Compare with intended difficulty\n",
    "    intended_difficulty = mcq.difficulty\n",
    "    predicted_difficulty = analysis['predicted_difficulty'].value\n",
    "\n",
    "    if intended_difficulty == predicted_difficulty:\n",
    "        print(f\"\\nâœ… Difficulty assessment matches intended level: {intended_difficulty}\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸  Difficulty mismatch:\")\n",
    "        print(f\"   Intended: {intended_difficulty}\")\n",
    "        print(f\"   Predicted: {predicted_difficulty}\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ No MCQ available for difficulty analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6aafc9",
   "metadata": {},
   "source": [
    "## 9. Batch Generation and Testing\n",
    "\n",
    "Implementing scalable batch processing for generating multiple MCQs with error handling and retry mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9e0ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchMCQGenerator:\n",
    "    \"\"\"Batch processing for MCQ generation with error handling\"\"\"\n",
    "\n",
    "    def __init__(self, mcq_generator: MCQGenerator, retriever: ContextAwareRetriever,\n",
    "                 quality_validator: QualityValidator, difficulty_analyzer: DifficultyAnalyzer):\n",
    "        self.mcq_generator = mcq_generator\n",
    "        self.retriever = retriever\n",
    "        self.quality_validator = quality_validator\n",
    "        self.difficulty_analyzer = difficulty_analyzer\n",
    "        self.max_retries = 3\n",
    "        self.min_quality_score = 60.0\n",
    "\n",
    "    def generate_batch(self, topics: List[str],\n",
    "                      count_per_topic: int = 3,\n",
    "                      difficulties: Optional[List[DifficultyLevel]] = None,\n",
    "                      question_types: Optional[List[QuestionType]] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Generate batch of MCQs with comprehensive reporting\"\"\"\n",
    "\n",
    "        if difficulties is None:\n",
    "            difficulties = [DifficultyLevel.EASY, DifficultyLevel.MEDIUM, DifficultyLevel.HARD]\n",
    "\n",
    "        if question_types is None:\n",
    "            question_types = [QuestionType.DEFINITION, QuestionType.APPLICATION]\n",
    "\n",
    "        total_target = len(topics) * count_per_topic\n",
    "        results = {\n",
    "            \"mcqs\": [],\n",
    "            \"failed_generations\": [],\n",
    "            \"statistics\": {},\n",
    "            \"quality_report\": {},\n",
    "            \"difficulty_analysis\": {}\n",
    "        }\n",
    "\n",
    "        print(f\"ðŸš€ Starting batch generation...\")\n",
    "        print(f\"ðŸ“Š Target: {total_target} MCQs across {len(topics)} topics\")\n",
    "        print(f\"ðŸŽ¯ Difficulties: {[d.value for d in difficulties]}\")\n",
    "        print(f\"ðŸ“ Question types: {[q.value for q in question_types]}\")\n",
    "\n",
    "        # Generate MCQs for each topic\n",
    "        for topic_idx, topic in enumerate(topics, 1):\n",
    "            print(f\"\\\\nðŸ“š Processing topic {topic_idx}/{len(topics)}: {topic}\")\n",
    "\n",
    "            topic_mcqs = []\n",
    "            topic_failures = []\n",
    "\n",
    "            for q_idx in range(count_per_topic):\n",
    "                # Cycle through difficulties and question types\n",
    "                difficulty = difficulties[q_idx % len(difficulties)]\n",
    "                question_type = question_types[q_idx % len(question_types)]\n",
    "\n",
    "                print(f\"  ðŸŽ¯ Generating Q{q_idx+1}: {difficulty.value} {question_type.value}\")\n",
    "\n",
    "                mcq, error = self._generate_single_mcq_with_retry(\n",
    "                    topic, difficulty, question_type\n",
    "                )\n",
    "\n",
    "                if mcq:\n",
    "                    topic_mcqs.append(mcq)\n",
    "                    quality_score = mcq.confidence_score\n",
    "                    print(f\"    âœ… Success (Quality: {quality_score:.1f})\")\n",
    "                else:\n",
    "                    topic_failures.append({\n",
    "                        \"topic\": topic,\n",
    "                        \"difficulty\": difficulty.value,\n",
    "                        \"question_type\": question_type.value,\n",
    "                        \"error\": error\n",
    "                    })\n",
    "                    print(f\"    âŒ Failed: {error}\")\n",
    "\n",
    "            results[\"mcqs\"].extend(topic_mcqs)\n",
    "            results[\"failed_generations\"].extend(topic_failures)\n",
    "\n",
    "            print(f\"  ðŸ“Š Topic summary: {len(topic_mcqs)}/{count_per_topic} successful\")\n",
    "\n",
    "        # Generate comprehensive statistics\n",
    "        results[\"statistics\"] = self._calculate_statistics(results[\"mcqs\"], total_target)\n",
    "        results[\"quality_report\"] = self.quality_validator.generate_quality_report(results[\"mcqs\"])\n",
    "        results[\"difficulty_analysis\"] = self.difficulty_analyzer.calibrate_difficulty_distribution(results[\"mcqs\"])\n",
    "\n",
    "        self._print_batch_summary(results)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _generate_single_mcq_with_retry(self, topic: str, difficulty: DifficultyLevel,\n",
    "                                       question_type: QuestionType) -> Tuple[Optional[MCQQuestion], Optional[str]]:\n",
    "        \"\"\"Generate single MCQ with retry mechanism\"\"\"\n",
    "\n",
    "        for attempt in range(1, self.max_retries + 1):\n",
    "            try:\n",
    "                # Retrieve context\n",
    "                context_docs = self.retriever.retrieve_by_topic(topic, k=3)\n",
    "                if not context_docs:\n",
    "                    return None, f\"No relevant context found for topic: {topic}\"\n",
    "\n",
    "                context = self.retriever.get_context_summary(context_docs)\n",
    "\n",
    "                # Generate MCQ\n",
    "                mcq = self.mcq_generator.generate_mcq_from_context(\n",
    "                    context, topic, difficulty, question_type\n",
    "                )\n",
    "\n",
    "                # Validate quality\n",
    "                confidence_score = self.quality_validator.calculate_confidence_score(mcq)\n",
    "                mcq.confidence_score = confidence_score\n",
    "\n",
    "                if confidence_score >= self.min_quality_score:\n",
    "                    return mcq, None\n",
    "                else:\n",
    "                    if attempt < self.max_retries:\n",
    "                        print(f\"    ðŸ”„ Retry {attempt}: Low quality score ({confidence_score:.1f})\")\n",
    "                        continue\n",
    "                    else:\n",
    "                        return None, f\"Quality too low after {self.max_retries} attempts\"\n",
    "\n",
    "            except Exception as e:\n",
    "                if attempt < self.max_retries:\n",
    "                    print(f\"    ðŸ”„ Retry {attempt}: {str(e)[:50]}...\")\n",
    "                    continue\n",
    "                else:\n",
    "                    return None, f\"Generation failed: {str(e)}\"\n",
    "\n",
    "        return None, \"Max retries exceeded\"\n",
    "\n",
    "    def _calculate_statistics(self, mcqs: List[MCQQuestion], target_count: int) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate generation statistics\"\"\"\n",
    "        if not mcqs:\n",
    "            return {\"error\": \"No MCQs generated\"}\n",
    "\n",
    "        # Basic statistics\n",
    "        stats = {\n",
    "            \"total_generated\": len(mcqs),\n",
    "            \"target_count\": target_count,\n",
    "            \"success_rate\": len(mcqs) / target_count * 100,\n",
    "            \"average_quality\": np.mean([mcq.confidence_score for mcq in mcqs]),\n",
    "            \"quality_distribution\": {}\n",
    "        }\n",
    "\n",
    "        # Quality distribution\n",
    "        quality_ranges = [(0, 40), (40, 60), (60, 80), (80, 100)]\n",
    "        for low, high in quality_ranges:\n",
    "            count = sum(1 for mcq in mcqs if low <= mcq.confidence_score < high)\n",
    "            stats[\"quality_distribution\"][f\"{low}-{high}\"] = count\n",
    "\n",
    "        # Topic distribution\n",
    "        topic_counts = {}\n",
    "        for mcq in mcqs:\n",
    "            topic_counts[mcq.topic] = topic_counts.get(mcq.topic, 0) + 1\n",
    "        stats[\"topic_distribution\"] = topic_counts\n",
    "\n",
    "        # Difficulty distribution\n",
    "        difficulty_counts = {}\n",
    "        for mcq in mcqs:\n",
    "            difficulty_counts[mcq.difficulty] = difficulty_counts.get(mcq.difficulty, 0) + 1\n",
    "        stats[\"difficulty_distribution\"] = difficulty_counts\n",
    "\n",
    "        # Question type distribution\n",
    "        type_counts = {}\n",
    "        for mcq in mcqs:\n",
    "            type_counts[mcq.question_type] = type_counts.get(mcq.question_type, 0) + 1\n",
    "        stats[\"question_type_distribution\"] = type_counts\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def _print_batch_summary(self, results: Dict[str, Any]):\n",
    "        \"\"\"Print comprehensive batch summary\"\"\"\n",
    "        stats = results[\"statistics\"]\n",
    "        quality_report = results[\"quality_report\"]\n",
    "\n",
    "        print(f\"\\\\nðŸŽ‰ Batch Generation Complete!\")\n",
    "        print(f\"={'='*50}\")\n",
    "\n",
    "        print(f\"ðŸ“Š Generation Statistics:\")\n",
    "        print(f\"  Total Generated: {stats['total_generated']}/{stats['target_count']}\")\n",
    "        print(f\"  Success Rate: {stats['success_rate']:.1f}%\")\n",
    "        print(f\"  Average Quality: {stats['average_quality']:.1f}/100\")\n",
    "\n",
    "        print(f\"\\\\nðŸ“ˆ Quality Distribution:\")\n",
    "        for range_str, count in stats['quality_distribution'].items():\n",
    "            print(f\"  {range_str}: {count} questions\")\n",
    "\n",
    "        print(f\"\\\\nðŸŽ¯ Difficulty Distribution:\")\n",
    "        for difficulty, count in stats['difficulty_distribution'].items():\n",
    "            print(f\"  {difficulty.title()}: {count} questions\")\n",
    "\n",
    "        if results[\"failed_generations\"]:\n",
    "            print(f\"\\\\nâŒ Failed Generations: {len(results['failed_generations'])}\")\n",
    "            failure_reasons = {}\n",
    "            for failure in results[\"failed_generations\"]:\n",
    "                reason = failure[\"error\"]\n",
    "                failure_reasons[reason] = failure_reasons.get(reason, 0) + 1\n",
    "\n",
    "            for reason, count in failure_reasons.items():\n",
    "                print(f\"  {reason}: {count} times\")\n",
    "\n",
    "        print(f\"\\\\nðŸ’¡ Recommendations:\")\n",
    "        if quality_report.get(\"recommendations\"):\n",
    "            for rec in quality_report[\"recommendations\"]:\n",
    "                print(f\"  â€¢ {rec}\")\n",
    "\n",
    "        if stats['success_rate'] < 80:\n",
    "            print(f\"  â€¢ Consider adjusting generation parameters\")\n",
    "        if stats['average_quality'] < 70:\n",
    "            print(f\"  â€¢ Review prompt engineering and validation criteria\")\n",
    "\n",
    "    def export_results(self, results: Dict[str, Any], output_file: str):\n",
    "        \"\"\"Export results to JSON file\"\"\"\n",
    "        export_data = {\n",
    "            \"metadata\": {\n",
    "                \"generation_timestamp\": time.time(),\n",
    "                \"total_questions\": len(results[\"mcqs\"]),\n",
    "                \"success_rate\": results[\"statistics\"][\"success_rate\"],\n",
    "                \"average_quality\": results[\"statistics\"][\"average_quality\"]\n",
    "            },\n",
    "            \"questions\": [mcq.to_dict() for mcq in results[\"mcqs\"]],\n",
    "            \"statistics\": results[\"statistics\"],\n",
    "            \"quality_report\": results[\"quality_report\"],\n",
    "            \"difficulty_analysis\": results[\"difficulty_analysis\"],\n",
    "            \"failed_generations\": results[\"failed_generations\"]\n",
    "        }\n",
    "\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(export_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"ðŸ“ Results exported to: {output_file}\")\n",
    "\n",
    "# Initialize batch generator\n",
    "if all(var in locals() for var in ['mcq_generator', 'retriever', 'quality_validator', 'difficulty_analyzer']):\n",
    "    batch_generator = BatchMCQGenerator(\n",
    "        mcq_generator, retriever, quality_validator, difficulty_analyzer\n",
    "    )\n",
    "    print(\"âœ… Batch MCQ Generator initialized!\")\n",
    "else:\n",
    "    print(\"âŒ Required components not available for batch generator initialization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004c4c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run batch generation test\n",
    "if 'batch_generator' in locals():\n",
    "    print(\"ðŸ§ª Testing Batch MCQ Generation...\")\n",
    "\n",
    "    # Define test parameters\n",
    "    test_topics = [\n",
    "        \"Object-Oriented Programming\",\n",
    "        \"Inheritance and Polymorphism\",\n",
    "        \"Data Structures\"\n",
    "    ]\n",
    "\n",
    "    test_difficulties = [\n",
    "        DifficultyLevel.EASY,\n",
    "        DifficultyLevel.MEDIUM,\n",
    "        DifficultyLevel.HARD\n",
    "    ]\n",
    "\n",
    "    test_question_types = [\n",
    "        QuestionType.DEFINITION,\n",
    "        QuestionType.APPLICATION\n",
    "    ]\n",
    "\n",
    "    # Generate batch\n",
    "    batch_results = batch_generator.generate_batch(\n",
    "        topics=test_topics,\n",
    "        count_per_topic=2,  # Generate 2 questions per topic\n",
    "        difficulties=test_difficulties,\n",
    "        question_types=test_question_types\n",
    "    )\n",
    "\n",
    "    # Display sample generated MCQs\n",
    "    print(f\"\\\\nðŸ“ Sample Generated MCQs:\")\n",
    "    print(f\"=\"*60)\n",
    "\n",
    "    for i, mcq in enumerate(batch_results[\"mcqs\"][:3], 1):  # Show first 3 MCQs\n",
    "        print(f\"\\\\nðŸŽ¯ Sample MCQ {i}:\")\n",
    "        print(f\"Topic: {mcq.topic}\")\n",
    "        print(f\"Difficulty: {mcq.difficulty}\")\n",
    "        print(f\"Type: {mcq.question_type}\")\n",
    "        print(f\"Quality Score: {mcq.confidence_score:.1f}/100\")\n",
    "        print(f\"\\\\nQuestion: {mcq.question}\")\n",
    "\n",
    "        print(f\"\\\\nOptions:\")\n",
    "        for option in mcq.options:\n",
    "            marker = \"âœ…\" if option.is_correct else \"  \"\n",
    "            print(f\"  {marker} {option.label}: {option.text}\")\n",
    "\n",
    "        print(f\"\\\\nExplanation: {mcq.explanation}\")\n",
    "        print(f\"-\" * 60)\n",
    "\n",
    "    # Export results\n",
    "    output_filename = f\"batch_mcq_results_{int(time.time())}.json\"\n",
    "    batch_generator.export_results(batch_results, output_filename)\n",
    "\n",
    "else:\n",
    "    print(\"âŒ Batch generator not available for testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82437f85",
   "metadata": {},
   "source": [
    "## 10. Performance Evaluation Metrics\n",
    "\n",
    "Implementing comprehensive evaluation metrics for the RAG-MCQ system including relevance, clarity, generation speed, and success rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f99d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceEvaluator:\n",
    "    \"\"\"Comprehensive performance evaluation for RAG-MCQ system\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.metrics = {}\n",
    "        self.benchmarks = {\n",
    "            \"generation_time_per_question\": 30.0,  # seconds\n",
    "            \"minimum_success_rate\": 80.0,  # percentage\n",
    "            \"minimum_quality_score\": 70.0,  # 0-100\n",
    "            \"maximum_retry_rate\": 20.0  # percentage\n",
    "        }\n",
    "\n",
    "    def evaluate_system_performance(self, batch_results: Dict[str, Any],\n",
    "                                   generation_time: float) -> Dict[str, Any]:\n",
    "        \"\"\"Comprehensive system performance evaluation\"\"\"\n",
    "\n",
    "        mcqs = batch_results[\"mcqs\"]\n",
    "        stats = batch_results[\"statistics\"]\n",
    "\n",
    "        evaluation = {\n",
    "            \"performance_metrics\": {},\n",
    "            \"quality_metrics\": {},\n",
    "            \"efficiency_metrics\": {},\n",
    "            \"recommendations\": [],\n",
    "            \"overall_score\": 0.0\n",
    "        }\n",
    "\n",
    "        # Performance metrics\n",
    "        evaluation[\"performance_metrics\"] = self._calculate_performance_metrics(\n",
    "            mcqs, stats, generation_time\n",
    "        )\n",
    "\n",
    "        # Quality metrics\n",
    "        evaluation[\"quality_metrics\"] = self._calculate_quality_metrics(mcqs)\n",
    "\n",
    "        # Efficiency metrics\n",
    "        evaluation[\"efficiency_metrics\"] = self._calculate_efficiency_metrics(\n",
    "            batch_results, generation_time\n",
    "        )\n",
    "\n",
    "        # Generate recommendations\n",
    "        evaluation[\"recommendations\"] = self._generate_performance_recommendations(evaluation)\n",
    "\n",
    "        # Calculate overall score\n",
    "        evaluation[\"overall_score\"] = self._calculate_overall_score(evaluation)\n",
    "\n",
    "        return evaluation\n",
    "\n",
    "    def _calculate_performance_metrics(self, mcqs: List[MCQQuestion],\n",
    "                                     stats: Dict, generation_time: float) -> Dict[str, float]:\n",
    "        \"\"\"Calculate core performance metrics\"\"\"\n",
    "        total_target = stats.get(\"target_count\", len(mcqs))\n",
    "\n",
    "        metrics = {\n",
    "            \"success_rate\": len(mcqs) / total_target * 100 if total_target > 0 else 0,\n",
    "            \"average_generation_time\": generation_time / len(mcqs) if mcqs else 0,\n",
    "            \"throughput_questions_per_minute\": len(mcqs) / (generation_time / 60) if generation_time > 0 else 0,\n",
    "            \"validity_rate\": sum(1 for mcq in mcqs if mcq.confidence_score >= 60) / len(mcqs) * 100 if mcqs else 0\n",
    "        }\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def _calculate_quality_metrics(self, mcqs: List[MCQQuestion]) -> Dict[str, float]:\n",
    "        \"\"\"Calculate quality-related metrics\"\"\"\n",
    "        if not mcqs:\n",
    "            return {}\n",
    "\n",
    "        confidence_scores = [mcq.confidence_score for mcq in mcqs]\n",
    "\n",
    "        # Content diversity metrics\n",
    "        unique_questions = len(set(mcq.question for mcq in mcqs))\n",
    "        question_diversity = unique_questions / len(mcqs) * 100\n",
    "\n",
    "        # Option quality metrics\n",
    "        avg_option_lengths = []\n",
    "        for mcq in mcqs:\n",
    "            option_lengths = [len(opt.text) for opt in mcq.options]\n",
    "            avg_option_lengths.append(np.mean(option_lengths))\n",
    "\n",
    "        metrics = {\n",
    "            \"average_quality_score\": np.mean(confidence_scores),\n",
    "            \"quality_score_std\": np.std(confidence_scores),\n",
    "            \"min_quality_score\": np.min(confidence_scores),\n",
    "            \"max_quality_score\": np.max(confidence_scores),\n",
    "            \"high_quality_rate\": sum(1 for score in confidence_scores if score >= 80) / len(confidence_scores) * 100,\n",
    "            \"question_diversity\": question_diversity,\n",
    "            \"average_option_length\": np.mean(avg_option_lengths),\n",
    "            \"explanation_completeness\": sum(1 for mcq in mcqs if len(mcq.explanation) >= 50) / len(mcqs) * 100\n",
    "        }\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def _calculate_efficiency_metrics(self, batch_results: Dict, generation_time: float) -> Dict[str, float]:\n",
    "        \"\"\"Calculate efficiency and resource utilization metrics\"\"\"\n",
    "        mcqs = batch_results[\"mcqs\"]\n",
    "        failed_generations = batch_results.get(\"failed_generations\", [])\n",
    "\n",
    "        total_attempts = len(mcqs) + len(failed_generations)\n",
    "\n",
    "        metrics = {\n",
    "            \"retry_rate\": len(failed_generations) / total_attempts * 100 if total_attempts > 0 else 0,\n",
    "            \"resource_efficiency\": len(mcqs) / generation_time if generation_time > 0 else 0,\n",
    "            \"context_utilization\": self._calculate_context_utilization(mcqs),\n",
    "            \"prompt_efficiency\": self._calculate_prompt_efficiency(mcqs)\n",
    "        }\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def _calculate_context_utilization(self, mcqs: List[MCQQuestion]) -> float:\n",
    "        \"\"\"Calculate how well the context is utilized\"\"\"\n",
    "        if not mcqs:\n",
    "            return 0.0\n",
    "\n",
    "        # Simplified metric: average context length vs question relevance\n",
    "        context_lengths = [len(mcq.context) for mcq in mcqs]\n",
    "        quality_scores = [mcq.confidence_score for mcq in mcqs]\n",
    "\n",
    "        # Higher quality with reasonable context length indicates good utilization\n",
    "        avg_context_length = np.mean(context_lengths)\n",
    "        avg_quality = np.mean(quality_scores)\n",
    "\n",
    "        # Optimal context length range: 300-800 characters\n",
    "        if 300 <= avg_context_length <= 800:\n",
    "            length_score = 100\n",
    "        else:\n",
    "            length_score = max(0, 100 - abs(avg_context_length - 550) / 10)\n",
    "\n",
    "        # Combine with quality score\n",
    "        utilization_score = (length_score + avg_quality) / 2\n",
    "        return utilization_score\n",
    "\n",
    "    def _calculate_prompt_efficiency(self, mcqs: List[MCQQuestion]) -> float:\n",
    "        \"\"\"Calculate prompt efficiency based on output quality\"\"\"\n",
    "        if not mcqs:\n",
    "            return 0.0\n",
    "\n",
    "        # Measure consistency in output format and quality\n",
    "        format_consistency = self._check_format_consistency(mcqs)\n",
    "        quality_consistency = self._check_quality_consistency(mcqs)\n",
    "\n",
    "        return (format_consistency + quality_consistency) / 2\n",
    "\n",
    "    def _check_format_consistency(self, mcqs: List[MCQQuestion]) -> float:\n",
    "        \"\"\"Check consistency in MCQ format\"\"\"\n",
    "        if not mcqs:\n",
    "            return 0.0\n",
    "\n",
    "        consistent_count = 0\n",
    "        for mcq in mcqs:\n",
    "            # Check if MCQ follows expected format\n",
    "            has_4_options = len(mcq.options) == 4\n",
    "            has_correct_labels = all(opt.label in [\"A\", \"B\", \"C\", \"D\"] for opt in mcq.options)\n",
    "            has_one_correct = sum(1 for opt in mcq.options if opt.is_correct) == 1\n",
    "            has_explanation = len(mcq.explanation) > 10\n",
    "\n",
    "            if all([has_4_options, has_correct_labels, has_one_correct, has_explanation]):\n",
    "                consistent_count += 1\n",
    "\n",
    "        return consistent_count / len(mcqs) * 100\n",
    "\n",
    "    def _check_quality_consistency(self, mcqs: List[MCQQuestion]) -> float:\n",
    "        \"\"\"Check consistency in quality scores\"\"\"\n",
    "        if not mcqs:\n",
    "            return 0.0\n",
    "\n",
    "        quality_scores = [mcq.confidence_score for mcq in mcqs]\n",
    "        quality_std = np.std(quality_scores)\n",
    "\n",
    "        # Lower standard deviation indicates more consistent quality\n",
    "        # Scale to 0-100 where lower std = higher score\n",
    "        consistency_score = max(0, 100 - quality_std)\n",
    "        return consistency_score\n",
    "\n",
    "    def _generate_performance_recommendations(self, evaluation: Dict) -> List[str]:\n",
    "        \"\"\"Generate recommendations based on performance evaluation\"\"\"\n",
    "        recommendations = []\n",
    "\n",
    "        perf_metrics = evaluation[\"performance_metrics\"]\n",
    "        quality_metrics = evaluation[\"quality_metrics\"]\n",
    "        efficiency_metrics = evaluation[\"efficiency_metrics\"]\n",
    "\n",
    "        # Success rate recommendations\n",
    "        if perf_metrics.get(\"success_rate\", 0) < self.benchmarks[\"minimum_success_rate\"]:\n",
    "            recommendations.append(\"Improve generation stability - success rate below target\")\n",
    "\n",
    "        # Quality recommendations\n",
    "        if quality_metrics.get(\"average_quality_score\", 0) < self.benchmarks[\"minimum_quality_score\"]:\n",
    "            recommendations.append(\"Enhance prompt engineering to improve quality scores\")\n",
    "\n",
    "        # Efficiency recommendations\n",
    "        if perf_metrics.get(\"average_generation_time\", 0) > self.benchmarks[\"generation_time_per_question\"]:\n",
    "            recommendations.append(\"Optimize generation pipeline for better performance\")\n",
    "\n",
    "        if efficiency_metrics.get(\"retry_rate\", 0) > self.benchmarks[\"maximum_retry_rate\"]:\n",
    "            recommendations.append(\"Reduce retry rate by improving initial generation quality\")\n",
    "\n",
    "        # Diversity recommendations\n",
    "        if quality_metrics.get(\"question_diversity\", 0) < 90:\n",
    "            recommendations.append(\"Improve question diversity to avoid repetition\")\n",
    "\n",
    "        # Context utilization recommendations\n",
    "        if efficiency_metrics.get(\"context_utilization\", 0) < 70:\n",
    "            recommendations.append(\"Optimize context retrieval and utilization\")\n",
    "\n",
    "        return recommendations\n",
    "\n",
    "    def _calculate_overall_score(self, evaluation: Dict) -> float:\n",
    "        \"\"\"Calculate overall system performance score\"\"\"\n",
    "        perf_metrics = evaluation[\"performance_metrics\"]\n",
    "        quality_metrics = evaluation[\"quality_metrics\"]\n",
    "        efficiency_metrics = evaluation[\"efficiency_metrics\"]\n",
    "\n",
    "        # Weighted scoring\n",
    "        weights = {\n",
    "            \"success_rate\": 0.25,\n",
    "            \"quality_score\": 0.30,\n",
    "            \"generation_time\": 0.20,\n",
    "            \"efficiency\": 0.25\n",
    "        }\n",
    "\n",
    "        # Normalize metrics to 0-100 scale\n",
    "        success_score = min(100, perf_metrics.get(\"success_rate\", 0))\n",
    "        quality_score = quality_metrics.get(\"average_quality_score\", 0)\n",
    "\n",
    "        # Time score (inverse - lower time = higher score)\n",
    "        time_score = min(100, self.benchmarks[\"generation_time_per_question\"] /\n",
    "                        max(0.1, perf_metrics.get(\"average_generation_time\", 30)) * 100)\n",
    "\n",
    "        efficiency_score = efficiency_metrics.get(\"context_utilization\", 0)\n",
    "\n",
    "        overall_score = (\n",
    "            weights[\"success_rate\"] * success_score +\n",
    "            weights[\"quality_score\"] * quality_score +\n",
    "            weights[\"generation_time\"] * time_score +\n",
    "            weights[\"efficiency\"] * efficiency_score\n",
    "        )\n",
    "\n",
    "        return overall_score\n",
    "\n",
    "    def visualize_performance(self, evaluation: Dict):\n",
    "        \"\"\"Create performance visualization\"\"\"\n",
    "        if not evaluation:\n",
    "            print(\"âŒ No evaluation data available for visualization\")\n",
    "            return\n",
    "\n",
    "        # Create performance dashboard\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('RAG-MCQ System Performance Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "        # 1. Performance Metrics Bar Chart\n",
    "        perf_metrics = evaluation[\"performance_metrics\"]\n",
    "        metrics_names = list(perf_metrics.keys())\n",
    "        metrics_values = list(perf_metrics.values())\n",
    "\n",
    "        axes[0, 0].bar(range(len(metrics_names)), metrics_values, color='skyblue')\n",
    "        axes[0, 0].set_title('Performance Metrics')\n",
    "        axes[0, 0].set_xticks(range(len(metrics_names)))\n",
    "        axes[0, 0].set_xticklabels([name.replace('_', ' ').title() for name in metrics_names],\n",
    "                                  rotation=45, ha='right')\n",
    "        axes[0, 0].set_ylabel('Score/Rate')\n",
    "\n",
    "        # 2. Quality Distribution\n",
    "        quality_metrics = evaluation[\"quality_metrics\"]\n",
    "        quality_names = ['Avg Quality', 'Min Quality', 'Max Quality', 'High Quality Rate']\n",
    "        quality_values = [\n",
    "            quality_metrics.get(\"average_quality_score\", 0),\n",
    "            quality_metrics.get(\"min_quality_score\", 0),\n",
    "            quality_metrics.get(\"max_quality_score\", 0),\n",
    "            quality_metrics.get(\"high_quality_rate\", 0)\n",
    "        ]\n",
    "\n",
    "        axes[0, 1].bar(quality_names, quality_values, color='lightgreen')\n",
    "        axes[0, 1].set_title('Quality Metrics')\n",
    "        axes[0, 1].set_ylabel('Score')\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "        # 3. Efficiency Metrics\n",
    "        efficiency_metrics = evaluation[\"efficiency_metrics\"]\n",
    "        eff_names = list(efficiency_metrics.keys())\n",
    "        eff_values = list(efficiency_metrics.values())\n",
    "\n",
    "        axes[1, 0].bar(eff_names, eff_values, color='orange')\n",
    "        axes[1, 0].set_title('Efficiency Metrics')\n",
    "        axes[1, 0].set_xticks(range(len(eff_names)))\n",
    "        axes[1, 0].set_xticklabels([name.replace('_', ' ').title() for name in eff_names],\n",
    "                                  rotation=45, ha='right')\n",
    "        axes[1, 0].set_ylabel('Score/Rate')\n",
    "\n",
    "        # 4. Overall Score Gauge\n",
    "        overall_score = evaluation[\"overall_score\"]\n",
    "        axes[1, 1].pie([overall_score, 100-overall_score],\n",
    "                      labels=[f'Score: {overall_score:.1f}', ''],\n",
    "                      colors=['lightcoral', 'lightgray'],\n",
    "                      startangle=90)\n",
    "        axes[1, 1].set_title('Overall Performance Score')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Print performance summary\n",
    "        print(f\"\\\\nðŸŽ¯ Performance Summary:\")\n",
    "        print(f\"Overall Score: {overall_score:.1f}/100\")\n",
    "\n",
    "        if overall_score >= 80:\n",
    "            print(\"âœ… Excellent performance!\")\n",
    "        elif overall_score >= 70:\n",
    "            print(\"ðŸŸ¡ Good performance with room for improvement\")\n",
    "        else:\n",
    "            print(\"ðŸ”´ Performance needs significant improvement\")\n",
    "\n",
    "# Initialize performance evaluator\n",
    "performance_evaluator = PerformanceEvaluator()\n",
    "\n",
    "# Evaluate system performance if batch results are available\n",
    "if 'batch_results' in locals():\n",
    "    print(\"ðŸ“Š Evaluating System Performance...\")\n",
    "\n",
    "    # Simulate generation time (in real scenario, this would be measured)\n",
    "    simulated_generation_time = len(batch_results[\"mcqs\"]) * 5  # 5 seconds per question\n",
    "\n",
    "    evaluation = performance_evaluator.evaluate_system_performance(\n",
    "        batch_results, simulated_generation_time\n",
    "    )\n",
    "\n",
    "    # Display evaluation results\n",
    "    print(f\"\\\\nðŸ“ˆ Performance Evaluation Results:\")\n",
    "    print(f\"=\"*60)\n",
    "\n",
    "    print(f\"\\\\nðŸŽ¯ Performance Metrics:\")\n",
    "    for metric, value in evaluation[\"performance_metrics\"].items():\n",
    "        print(f\"  {metric.replace('_', ' ').title()}: {value:.2f}\")\n",
    "\n",
    "    print(f\"\\\\nðŸ† Quality Metrics:\")\n",
    "    for metric, value in evaluation[\"quality_metrics\"].items():\n",
    "        print(f\"  {metric.replace('_', ' ').title()}: {value:.2f}\")\n",
    "\n",
    "    print(f\"\\\\nâš¡ Efficiency Metrics:\")\n",
    "    for metric, value in evaluation[\"efficiency_metrics\"].items():\n",
    "        print(f\"  {metric.replace('_', ' ').title()}: {value:.2f}\")\n",
    "\n",
    "    print(f\"\\\\nðŸ’¯ Overall Score: {evaluation['overall_score']:.1f}/100\")\n",
    "\n",
    "    if evaluation[\"recommendations\"]:\n",
    "        print(f\"\\\\nðŸ’¡ Recommendations:\")\n",
    "        for rec in evaluation[\"recommendations\"]:\n",
    "            print(f\"  â€¢ {rec}\")\n",
    "\n",
    "    # Create visualization\n",
    "    performance_evaluator.visualize_performance(evaluation)\n",
    "\n",
    "else:\n",
    "    print(\"âŒ No batch results available for performance evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537bd142",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "### ðŸŽ‰ What We've Accomplished\n",
    "\n",
    "This notebook demonstrated a comprehensive RAG system for Multiple Choice Question generation with the following key features:\n",
    "\n",
    "#### âœ… Core Components Implemented\n",
    "1. **Document Processing Pipeline** - PDF loading, text extraction, and semantic chunking\n",
    "2. **Vector Database & Embeddings** - FAISS with Vietnamese language support\n",
    "3. **Context-Aware Retrieval** - Diverse document retrieval with similarity thresholds\n",
    "4. **LLM-Powered Generation** - Structured MCQ generation with JSON output\n",
    "5. **Advanced Prompt Engineering** - Specialized prompts for different question types\n",
    "6. **Quality Validation System** - Comprehensive validation with scoring\n",
    "7. **Difficulty Assessment** - Intelligent difficulty classification\n",
    "8. **Batch Processing** - Scalable generation with error handling\n",
    "9. **Performance Evaluation** - Comprehensive metrics and reporting\n",
    "\n",
    "#### ðŸŽ¯ Key Achievements\n",
    "- **Multi-language Support**: Vietnamese language optimization\n",
    "- **Educational Focus**: Question types aligned with learning objectives\n",
    "- **Quality Assurance**: Automatic validation and confidence scoring\n",
    "- **Scalability**: Batch processing capabilities\n",
    "- **Comprehensive Evaluation**: Multiple metrics for system assessment\n",
    "\n",
    "### ðŸš€ Next Steps for Production\n",
    "\n",
    "#### Phase 1: Enhancement & Optimization\n",
    "- [ ] **Real LLM Integration**: Replace mock LLM with actual models (Gemma, Vicuna, etc.)\n",
    "- [ ] **GPU Optimization**: Implement CUDA acceleration for faster processing\n",
    "- [ ] **Memory Management**: Optimize memory usage for large document collections\n",
    "- [ ] **Caching System**: Implement embedding and response caching\n",
    "\n",
    "#### Phase 2: Advanced Features\n",
    "- [ ] **Multi-Modal Support**: Add support for images, diagrams, and code snippets\n",
    "- [ ] **Adaptive Learning**: Implement difficulty adjustment based on user performance\n",
    "- [ ] **Human-in-the-Loop**: Add expert review and feedback mechanisms\n",
    "- [ ] **Multi-Language Expansion**: Support for English and other languages\n",
    "\n",
    "#### Phase 3: Production Deployment\n",
    "- [ ] **Web API Development**: Create REST API for system integration\n",
    "- [ ] **User Interface**: Build web interface for question management\n",
    "- [ ] **Database Integration**: Implement persistent storage for questions and metadata\n",
    "- [ ] **Authentication & Authorization**: Add user management and access control\n",
    "\n",
    "#### Phase 4: Advanced Analytics\n",
    "- [ ] **Learning Analytics**: Track question effectiveness and student performance\n",
    "- [ ] **Content Gap Analysis**: Identify areas needing more questions\n",
    "- [ ] **Automatic Curriculum Mapping**: Align questions with learning objectives\n",
    "- [ ] **Personalization**: Adaptive question selection based on learner profiles\n",
    "\n",
    "### ðŸ“Š System Performance Summary\n",
    "\n",
    "Based on our demonstration:\n",
    "- **Generation Success Rate**: High (with proper configuration)\n",
    "- **Quality Validation**: Comprehensive multi-factor assessment\n",
    "- **Scalability**: Batch processing with error handling\n",
    "- **Flexibility**: Multiple question types and difficulty levels\n",
    "- **Educational Value**: Aligned with pedagogical best practices\n",
    "\n",
    "### ðŸ› ï¸ Technical Requirements for Production\n",
    "\n",
    "#### Hardware Requirements\n",
    "- **GPU**: NVIDIA GPU with 8GB+ VRAM for model inference\n",
    "- **RAM**: 16GB+ system RAM for document processing\n",
    "- **Storage**: 100GB+ for models, embeddings, and document storage\n",
    "- **CPU**: Multi-core processor for parallel document processing\n",
    "\n",
    "#### Software Dependencies\n",
    "- **Python 3.8+** with virtual environment\n",
    "- **CUDA toolkit** for GPU acceleration\n",
    "- **LangChain ecosystem** for RAG pipeline\n",
    "- **Transformers library** for model inference\n",
    "- **FAISS** for vector similarity search\n",
    "- **FastAPI/Streamlit** for web interface\n",
    "\n",
    "### ðŸ“š Educational Impact\n",
    "\n",
    "This RAG-MCQ system can significantly impact education by:\n",
    "- **Reducing Teacher Workload**: Automated question generation\n",
    "- **Improving Assessment Quality**: Consistent, validated questions\n",
    "- **Personalizing Learning**: Adaptive difficulty and topics\n",
    "- **Scaling Education**: Support for large student populations\n",
    "- **Enhancing Learning**: Immediate feedback and explanations\n",
    "\n",
    "### ðŸ”¬ Research Opportunities\n",
    "\n",
    "- **Question Quality Metrics**: Develop better automatic quality assessment\n",
    "- **Distractor Generation**: Improve incorrect option generation\n",
    "- **Cognitive Load Theory**: Apply learning theory to difficulty assessment\n",
    "- **Multi-Document Synthesis**: Generate questions requiring multiple sources\n",
    "- **Real-Time Adaptation**: Dynamic question adjustment during assessment\n",
    "\n",
    "### ðŸ’¡ Final Recommendations\n",
    "\n",
    "1. **Start Small**: Begin with a limited domain and gradually expand\n",
    "2. **Validate Extensively**: Test with real educators and students\n",
    "3. **Iterate Quickly**: Use feedback to improve the system continuously\n",
    "4. **Focus on Quality**: Prioritize question quality over quantity\n",
    "5. **Monitor Performance**: Track all metrics for continuous improvement\n",
    "\n",
    "This demonstration provides a solid foundation for building a production-ready RAG system for MCQ generation that can serve educational institutions, online learning platforms, and assessment organizations."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
