{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0657b5ff",
   "metadata": {},
   "source": [
    "# download libs\n",
    "pip install torch transformers accelerate bitsandbytes langchain langchain-community langchain-experimental langchain-huggingface langchain-chroma langchain-text-splitters langchain-core chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4175c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87f71ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig # for compressing model e.g. 16bits -> 4bits\n",
    "\n",
    "from transformers import (\n",
    "                          AutoTokenizer, # Tokenize Model\n",
    "                          AutoModelForCausalLM,  # LLM Loader - used for loading and using pre-trained models designed for causal language modeling tasks\n",
    "                          pipeline) # pipline to setup llm-task oritented model\n",
    "                                    # pipline(\"text-classification\", model='model', device=0)\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings # huggingface sentence_transformer embedding models\n",
    "from langchain_huggingface.llms import HuggingFacePipeline # like transformer pipeline\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory # Deprecated\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory # Deprecated\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader # PDF Processing\n",
    "from langchain.chains import ConversationalRetrievalChain # Deprecated\n",
    "from langchain_experimental.text_splitter import SemanticChunker # module for chunking text\n",
    "\n",
    "from langchain_chroma import Chroma # AI-native vector databases (ai-native mean built for handle large-scale AI workloads efficiently)\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter # recursively divide text, then merge them together if merge_size < chunk_size\n",
    "from langchain_core.runnables import RunnablePassthrough # Use for testing (make 'example' easy to execute and experiment with)\n",
    "from langchain_core.output_parsers import StrOutputParser # format LLM's output text into (list, dict or any custom structure we can work with)\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910833ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read PDF file\n",
    "Loader = PyPDFLoader\n",
    "# FILE_PATH = \"25 Thu·∫≠t Ng·ªØ AI - Machine Learning d·ªÖ hi·ªÉu cho ng∆∞·ªùi m·ªõi.pdf\"\n",
    "FILE_PATH = \"iot_security_report.pdf\"\n",
    "loader = Loader(FILE_PATH)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cda4b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3759cf",
   "metadata": {},
   "source": [
    "[bkai-foundation-model 2024](https://huggingface.co/bkai-foundation-models/vietnamese-bi-encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6028fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name = \"bkai-foundation-models/vietnamese-bi-encoder\",\n",
    "    model_kwargs = {'device': 'cuda'},\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    ") # convert text to vector (not chunking yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3bda34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runtime:\n",
    "# + bkai-foundation-models/vietnamese-bi-encoder: 3 mins\n",
    "# + keepitreal/vietnamese-sbert: 3mins\n",
    "semantic_splitter = SemanticChunker(\n",
    "    embeddings=embeddings,\n",
    "    buffer_size=1, # total sentence collected before perform text split\n",
    "    breakpoint_threshold_type='percentile', # set splitting style: 'percentage' of similarity\n",
    "    breakpoint_threshold_amount=95, # split text if similarity score > 95%\n",
    "    min_chunk_size=500,\n",
    "    add_start_index=True, # assign index for chunk\n",
    ")\n",
    "\n",
    "docs = semantic_splitter.split_documents(documents)\n",
    "print(\"Number of sementic chunks:\", len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26c703e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = Chroma.from_documents(documents=docs,\n",
    "                                  embedding=embeddings)\n",
    "\n",
    "retriever = vector_db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0718fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = retriever.invoke(\"IoT l√† g√¨ ?\")\n",
    "print(\"Num of relevant documents: \", len(result))\n",
    "\n",
    "#? Kh√¥ng Embedd ƒë∆∞·ª£c h√¨nh (√Ω nghƒ©a c·ªßa h√¨nh)\n",
    "#? May retrieve duplicate documents\n",
    "for i, doc in enumerate(result, 1):\n",
    "    print(f\"\\nüìÑ Documellmnt {i}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"üìÑ Page       : {doc.metadata.get('page_label', doc.metadata.get('page'))}\")\n",
    "    print(f\"üìù Content    :\\n{doc.page_content.strip()}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd64125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('token.txt', 'r') as f:\n",
    "    hg_token = f.read() #? read huggingface token from token.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56800112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up config\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01f83da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f89b6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#? Initialize Model and Tokenizer\n",
    "#? PhoGPT-5.5B\n",
    "#? Phi-2 (2.7B)\n",
    "#? lmsys/vicuna-7b-v1.5\n",
    "MODEL_NAME= \"google/gemma-2b-it\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=nf4_config, # add config\n",
    "    low_cpu_mem_usage=True,\n",
    "    token=hg_token\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b54f090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model is on CUDA\n",
    "if next(model.parameters()).is_cuda:\n",
    "    print(\"Model is running on CUDA.\")\n",
    "else:\n",
    "    print(\"Model is not running on CUDA.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec9d7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    use_fast=True,\n",
    "    padding_side='left',   # 'left' or 'right' depending on model style (e.g., causal LM often prefers left)\n",
    "    truncation_side='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d2d52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# #? Integrated tokenizer and model into a Pipeline (for convinient)\n",
    "model_pipeline = pipeline(\n",
    "    'text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1024, # output token\n",
    "    device_map=\"auto\" # auto allocate GPU if available\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=model_pipeline,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0974a120",
   "metadata": {},
   "source": [
    "## Learn how to prompt so the LLM can generate better multiple-choice question\n",
    "\n",
    "V√≠ d·ª• v·ªÅ m·ªôt c√¢u h·ªèi tr·∫Øc nghi·ªám t·ªët:\n",
    "\n",
    "C√¢u h·ªèi: T·∫•n c√¥ng side-channel l√† g√¨?\n",
    "\n",
    "Ph∆∞∆°ng √°n:\n",
    "\n",
    "A. L√† t·∫•n c√¥ng t·ª´ xa v√†o giao di·ªán web.\n",
    "\n",
    "B. L√† ki·ªÉu t·∫•n c√¥ng d·ª±a tr√™n h√†nh vi ti√™u th·ª• nƒÉng l∆∞·ª£ng c·ªßa thi·∫øt b·ªã.\n",
    "\n",
    "C. L√† t·∫•n c√¥ng tr·ª±c di·ªán v√†o h·∫° t·∫ßng m·∫°ng\n",
    "\n",
    "D. L√† t·∫•n c√¥ng d·ª±a v√†o b·ª©c x·∫° ƒëi·ªán t·ª´ ƒë·ªÉ l·∫•y kh√≥a m√£ h√≥a.\n",
    "\n",
    "ƒê√°p √°n ƒë√∫ng: D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c006a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "parser = StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adc7736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_result(result, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "            f.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e1c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f320669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "        Tr·∫£ l·ªùi ng·∫Øn g·ªçn, r√µ r√†ng b·∫±ng ti·∫øng vi·ªát v√† ch·ªâ d·ª±a tr√™n th√¥ng tin c√≥ s·∫µn b√™n d∆∞·ªõi.\n",
    "        N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin, h√£y n√≥i r√µ l√† kh√¥ng c√≥ d·ªØ li·ªáu li√™n quan.\n",
    "\n",
    "        N·ªôi dung t√†i li·ªáu:\n",
    "        {context}\n",
    "\n",
    "        C√¢u h·ªèi:\n",
    "        {question}\n",
    "\n",
    "        Tr·∫£ l·ªùi:\n",
    "\"\"\") #? d√πng {{ }} ƒë·ªÉ langchain kh√¥ng nh·∫≠n string b√™n trong {} l√† Bi·∫øn\n",
    "\n",
    "rag_chain = (\n",
    "{\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | parser\n",
    ")\n",
    "\n",
    "query = 'Li·ªát k√™ c√°c th√†nh ph·∫ßn trong h·ªá th·ªëng IoT ?'\n",
    "result = rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951da108",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2853a3fd",
   "metadata": {},
   "source": [
    "### Customize RAG Output to Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9409b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_choice_prompt = \"\"\"\n",
    "        D·ª±a v√†o n·ªôi dung sau, h√£y:\n",
    "        1. T√≥m t·∫Øt t·ªëi ƒëa 3 √Ω ch√≠nh, k√®m theo s·ªë trang n·∫øu c√≥.\n",
    "        2. Tr·∫£ l·ªùi c√¢u h·ªèi b·∫±ng ti·∫øng Vi·ªát ng·∫Øn g·ªçn v√† ch√≠nh x√°c.\n",
    "        3. N·∫øu kh√¥ng c√≥ th√¥ng tin li√™n quan, h√£y ƒë·ªÉ \"Answer\" l√† \"Kh√¥ng c√≥ d·ªØ li·ªáu li√™n quan\".\n",
    "\n",
    "        ƒê·∫£m b·∫£o tr·∫£ k·∫øt qu·∫£ **·ªü d·∫°ng JSON** v·ªõi c·∫•u tr√∫c sau:\n",
    "        {{\"main_ideas\": [\n",
    "            {{\"point\": \"√ù ch√≠nh 1\", \"source\": \"Trang ...\"}},\n",
    "            {{\"point\": \"√ù ch√≠nh 2\", \"source\": \"Trang ...\"}},\n",
    "            {{\"point\": \"√ù ch√≠nh 3\", \"source\": \"Trang ...\"}}\n",
    "        ],\n",
    "        \"answer\": \"C√¢u tr·∫£ l·ªùi ng·∫Øn g·ªçn\"\n",
    "        }}\n",
    "\n",
    "        Vui l√≤ng ch·ªâ tr·∫£ l·ªùi b·∫±ng format JSON, kh√¥ng gi·∫£i th√≠ch th√™m.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question:\n",
    "        {question}\n",
    "\n",
    "        Answer:\n",
    "\n",
    "\"\"\" #? d√πng {{ }} ƒë·ªÉ langchain kh√¥ng nh·∫≠n string b√™n trong {} l√† Bi·∫øn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee898dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class MainIdea(BaseModel):\n",
    "    point: str\n",
    "    source: str\n",
    "\n",
    "class QAResponse(BaseModel):\n",
    "    main_ideas: List[MainIdea]\n",
    "    answer: str\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=QAResponse)\n",
    "prompt_template = PromptTemplate(\n",
    "    template=multi_choice_prompt,\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt_template\n",
    "    | llm\n",
    "    | parser\n",
    ")\n",
    "\n",
    "# query = 'Li·ªát k√™ c√°c th√†nh ph·∫ßn trong h·ªá th·ªëng IoT ?'\n",
    "query = 'IoT l√† g√¨ ?'\n",
    "result = rag_chain.invoke(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34976175",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_choice_prompt = \"\"\"\n",
    "        D·ª±a v√†o n·ªôi dung sau, h√£y:\n",
    "        1. T√≥m t·∫Øt t·ªëi ƒëa 3 √Ω ch√≠nh, k√®m theo s·ªë trang n·∫øu c√≥.\n",
    "        2. Tr·∫£ l·ªùi c√¢u h·ªèi b·∫±ng ti·∫øng Vi·ªát ng·∫Øn g·ªçn v√† ch√≠nh x√°c.\n",
    "        3. N·∫øu kh√¥ng c√≥ th√¥ng tin li√™n quan, h√£y ƒë·ªÉ \"Answer\" l√† \"Kh√¥ng c√≥ d·ªØ li·ªáu li√™n quan\".\n",
    "\n",
    "        ƒê·∫£m b·∫£o tr·∫£ k·∫øt qu·∫£ **·ªü d·∫°ng JSON** v·ªõi c·∫•u tr√∫c sau:\n",
    "        {{\"main_ideas\": [\n",
    "            {{\"point\": \"√ù ch√≠nh 1\", \"source\": \"Trang ...\"}},\n",
    "            {{\"point\": \"√ù ch√≠nh 2\", \"source\": \"Trang ...\"}},\n",
    "            {{\"point\": \"√ù ch√≠nh 3\", \"source\": \"Trang ...\"}}\n",
    "        ],\n",
    "        \"answer\": \"C√¢u tr·∫£ l·ªùi ng·∫Øn g·ªçn\"\n",
    "        }}\n",
    "\n",
    "        Vui l√≤ng ch·ªâ tr·∫£ l·ªùi b·∫±ng format JSON, kh√¥ng gi·∫£i th√≠ch th√™m.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question:\n",
    "        {question}\n",
    "\n",
    "        Answer:\n",
    "\n",
    "\"\"\" #? d√πng {{ }} ƒë·ªÉ langchain kh√¥ng nh·∫≠n string b√™n trong {} l√† Bi·∫øn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc039d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_custom_rag(user_question):\n",
    "    prompt = PromptTemplate.from_template(multi_choice_prompt)\n",
    "    rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | parser\n",
    "    )\n",
    "\n",
    "\n",
    "    query = user_question\n",
    "    result = rag_chain.invoke(query)\n",
    "\n",
    "    file_path = 'output.txt'\n",
    "    save_result(result, file_path)\n",
    "\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48f7aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"IoT l√† gi?\"\n",
    "result = run_custom_rag(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55605c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"C√°c th√†nh ph·∫ßn trong h·ªá th·ªëng IoT bao g·ªìm nh·ªØng g√¨ ?\"\n",
    "result = run_custom_rag(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d59b789",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-multi-choice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
